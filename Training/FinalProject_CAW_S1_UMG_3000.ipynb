{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":128201,"status":"ok","timestamp":1663923722103,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"},"user_tz":-60},"id":"dg-Zaz71X5ao","outputId":"3827f184-9e31-46db-a361-62e024bd8dce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.9.0\n","  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[K     |████████████████████████████████| 831.4 MB 2.2 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (4.1.1)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.9.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: librosa==0.8.1 in /usr/local/lib/python3.7/dist-packages (0.8.1)\n","Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (0.56.2)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (1.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (21.3)\n","Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (0.10.3.post1)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (1.0.2)\n","Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (0.4.0)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (4.4.2)\n","Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (3.0.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (1.21.6)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (1.7.3)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (1.1.0)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa==0.8.1) (0.39.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa==0.8.1) (4.12.0)\n","Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa==0.8.1) (57.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa==0.8.1) (3.0.9)\n","Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.1) (1.4.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.1) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (1.24.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.1) (3.1.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa==0.8.1) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.8.1) (2.21)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa==0.8.1) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa==0.8.1) (3.8.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting soundfile==0.10.2\n","  Downloading SoundFile-0.10.2-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile==0.10.2) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile==0.10.2) (2.21)\n","Installing collected packages: soundfile\n","  Attempting uninstall: soundfile\n","    Found existing installation: SoundFile 0.10.3.post1\n","    Uninstalling SoundFile-0.10.3.post1:\n","      Successfully uninstalled SoundFile-0.10.3.post1\n","Successfully installed soundfile-0.10.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting bokeh==2.3.0\n","  Downloading bokeh-2.3.0.tar.gz (10.6 MB)\n","\u001b[K     |████████████████████████████████| 10.6 MB 9.9 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (6.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (2.8.2)\n","Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (2.11.3)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (1.21.6)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (7.1.2)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (21.3)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (5.1.1)\n","Requirement already satisfied: typing_extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (4.1.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.7->bokeh==2.3.0) (2.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh==2.3.0) (3.0.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh==2.3.0) (1.15.0)\n","Building wheels for collected packages: bokeh\n","  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bokeh: filename=bokeh-2.3.0-py3-none-any.whl size=11292273 sha256=100de8393744bb25f457374192cc180d3c29754b2e90adf62e78770010599778\n","  Stored in directory: /root/.cache/pip/wheels/fe/2b/67/993b844d1b11a6129b91880955c1b315438d00fc39d2dcf489\n","Successfully built bokeh\n","Installing collected packages: bokeh\n","  Attempting uninstall: bokeh\n","    Found existing installation: bokeh 2.3.3\n","    Uninstalling bokeh-2.3.3:\n","      Successfully uninstalled bokeh-2.3.3\n","Successfully installed bokeh-2.3.0\n"]}],"source":["!pip install torch==1.9.0\n","!pip install librosa==0.8.1\n","!pip install soundfile==0.10.2\n","!pip install bokeh==2.3.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jkAW3HWGajZj"},"outputs":[],"source":["import torch\n","import librosa\n","import soundfile as sf\n","import torch.nn as nn\n","import numpy as np\n","from torch.nn.utils import weight_norm\n","from torch import optim\n","from math import ceil\n","import glob\n","import time\n","import random\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48340,"status":"ok","timestamp":1663923773469,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"},"user_tz":-60},"id":"LqIHLYluDlVe","outputId":"a0ad367e-4d69-4a62-8a4b-d03045a859b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["#Connect colab to your google drive\n","from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2947,"status":"ok","timestamp":1663923776411,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"},"user_tz":-60},"id":"2-dJwYs_iAkN","outputId":"d79efbc6-5ec7-4b09-d94a-7b970ce887f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SajXpcQKDUgM"},"outputs":[],"source":["#prepare input folder\n","input_folder='inputs'\n","if not os.path.exists(input_folder):\n","    os.mkdir(input_folder)\n","#copy file from drive to colab --remember it has to be from mydrive---for whatever reason it does not go any deeper!\n","!cp /content/drive/MyDrive/S1_UMG.wav /content/inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4q0K6-KXcxmO"},"outputs":[],"source":["#paramaters\n","inpainting_indices= [0, 1]\n","is_cuda = torch.cuda.is_available()\n","gpu_num = 0\n","manual_random_seed = -1\n","input_file = 'S1_UMG.wav'\n","segments_to_train = []\n","start_time = 0\n","init_sample_rate =  16000\n","fs_list = [320, 400, 500, 640, 800, 1000, 1280, 1600, 2000, 2500, 4000, 8000, 10000, 12000, 14400, 16000]\n","max_length = 25\n","run_mode = 'normal' #['normal', 'inpainting', 'denoising']\n","num_epochs = 3000\n","learning_rate = 0.0015\n","scheduler_lr_decay = 0.1\n","beta1 = 0.5\n","speech = False\n","num_layers = 8\n","output_folder = 'outputs'\n","filter_size = 9\n","set_first_scale_by_energy = True\n","min_energy_th = 0.0025\n","hidden_channels_init = 16\n","growing_hidden_channels_factor = 6\n","plot_losses = False\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","initial_noise_amp = 1\n","noise_amp_factor = 0.01"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bELoeAltdEdB"},"outputs":[],"source":["#functions\n","def get_input_signal(input_file, max_length):\n","    file_name = input_file.split('.')\n","    if len(file_name) < 2:\n","        input_file = '.'.join([input_file, 'wav'])\n","    output_folder = file_name[0].replace(' ', '_')\n","    if len(segments_to_train) == 0:\n","        samples, Fs = librosa.load(os.path.join('inputs', input_file), sr=None,\n","                                   offset=start_time, duration=2 * max_length)\n","\n","    if samples.shape[0] / Fs > max_length:\n","        n_samples = int(max_length * Fs)\n","        samples = samples[:n_samples]\n","\n","    output_folder = output_folder\n","    output_folder = os.path.join('outputs', output_folder)\n","    Fs = Fs\n","    if init_sample_rate < Fs:\n","        hr_samples = samples.copy()\n","        samples = librosa.resample(hr_samples, Fs, init_sample_rate)\n","        Fs = init_sample_rate\n","    norm_factor = max(abs(samples.reshape(-1)))\n","    samples = samples / norm_factor\n","    return samples, Fs\n","\n","def create_input_signals(scales, set_first_scale_by_energy, min_energy_th,  filter_size, input_signal, Fs):\n","    # Performs downscaling for desired scales and outputs list of signals\n","    signals_list = []\n","    fs_list = []\n","    n_scales = len(scales)\n","    set_first_scale = False\n","    rf = calc_receptive_field(filter_size, dilation_factors)\n","    for k in range(n_scales):\n","        downsample = scales[k]\n","        fs = int(Fs / downsample)\n","        if downsample == 1:\n","            coarse_sig = input_signal\n","        else:\n","            coarse_sig = torch.Tensor(librosa.resample(input_signal.squeeze().numpy(), Fs, fs))\n","        if speech and fs < 500:\n","            continue\n","        if set_first_scale_by_energy and not speech:\n","            e = (coarse_sig ** 2).mean()\n","            if e < min_energy_th and not set_first_scale:\n","                continue\n","        set_first_scale = True\n","        signals_list.append(coarse_sig)\n","        assert np.mod(fs, 1) == 0, 'Sampling rate is not integer'\n","        fs_list.append(int(fs))\n","\n","        # Write downsampled real sound\n","        filename = 'real@%dHz.wav' % fs\n","        write_signal(os.path.join(output_folder, filename), coarse_sig.cpu(), fs)\n","\n","    return signals_list, fs_list\n","\n","def calc_receptive_field(filter_size, dilation_factors, Fs=None):\n","    if Fs is None:\n","        # in samples\n","        return (filter_size * dilation_factors[0] + sum(dilation_factors[1:]) * (filter_size - 1))\n","    else:\n","        # in [ms]\n","        return (filter_size * dilation_factors[0] + sum(dilation_factors[1:]) * (filter_size - 1)) / Fs * 1e3\n","\n","def write_signal(path, signal, fs, overwrite=False, subtype='PCM_16'):\n","    if signal is None:\n","        return\n","    if torch.is_tensor(signal):\n","        signal = signal.squeeze().detach().cpu().numpy()\n","    if not path.endswith('.wav'):\n","        path = path + '.wav'\n","    if not overwrite:\n","        if os.path.exists(path):\n","            files = glob.glob(path[:-4].replace('[Hz]', '[[]Hz[]]') + '*')\n","            path = path[:-4] + '_' + str(len(files)) + path[-4:]\n","    maxAmp = max(abs(signal.reshape(-1)))\n","    if maxAmp > 1:\n","        signal = signal / maxAmp  # normalize to avoid clipping\n","    sf.write(path, signal, fs, subtype=subtype)\n","\n","def calc_pad_size(dilation_factors, filter_size):\n","    return int(np.ceil(sum(dilation_factors) * (filter_size - 1) / 2))\n","\n","def get_noise(device, shape):\n","    return torch.randn(shape, device=device)\n","\n","def draw_signal(generators_list, signals_lengths_list, fs_list, noise_amp_list, filter_size, dilation_factors, device, reconstruction_noise_list=None,\n","                condition=None, output_all_scales=False):\n","    # Draws a signal up to current scale, using learned generators\n","    pad_size = calc_pad_size(dilation_factors, filter_size)\n","    if output_all_scales:\n","        signals_all_scales = []\n","    for scale_idx, (netG, noise_amp) in enumerate(zip(generators_list, noise_amp_list)):\n","        signal_padder = nn.ConstantPad1d(pad_size, 0)\n","        if condition is None:\n","            n_samples = signals_lengths_list[scale_idx]\n","            if reconstruction_noise_list is not None:\n","                noise_signal = reconstruction_noise_list[scale_idx]\n","            else:\n","                noise_signal = get_noise(device, (1, 1, n_samples))\n","                noise_signal = noise_signal * noise_amp\n","\n","            if scale_idx == 0:\n","                prev_sig = torch.full(noise_signal.shape, 0, device=device, dtype=noise_signal.dtype)\n","            else:\n","                prev_sig = signal_padder(prev_sig)\n","\n","            # pad noise with zeros, to match signal after filtering\n","            if reconstruction_noise_list is None:\n","                # reconstruction_noise is already padded\n","                noise_signal = signal_padder(noise_signal)\n","                if scale_idx == 0:\n","                    prev_sig = signal_padder(prev_sig)\n","        else:\n","            if scale_idx < condition[\"condition_scale_idx\"]:\n","                continue\n","            elif scale_idx == condition[\"condition_scale_idx\"]:\n","                prev_sig = resample_sig(device, condition[\"condition_signal\"], condition['condition_fs'],\n","                                        fs_list[scale_idx]).expand(1, 1, -1)\n","            noise_signal = get_noise(device, prev_sig.shape[2]).expand(1, 1, -1)\n","            noise_signal = signal_padder(noise_signal)\n","            noise_signal = noise_signal * noise_amp\n","            prev_sig = signal_padder(prev_sig)\n","\n","        # Generate this scale signal\n","        cur_sig = netG((noise_signal + prev_sig).detach(), prev_sig)\n","\n","        if output_all_scales:\n","            signals_all_scales.append(torch.squeeze(cur_sig).detach().cpu().numpy())\n","\n","        # Upsample for next scale\n","        if scale_idx < len(fs_list) - 1:\n","            up_sig = resample_sig( device, cur_sig, orig_fs=fs_list[scale_idx], target_fs=fs_list[scale_idx + 1])\n","            if up_sig.shape[2] > signals_lengths_list[scale_idx + 1]:\n","                assert abs(\n","                    up_sig.shape[2] > signals_lengths_list[scale_idx + 1]) < 20, 'Should not happen, check this!'\n","                up_sig = up_sig[:, :, :signals_lengths_list[scale_idx + 1]]\n","            elif up_sig.shape[2] < signals_lengths_list[scale_idx + 1]:\n","                assert abs(\n","                    up_sig.shape[2] < signals_lengths_list[scale_idx + 1]) < 20, 'Should not happen, check this!'\n","                up_sig = torch.cat(\n","                    (up_sig, up_sig.new_zeros(1, 1, signals_lengths_list[scale_idx + 1] - up_sig.shape[2])),\n","                    dim=2)\n","        else:\n","            up_sig = cur_sig\n","        prev_sig = up_sig\n","        prev_sig = prev_sig.detach()\n","\n","        del up_sig, cur_sig, noise_signal, netG\n","\n","    if output_all_scales:\n","        return signals_all_scales\n","    else:\n","        return prev_sig\n","\n","def resample_sig(device,input_signal, orig_fs=None, target_fs=None, resamplers=None):\n","    if resamplers == None:\n","        resamplers = {}\n","    if (orig_fs, target_fs) in resamplers.keys() and resamplers[(orig_fs, target_fs)].in_shape[2] == \\\n","            input_signal.shape[2]:\n","        resampler = resamplers[(orig_fs, target_fs)]\n","    else:\n","        in_shape = input_signal.shape\n","        scale_factors = (1, 1, target_fs / orig_fs)\n","        resampler = ResizeLayer(in_shape, scale_factors=scale_factors, device=device)\n","        resamplers[(orig_fs, target_fs)] = resampler\n","    new_sig = resampler(input_signal)\n","\n","    return new_sig\n","\n","def support_sz(sz):\n","    def wrapper(f):\n","        f.support_sz = sz\n","        return f\n","    return wrapper\n","\n","@support_sz(4)\n","def cubic(x):\n","    fw, to_dtype, eps = set_framework_dependencies(x)\n","    absx = fw.abs(x)\n","    absx2 = absx ** 2\n","    absx3 = absx ** 3\n","    return ((1.5 * absx3 - 2.5 * absx2 + 1.) * to_dtype(absx <= 1.) +\n","            (-0.5 * absx3 + 2.5 * absx2 - 4. * absx + 2.) *\n","            to_dtype((1. < absx) & (absx <= 2.)))\n","\n","class ResizeLayer(nn.Module):\n","    def __init__(self, in_shape, scale_factors=None, out_shape=None,\n","                 interp_method=cubic, support_sz=None,\n","                 antialiasing=True, device=None):\n","        super(ResizeLayer, self).__init__()\n","\n","        # fw stands for framework, that can be either numpy or torch. since\n","        # this is a torch layer, only one option in this case.\n","        fw = torch\n","        eps = fw.finfo(fw.float32).eps\n","\n","        # set missing scale factors or output shapem one according to another,\n","        # scream if both missing\n","        scale_factors, out_shape = set_scale_and_out_sz(in_shape, out_shape,\n","                                                        scale_factors, fw)\n","        \n","        # unless support size is specified by the user, it is an attribute\n","        # of the interpolation method\n","        if support_sz is None:\n","            support_sz = interp_method.support_sz\n","        \n","        self.n_dims = len(in_shape)       \n","\n","        # sort indices of dimensions according to scale of each dimension.\n","        # since we are going dim by dim this is efficient\n","        self.sorted_filtered_dims_and_scales = [(dim, scale_factors[dim])\n","                                                for dim in\n","                                                sorted(range(self.n_dims),\n","                                                key=lambda ind:\n","                                                scale_factors[ind])\n","                                                if scale_factors[dim] != 1.]\n","\n","        # iterate over dims\n","        field_of_view_list = []\n","        weights_list = []\n","        for dim, scale_factor in self.sorted_filtered_dims_and_scales:\n","\n","            # get 1d set of weights and fields of view for each output\n","            # location along this dim\n","            field_of_view, weights = prepare_weights_and_field_of_view_1d(\n","                dim, scale_factor, in_shape[dim], out_shape[dim],\n","                interp_method, support_sz, antialiasing, fw, eps, device)\n","\n","            # keep weights and fields of views for all dims\n","            weights_list.append(nn.Parameter(weights, requires_grad=False))\n","            field_of_view_list.append(nn.Parameter(field_of_view,\n","                                      requires_grad=False))\n","\n","        self.field_of_view = nn.ParameterList(field_of_view_list)\n","        self.weights = nn.ParameterList(weights_list)\n","        self.in_shape = in_shape\n","\n","    def forward(self, input):\n","        # output begins identical to input and changes with each iteration\n","        output = input\n","\n","        for (dim, scale_factor), field_of_view, weights in zip(\n","                self.sorted_filtered_dims_and_scales,\n","                self.field_of_view,\n","                self.weights):\n","            # multiply the weights by the values in the field of view and\n","            # aggreagate\n","            output = apply_weights(output, field_of_view, weights, dim,\n","                                   self.n_dims, torch)\n","        return output\n","\n","def prepare_weights_and_field_of_view_1d(dim, scale_factor, in_sz, out_sz,\n","                                         interp_method, support_sz, \n","                                         antialiasing, fw, eps, device=None):\n","    # If antialiasing is taking place, we modify the window size and the\n","    # interpolation method (see inside function)\n","    interp_method, cur_support_sz = apply_antialiasing_if_needed(\n","                                                             interp_method,\n","                                                             support_sz,\n","                                                             scale_factor,\n","                                                             antialiasing)\n","\n","    # STEP 1- PROJECTED GRID: The non-integer locations of the projection of\n","    # output pixel locations to the input tensor\n","    projected_grid = get_projected_grid(in_sz, out_sz, scale_factor, fw, device)\n","\n","    # STEP 2- FIELDS OF VIEW: for each output pixels, map the input pixels\n","    # that influence it\n","    field_of_view = get_field_of_view(projected_grid, cur_support_sz, in_sz,\n","                                      fw, eps)\n","\n","    # STEP 3- CALCULATE WEIGHTS: Match a set of weights to the pixels in the\n","    # field of view for each output pixel\n","    weights = get_weights(interp_method, projected_grid, field_of_view)\n","\n","    return field_of_view, weights\n","\n","def apply_weights(input, field_of_view, weights, dim, n_dims, fw):\n","    # STEP 4- APPLY WEIGHTS: Each output pixel is calculated by multiplying\n","    # its set of weights with the pixel values in its field of view.\n","    # We now multiply the fields of view with their matching weights.\n","    # We do this by tensor multiplication and broadcasting.\n","    # this step is separated to a different function, so that it can be\n","    # repeated with the same calculated weights and fields.\n","\n","    # for this operations we assume the resized dim is the first one.\n","    # so we transpose and will transpose back after multiplying\n","    tmp_input = fw_swapaxes(input, dim, 0, fw)\n","\n","    # field_of_view is a tensor of order 2: for each output (1d location\n","    # along cur dim)- a list of 1d neighbors locations.\n","    # note that this whole operations is applied to each dim separately,\n","    # this is why it is all in 1d.\n","    # neighbors = tmp_input[field_of_view] is a tensor of order image_dims+1:\n","    # for each output pixel (this time indicated in all dims), these are the\n","    # values of the neighbors in the 1d field of view. note that we only\n","    # consider neighbors along the current dim, but such set exists for every\n","    # multi-dim location, hence the final tensor order is image_dims+1.\n","    neighbors = tmp_input[field_of_view]\n","\n","    # weights is an order 2 tensor: for each output location along 1d- a list\n","    # of weighs matching the field of view. we augment it with ones, for\n","    # broadcasting, so that when multiplies some tensor the weights affect\n","    # only its first dim.\n","    tmp_weights = fw.reshape(weights, (*weights.shape, * [1] * (n_dims - 1)))\n","\n","    # now we simply multiply the weights with the neighbors, and then sum\n","    # along the field of view, to get a single value per out pixel\n","    tmp_output = (neighbors * tmp_weights).sum(1)\n","\n","    # we transpose back the resized dim to its original position\n","    return fw_swapaxes(tmp_output, 0, dim, fw)\n","\n","def get_weights(interp_method, projected_grid, field_of_view):\n","    # the set of weights per each output pixels is the result of the chosen\n","    # interpolation method applied to the distances between projected grid\n","    # locations and the pixel-centers in the field of view (distances are\n","    # directed, can be positive or negative)\n","    weights = interp_method(projected_grid[:, None] - field_of_view)\n","\n","    # we now carefully normalize the weights to sum to 1 per each output pixel\n","    sum_weights = weights.sum(1, keepdims=True)\n","    sum_weights[sum_weights == 0] = 1\n","    return weights / sum_weights\n","\n","def fw_ceil(x, fw):\n","    return x.ceil().long()\n","\n","\n","def fw_cat(x, fw):\n","    return fw.cat(x)\n","\n","\n","def fw_swapaxes(x, ax_1, ax_2, fw):\n","    return x.transpose(ax_1, ax_2)\n","    \n","def fw_set_device(x, device, fw):\n","    return x.to(device)\n","\n","def set_scale_and_out_sz(in_shape, out_shape, scale_factors, fw):\n","    # eventually we must have both scale-factors and out-sizes for all in/out\n","    # dims. however, we support many possible partial arguments\n","    if scale_factors is None and out_shape is None:\n","        raise ValueError(\"either scale_factors or out_shape should be \"\n","                         \"provided\")\n","    if out_shape is not None:\n","        # if out_shape has less dims than in_shape, we defaultly resize the\n","        # first dims for numpy and last dims for torch\n","        out_shape = list(out_shape) + list(in_shape[:-len(out_shape)])\n","        if scale_factors is None:\n","            # if no scale given, we calculate it as the out to in ratio\n","            # (not recomended)\n","            scale_factors = [out_sz / in_sz for out_sz, in_sz\n","                             in zip(out_shape, in_shape)]\n","    if scale_factors is not None:\n","        # by default, if a single number is given as scale, we assume resizing\n","        # two dims (most common are images with 2 spatial dims)\n","        scale_factors = (scale_factors\n","                         if isinstance(scale_factors, (list, tuple))\n","                         else [scale_factors, scale_factors])\n","        # if less scale_factors than in_shape dims, we defaultly resize the\n","        # first dims for numpy and last dims for torch\n","        scale_factors = list(scale_factors) + [1] * (len(in_shape) - len(scale_factors)) \n","        if out_shape is None:\n","            # when no out_shape given, it is calculated by multiplying the\n","            # scale by the in_shape (not recomended)\n","            out_shape = [ceil(scale_factor * in_sz)\n","                         for scale_factor, in_sz in\n","                         zip(scale_factors, in_shape)]\n","        # next line intentionally after out_shape determined for stability\n","        scale_factors = [float(sf) for sf in scale_factors]\n","    return scale_factors, out_shape\n","\n","def apply_antialiasing_if_needed(interp_method, support_sz, scale_factor,\n","                                 antialiasing):\n","    # antialiasing is \"stretching\" the field of view according to the scale\n","    # factor (only for downscaling). this is low-pass filtering. this\n","    # requires modifying both the interpolation (stretching the 1d\n","    # function and multiplying by the scale-factor) and the window size.\n","    if scale_factor >= 1.0 or not antialiasing:\n","        return interp_method, support_sz\n","    cur_interp_method = (lambda arg: scale_factor *\n","                         interp_method(scale_factor * arg))\n","    cur_support_sz = support_sz / scale_factor\n","    return cur_interp_method, cur_support_sz\n","\n","def get_projected_grid(in_sz, out_sz, scale_factor, fw, device=None):\n","    # we start by having the ouput coordinates which are just integer locations\n","    out_coordinates = fw.arange(out_sz)\n","    \n","    # if using torch we need to match the grid tensor device to the input device\n","    out_coordinates = fw_set_device(out_coordinates, device, fw)\n","        \n","    # This is projecting the ouput pixel locations in 1d to the input tensor,\n","    # as non-integer locations.\n","    # the following fomrula is derived in the paper\n","    # \"From Discrete to Continuous Convolutions\" by Shocher et al.\n","    return (out_coordinates / scale_factor +\n","            (in_sz - 1) / 2 - (out_sz - 1) / (2 * scale_factor))\n","\n","\n","def get_field_of_view(projected_grid, cur_support_sz, in_sz, fw, eps):\n","    # for each output pixel, map which input pixels influence it, in 1d.\n","    # we start by calculating the leftmost neighbor, using half of the window\n","    # size (eps is for when boundary is exact int)\n","    left_boundaries = fw_ceil(projected_grid - cur_support_sz / 2 - eps, fw)\n","\n","    # then we simply take all the pixel centers in the field by counting\n","    # window size pixels from the left boundary\n","    ordinal_numbers = fw.arange(ceil(cur_support_sz - eps))\n","    # in case using torch we need to match the device\n","    ordinal_numbers = fw_set_device(ordinal_numbers, projected_grid.device, fw)\n","    field_of_view = left_boundaries[:, None] + ordinal_numbers\n","\n","    # next we do a trick instead of padding, we map the field of view so that\n","    # it would be like mirror padding, without actually padding\n","    # (which would require enlarging the input tensor)\n","    mirror = fw_cat((fw.arange(in_sz), fw.arange(in_sz - 1, -1, step=-1)), fw)\n","    field_of_view = mirror[fw.remainder(field_of_view, mirror.shape[0])]\n","    field_of_view = fw_set_device(field_of_view,projected_grid.device, fw)\n","    return field_of_view\n","\n","def set_framework_dependencies(x):\n","    if type(x) is np.ndarray:\n","        to_dtype = lambda a: a\n","        fw = np\n","    else:\n","        to_dtype = lambda a: a.to(x.dtype)\n","        fw = torch\n","    eps = fw.finfo(fw.float32).eps\n","    return fw, to_dtype, eps\n","\n","def calc_gradient_penalty(run_mode, current_holes, netD, real_data, fake_data, LAMBDA, alpha=None, _grad_outputs=None, mask_ratio=None, not_valid_idx_start=None, not_valid_idx_end=None):\n","    # Gradient penalty method for WGAN\n","    if alpha is None:\n","        alpha = torch.rand(1, 1)\n","        alpha = alpha.expand(real_data.size())\n","        if torch.cuda.is_available():\n","            alpha = alpha.cuda(real_data.get_device())  # gpu) #if use_cuda else alpha\n","    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","    interpolates = torch.autograd.Variable(interpolates, requires_grad=True)\n","    use_mask = False\n","    mask_ratio = 1\n","    disc_interpolates = netD(interpolates, use_mask)\n","    if _grad_outputs is None:\n","        _grad_outputs = torch.ones(disc_interpolates.size())\n","        if torch.cuda.is_available():\n","            _grad_outputs = _grad_outputs.cuda(real_data.get_device())\n","    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n","                                    grad_outputs=_grad_outputs,\n","                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n","    gradient_penalty = ((mask_ratio * gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n","    del gradients, interpolates, _grad_outputs, disc_interpolates\n","    return gradient_penalty\n","\n","def stft(sig, n_fft, hop_length, window_size):\n","    s = torch.stft(sig, n_fft, hop_length, win_length=window_size,\n","                   window=torch.hann_window(window_size, device=sig.device), return_complex=False)\n","    return s\n","\n","def spec(x, n_fft, hop_length, window_size):\n","    s = stft(x, n_fft, hop_length, window_size)\n","    n = torch.norm(s, p=2, dim=-1)\n","    return n\n","\n","def norm(x):\n","    return (x.view(x.shape[0], -1) ** 2).sum(dim=-1).sqrt()\n","\n","\n","def squeeze(x):\n","    if len(x.shape) == 3:\n","        assert x.shape[-1] in [1, 2]\n","        x = torch.mean(x, -1)\n","    if len(x.shape) != 2:\n","        raise ValueError(f'Unknown input shape {x.shape}')\n","    return x\n","\n","def multi_scale_spectrogram_loss(multispec_loss_n_fft, multispec_loss_hop_length, multispec_loss_window_size, current_holes, x_in, x_out):\n","    losses = []\n","    args = [multispec_loss_n_fft,\n","            multispec_loss_hop_length,\n","            multispec_loss_window_size]\n","    for n_fft, hop_length, window_size in zip(*args):\n","        if window_size == -1:\n","            window_size = x_in.shape[1]\n","            hop_length = window_size + 1\n","            n_fft = int(2 ** np.ceil(np.log2(window_size)))\n","        spec_in = spec(squeeze(x_in.float()), n_fft, hop_length, window_size)\n","        spec_out = spec(squeeze(x_out.float()), n_fft, hop_length, window_size)\n","        losses.append(norm(spec_in - spec_out))\n","    return sum(losses) / len(losses)\n","\n","def reset_grads(model, require_grad):\n","    for p in model.parameters():\n","        p.requires_grad_(require_grad)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uv_GjFaJDb6G"},"outputs":[],"source":["#model \n","class Generator(nn.Module):\n","    def __init__(self, filter_size, hidden_channels, current_fs ):\n","        super(Generator, self).__init__()\n","        self.head = ConvBlock(filter_size, 1, hidden_channels, dilation_factors[0])\n","        self.body = nn.Sequential()\n","        self.Fs = current_fs\n","        for i in range(num_layers - 2):\n","            block = ConvBlock(filter_size, hidden_channels, hidden_channels, dilation_factors[i + 1])\n","            self.body.add_module('block%d' % (i + 1), block)\n","        self.tail = nn.Sequential()\n","        self.tail.add_module('tail0',\n","                             NormConv1d(in_channels=hidden_channels, out_channels=hidden_channels,\n","                                        kernel_size=filter_size,\n","                                        dilation=dilation_factors[-1]))\n","        self.filter = nn.Sequential(\n","            NormConv1d(in_channels=hidden_channels, out_channels=hidden_channels,\n","                       kernel_size=filter_size, padding=int((filter_size - 1) / 2)),\n","            nn.Tanh()\n","        )\n","        self.gate = nn.Sequential(\n","            NormConv1d(in_channels=hidden_channels, out_channels=hidden_channels,\n","                       kernel_size=filter_size, padding=int((filter_size - 1) / 2)),\n","            nn.Sigmoid()\n","        )\n","        self.out_conv = NormConv1d(hidden_channels, 1, kernel_size=1)\n","        self.pe_filter = PreEmphasisFilter(device)\n","\n","    def forward(self, noise_plus_sig, prev_sig):\n","        out_head = self.head(noise_plus_sig)\n","        out_body = self.body(out_head)\n","        out_tail = self.tail(out_body)\n","        filter = self.filter(out_tail)\n","        gate = self.gate(out_tail)\n","        out_tail = filter * gate\n","        out_tail = self.out_conv(out_tail)\n","        out_filt = self.pe_filter(out_tail)\n","        ind = int((prev_sig.shape[2] - out_filt.shape[2]) / 2)\n","        prev_sig = prev_sig[:, :, ind:(prev_sig.shape[2] - ind)]\n","        output = out_filt + prev_sig\n","        return output\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, run_mode, current_holes, hidden_channels, dilation_factors, num_layers, device,filter_size ):\n","        super(Discriminator, self).__init__()\n","        if run_mode == 'inpainting':\n","            mask = current_holes\n","        else:\n","            mask = None\n","        self.head = ConvBlock(filter_size, 1, hidden_channels, dilation_factors[0], mask=mask)\n","        mask = self.head.mask_out\n","        self.body = nn.ModuleList()\n","        for i in range(num_layers - 2):\n","            block = ConvBlock(filter_size, hidden_channels, hidden_channels,\n","                              dilation_factors[i + 1], mask=mask)\n","            mask = block.mask_out\n","            self.body.add_module('block%d' % (i + 1), block)\n","        self.mask_out = mask\n","        self.tail = NormConv1d(hidden_channels, 1, kernel_size=filter_size,\n","                               dilation=dilation_factors[-1])\n","        self.pe_filter = PreEmphasisFilter(device)\n","\n","    def forward(self, sig, use_mask=False):\n","        out_head = self.head(sig, use_mask)\n","        out_body = out_head\n","        for b in self.body:\n","            out_body = b(out_body, use_mask)\n","        out_tail = self.tail(out_body)\n","        output = self.pe_filter(out_tail)\n","        return output\n","\n","\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1 and classname.find('ConvBlock') == -1 and hasattr(m, 'weight'):\n","        if m.weight.numel() > 1 and m.weight.requires_grad:  # scalar blocks are initiailized upon creation\n","            m.weight.data.normal_(0.0, 0.02)\n","\n","    elif classname.find('Norm') != -1 and hasattr(m, 'weight'):\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n","\n","class PreEmphasisFilter(nn.Module):\n","    def __init__(self, device):\n","        super(PreEmphasisFilter, self).__init__()\n","        self.alpha = torch.Tensor([0.97]).to(device)\n","        self.alpha.requires_grad = False\n","\n","    def forward(self, x):\n","        output = torch.cat((x[:, :, 0].view(x.shape[0], x.shape[1], 1), x[:, :, 1:] - self.alpha * x[:, :, :-1]), dim=2)\n","        return output\n","\n","\n","class NormConv1d(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=True):\n","        super(NormConv1d, self).__init__()\n","        self.conv = weight_norm(nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n","                                          stride=stride, padding=padding, dilation=dilation, bias=bias))\n","\n","    def forward(self, x):\n","        output = self.conv(x)\n","        return output\n","\n","\n","class ConvBlock(nn.Sequential):\n","    def __init__(self, filter_size, in_channels, out_channels, dilation=1, mask=None):\n","        super(ConvBlock, self).__init__()\n","        if filter_size is None:\n","            filter_size = filter_size\n","        if mask is not None:\n","            self.mask_in = mask\n","            self.mask_out = []\n","            self.rf = int((filter_size - 1) * dilation)\n","            for hole in self.mask_in:\n","                self.mask_out.append([hole[0] - self.rf, hole[1]])\n","            # ???\n","            # for idx in range(len(self.mask_out) - 1):\n","            #     if self.mask_out[idx+1][0] < self.mask_out[idx][1]:\n","            #         self.mask_out[idx+1][0] = self.mask_out[idx][1] + 1\n","\n","        else:\n","            self.mask_out = None\n","        self.conv = NormConv1d(in_channels, out_channels, filter_size, dilation=dilation)\n","        self.norm = nn.BatchNorm1d(out_channels)\n","        self.activation = nn.LeakyReLU(0.2, inplace=True)\n","\n","    def forward(self, x, use_mask=False):\n","        out_conv = self.conv(x)\n","        if use_mask:\n","            #tmp = torch.cat((out_conv[:, :, :int(self.mask_out[0][0])], out_conv[:, :, int(self.mask_out[0][1] + 1):]), dim=2)\n","            tmp = out_conv[:, :, :int(self.mask_out[0][0])].clone()\n","            cut_idx = []\n","            cut_idx.append(tmp.shape[2])\n","            for idx in range(len(self.mask_out)-1):\n","                tmp = torch.cat((tmp, out_conv[:, :, int(self.mask_out[idx][1] + 1):int(self.mask_out[idx+1][0])]), dim=2)\n","                cut_idx.append(tmp.shape[2])\n","            tmp = torch.cat((tmp, out_conv[:, :, int(self.mask_out[-1][1] + 1):]), dim=2)\n","\n","            tmp_norm = self.norm(tmp)\n","            out_norm = out_conv\n","            out_norm[:, :, :int(self.mask_out[0][0])] = tmp_norm[:, :, :int(cut_idx[0])]\n","            for idx in range(len(self.mask_out) - 1):\n","                out_norm[:, :, int(self.mask_out[idx][1] + 1):int(self.mask_out[idx+1][0])] = tmp_norm[:, :, int(cut_idx[idx]):int(cut_idx[idx+1])] #tmp_norm[:, :, int(self.mask_out[idx][0]):int(self.mask_out[idx+1][0])]\n","                #out_norm[:, :, :int(self.mask_out[idx+1][0])] = tmp_norm[:, :, :int(self.mask_out[idx+1][0])]\n","            out_norm[:, :, int(self.mask_out[-1][1] + 1):] = tmp_norm[:, :, int(cut_idx[-1]):]\n","\n","        else:\n","            out_norm = self.norm(out_conv)\n","        return self.activation(out_norm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_r37Na9OBSA8"},"outputs":[],"source":["#training functions\n","def train(manual_random_seed, fs_list, scales, growing_hidden_channels_factor,learning_rate, beta1, scheduler_lr_decay, plot_losses,\n","          initial_noise_amp, noise_amp_factor, signals_list, dilation_factors, output_folder, inputs_lengths):\n","    if manual_random_seed != -1:\n","        random.seed(manual_random_seed)\n","        torch.manual_seed(manual_random_seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","    fs_list = fs_list\n","    n_scales = len(scales)\n","    generators_list = []\n","    noise_amp_list = []\n","    if run_mode == 'inpainting':\n","        energy_list = [(sig[mask] ** 2).mean().item() for sig, mask in zip(signals_list, masks)]\n","    else:\n","        energy_list = [(sig ** 2).mean().item() for sig in signals_list]\n","    reconstruction_noise_list = []\n","    output_signals = []\n","    loss_vectors = []\n","\n","    for scale_idx in range(n_scales):\n","        output_signals_single_scale, loss_vectors_single_scale, netG, reconstruction_noise_list, noise_amp = train_single_scale(\n","                      scales, device, run_mode, hidden_channels_init, growing_hidden_channels_factor,  learning_rate, beta1, \n","                      scheduler_lr_decay, plot_losses, initial_noise_amp, noise_amp_factor, signals_list, fs_list, \n","                      generators_list, noise_amp_list, energy_list, reconstruction_noise_list, dilation_factors, output_folder, inputs_lengths)\n","\n","        # Write fake sound\n","        fake_sound = output_signals_single_scale['fake_signal'].squeeze()\n","        filename = 'fake@%dHz.wav' % fs_list[scale_idx]\n","        write_signal(os.path.join(output_folder, filename), fake_sound,\n","                     fs_list[scale_idx], overwrite=False)\n","\n","        # Write reconstructed sound\n","        reconstructed_sound = output_signals_single_scale['reconstructed_signal'].squeeze()\n","        filename = 'reconstructed@%dHz.wav' % fs_list[scale_idx]\n","        write_signal(os.path.join(output_folder, filename),\n","                     reconstructed_sound, fs_list[scale_idx], overwrite=False)\n","        torch.save(reconstruction_noise_list,\n","                   os.path.join(output_folder, 'reconstruction_noise_list.pt'))\n","\n","        generators_list.append(netG)\n","        noise_amp_list.append(noise_amp)\n","        output_signals.append(output_signals_single_scale)\n","        loss_vectors.append(loss_vectors_single_scale)\n","\n","    return output_signals, loss_vectors, generators_list, noise_amp_list, energy_list, reconstruction_noise_list\n","\n","\n","def train_single_scale(scales, device, run_mode, hidden_channels_init, growing_hidden_channels_factor,\n","                       learning_rate, beta1, scheduler_lr_decay, plot_losses, initial_noise_amp, noise_amp_factor, signals_list,\n","                        fs_list, generators_list, noise_amp_list, energy_list, reconstruction_noise_list, dilation_factors, output_folder, inputs_lengths):\n","    # Terminology: 0 is the higher scale (original signal, no downsampling). Higher scale means larger downsampling, e.g shorter signals\n","    n_scales = len(scales)\n","    current_scale = n_scales - len(generators_list) - 1\n","    scale_idx = n_scales - current_scale - 1\n","    input_signal = signals_list[scale_idx].to(device)\n","    current_fs = fs_list[scale_idx]\n","    N = len(input_signal)\n","\n","    if run_mode == 'inpainting':\n","        current_mask = masks[scale_idx]\n","        current_mask = current_mask\n","        current_holes = torch.Tensor([(int(idx[0] / Fs * current_fs), int(idx[1] / Fs * current_fs)) for idx in inpainting_indices]).to(device)\n","    else:\n","        current_holes = None\n","\n","    # Create inputs\n","    real_signal = input_signal.reshape(1, 1, N)\n","\n","    hidden_channels = hidden_channels_init if scale_idx == 0 else int(\n","        hidden_channels_init * growing_hidden_channels_factor)\n","\n","    scale_num = n_scales - scale_idx - 1\n","    pad_size = calc_pad_size(dilation_factors, filter_size)\n","    signal_padder = nn.ConstantPad1d(pad_size, 0)\n","\n","    # Initialize models\n","    netD = Discriminator(run_mode, current_holes, hidden_channels, dilation_factors, num_layers, device, filter_size).to(device)\n","    netD.apply(weights_init)\n","    netG = Generator(filter_size, hidden_channels, current_fs).to(device)\n","    netG.apply(weights_init)\n","    receptive_field = calc_receptive_field(filter_size, dilation_factors, current_fs)\n","    receptive_field_percent = 100 * receptive_field / 1e3 / (N / current_fs)\n","    print('Signal in scale %d has %d samples, sample rate is %d[Hz].' % (\n","        scale_num, N, current_fs))\n","    print('Total receptive field is %d[msec] (%.1f%% of input).' % (receptive_field, receptive_field_percent))\n","    with open(os.path.join(output_folder, 'log.txt'), 'a') as f:\n","        f.write('*' * 30 + ' Scale ' + str(scale_num) + ' (' + str(current_fs) + ' [Hz]) ' + '*' * 30)\n","        f.write('\\nreceptive_field = %d[msec] (%.1f%% of input)' % (receptive_field, receptive_field_percent))\n","        f.write('\\nsignal_energy = %.4f' % energy_list[scale_idx])\n","\n","    if scale_idx == 0:\n","        reconstruction_noise = get_noise(device, real_signal.shape)\n","    else:\n","        reconstruction_noise = torch.zeros(real_signal.shape, device=device)\n","        if run_mode == 'inpainting':\n","            reconstruction_noise[:, :, torch.logical_not(current_mask)] = get_noise(device, torch.nonzero(\n","                torch.logical_not(current_mask)).shape[0]).expand(1, 1, -1).to(device)\n","\n","    reconstruction_noise = signal_padder(reconstruction_noise)\n","\n","    if scale_idx > 1:\n","        netG.load_state_dict(\n","            torch.load('%s/netGScale%d.pth' % (output_folder, scale_idx - 1), map_location=device))\n","        netD.load_state_dict(\n","            torch.load('%s/netDScale%d.pth' % (output_folder, scale_idx - 1), map_location=device))\n","\n","    output_folder = output_folder\n","\n","    # Create optimizers\n","    optimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n","    optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n","    schedulerD = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizerD, milestones=scheduler_milestones,\n","                                                      gamma=scheduler_lr_decay)\n","    schedulerG = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizerG, milestones=scheduler_milestones,\n","                                                      gamma=scheduler_lr_decay)\n","\n","    # Initialize error vectors\n","    v_err_real = np.zeros(num_epochs, )\n","    v_err_fake = np.zeros(num_epochs, )\n","    v_gp = np.zeros(num_epochs, )\n","    v_rec_loss = np.zeros(num_epochs, )\n","\n","    epochs_start_time = time.time()\n","    # prepare inputs for gradient penalty\n","    if not run_mode == 'inpainting':\n","        D_out_shape = torch.Size((1, 1, N - 2 * pad_size))\n","        _grad_outputs = torch.ones(D_out_shape, device=device)\n","    grad_pen_alpha_vec = torch.rand(num_epochs).to(device)\n","\n","    inputs_lengths = inputs_lengths\n","    for epoch_num in range(num_epochs):\n","        print_progress = epoch_num % 100 == 0\n","        # Create noise\n","        noise_signal = get_noise(device, real_signal.shape)\n","        noise_signal = signal_padder(noise_signal)\n","        #################################################################\n","        # Optimize D by maximizing D(realSignal)+(1-D(G(noise_signal))) #\n","        #################################################################\n","        netD.zero_grad()\n","        # Run on real signal\n","        not_valid_idx_start = []\n","        not_valid_idx_end = []\n","        if run_mode == 'inpainting':\n","            out_D_real = netD(real_signal, use_mask=True)\n","            tot_samples = out_D_real.shape[2]\n","            not_valid_idx_start = [int(idx[0] - receptive_field / 1e3 * current_fs + 1) for idx in current_holes]\n","            not_valid_idx_end = [int(idx[1] + 1) for idx in current_holes]  # +1 is because of pe filter\n","            out_D_real_cp = out_D_real.clone()\n","            out_D_real = out_D_real_cp[:, :, :not_valid_idx_start[0]]\n","            if len(current_holes) > 1:\n","                for i in range(len(current_holes) - 1):\n","                    out_D_real = torch.cat((out_D_real, out_D_real_cp[:, :, not_valid_idx_end[i] + 1:not_valid_idx_start[i+1]]), dim=2)\n","            out_D_real = torch.cat((out_D_real, out_D_real_cp[:, :, not_valid_idx_end[-1] + 1:]), dim=2)\n","            mask_ratio = tot_samples / out_D_real.shape[2]\n","        else:\n","            mask_ratio = 1\n","            out_D_real = netD(real_signal)\n","        err_real_D = -out_D_real.mean()\n","        err_real_D.backward(retain_graph=True)\n","        err_real_D = err_real_D.detach()\n","        if print_progress or plot_losses:\n","            err_real_D_val = err_real_D.item()\n","\n","        if epoch_num == 0:\n","            if run_mode == 'inpainting':\n","                D_out_shape = out_D_real.shape\n","                _grad_outputs = torch.ones(D_out_shape, device=device)\n","            if scale_idx == 0:  # We are at coarsest scale\n","                prev_signal = torch.full(noise_signal.shape, 0, device=device, dtype=noise_signal.dtype)\n","                prev_reconstructed_signal = torch.zeros(reconstruction_noise.shape, device=device)\n","                noise_amp = initial_noise_amp\n","            else:\n","                prev_signal = draw_signal(generators_list, inputs_lengths, fs_list, noise_amp_list, filter_size, dilation_factors, device)\n","                prev_signal = signal_padder(prev_signal)\n","                prev_reconstructed_signal = draw_signal(generators_list, inputs_lengths,\n","                                                        fs_list,\n","                                                        noise_amp_list, filter_size, dilation_factors, device,\n","                                                        reconstruction_noise_list)\n","                prev_reconstructed_signal = signal_padder(prev_reconstructed_signal)\n","                innovation = energy_list[scale_idx] - energy_list[scale_idx - 1]\n","                energy_diff = torch.sqrt(torch.Tensor([innovation])).to(device)\n","                noise_amp = noise_amp_factor * max(torch.Tensor([0]).to(device),\n","                                                          energy_diff)\n","\n","            if scale_idx == 1 and add_cond_noise:\n","                noise_amp = prev_reconstructed_signal.std()\n","\n","            with open(os.path.join(output_folder, 'log.txt'), 'a') as f:\n","                f.write('\\nnoise_amp: %.6f' % noise_amp)\n","\n","            reconstruction_noise = reconstruction_noise * noise_amp\n","            reconstruction_noise_list.append(reconstruction_noise)\n","        else:\n","            if scale_idx > 0:\n","                prev_signal = draw_signal(generators_list, inputs_lengths, fs_list, noise_amp_list, filter_size, dilation_factors, device)\n","                prev_signal = signal_padder(prev_signal)\n","\n","        input_noise = noise_signal * noise_amp\n","\n","        # Run on fake signal\n","        fake_signal = netG((input_noise + prev_signal).detach(), prev_signal)\n","        out_D_fake = netD(fake_signal.detach())\n","        err_fake_D = out_D_fake.mean()\n","        del out_D_real, out_D_fake\n","        err_fake_D.backward(retain_graph=True)\n","        err_fake_D = err_fake_D.detach()\n","        if print_progress or plot_losses:\n","            err_fake_D_val = err_fake_D.item()\n","\n","        lambda_grad=0.01\n","        gradient_penalty = calc_gradient_penalty(run_mode, current_holes, netD, real_signal, fake_signal, lambda_grad,\n","                                                 grad_pen_alpha_vec[epoch_num], _grad_outputs, mask_ratio)\n","        gradient_penalty.backward()\n","        if print_progress or plot_losses:\n","            gradient_penalty_val = gradient_penalty.item()\n","        del gradient_penalty\n","\n","        optimizerD.step()\n","\n","        if plot_losses:\n","            v_err_real[epoch_num] = err_real_D_val\n","            v_err_fake[epoch_num] = err_fake_D_val\n","            v_gp[epoch_num] = gradient_penalty_val\n","\n","        #############################################\n","        # Update G by maximizing D(G(noise_signal)) #\n","        #############################################\n","        netG.zero_grad()\n","        output = netD(fake_signal)\n","        errG = -output.mean()\n","        del output\n","        errG.backward(retain_graph=True)\n","        errG = errG.detach()\n","        if print_progress or plot_losses:\n","            errG_val = errG.item()\n","        if scale_idx == 0:\n","            reconstructed_signal = netG((reconstruction_noise + prev_reconstructed_signal).detach(),\n","                                        prev_reconstructed_signal)\n","        else:\n","            reconstructed_signal = netG((reconstruction_noise + prev_reconstructed_signal).detach(),\n","                                        prev_reconstructed_signal)\n","        if alpha1 > 0:\n","            if run_mode == 'inpainting':\n","                rec_loss_t = alpha1 * torch.mean(\n","                    (real_signal[:, :, current_mask] - reconstructed_signal[:, :, current_mask]) ** 2)\n","            else:\n","                rec_loss_t = alpha1 * torch.mean((real_signal - reconstructed_signal) ** 2)\n","        else:\n","            rec_loss_t = 0\n","        if alpha2 > 0:\n","            multispec_loss_n_fft = (2048, 1024, 512)\n","            multispec_loss_hop_length = (240, 120, 50)\n","            multispec_loss_window_size = (1200, 600, 240)\n","            rec_loss_f = alpha2 * multi_scale_spectrogram_loss(multispec_loss_n_fft, multispec_loss_hop_length, multispec_loss_window_size,\n","                                                               current_holes, real_signal.permute(0, 2, 1),reconstructed_signal.permute(0, 2, 1))\n","        else:\n","            rec_loss_f = 0\n","        rec_loss = rec_loss_t + rec_loss_f\n","        rec_loss.backward(retain_graph=True)\n","        rec_loss = rec_loss.detach()\n","        if alpha1 > 0:\n","            rec_loss_t = rec_loss_t.detach()\n","        if alpha2 > 0:\n","            rec_loss_f = rec_loss_f.detach()\n","        if print_progress or plot_losses:\n","            rec_loss_val = rec_loss.item()\n","\n","        optimizerG.step()\n","\n","        if plot_losses:\n","            v_rec_loss[epoch_num] = rec_loss_val\n","\n","        if print_progress:\n","            print('[%d/%d] D(real): %.2f. D(fake): %.2f. rec_loss: %.4f. gp: %.4f ' % (\n","                epoch_num, num_epochs, -err_real_D_val, err_fake_D_val, rec_loss_val, gradient_penalty_val))\n","\n","        schedulerD.step()\n","        schedulerG.step()\n","\n","        # Some memory cleanup\n","        fake_signal = fake_signal.detach()\n","        reconstructed_signal = reconstructed_signal.detach()\n","        if epoch_num < num_epochs - 1:\n","            del fake_signal, reconstructed_signal, rec_loss, rec_loss_t, rec_loss_f\n","        del noise_signal, input_noise\n","        if scale_idx > 0:\n","            del prev_signal\n","\n","    epochs_stop_time = time.time()\n","    runtime_msg = 'Total time in scale %d: %d[sec] (%.2f[sec]/epoch on avg.). D(real): %f, D(fake): %f, rec_loss: %.4f. gp: %.4f' % (\n","        current_scale, epochs_stop_time - epochs_start_time,\n","        (epochs_stop_time - epochs_start_time) / num_epochs,\n","        -err_real_D_val, err_fake_D_val, rec_loss_val, gradient_penalty_val)\n","    print(runtime_msg)\n","    with open(os.path.join(output_folder, 'log.txt'), 'a') as f:\n","        f.write('\\n%s\\n' % runtime_msg)\n","\n","    # Save this scale models\n","    torch.save(netG.state_dict(), '%s/netGScale%d.pth' % (output_folder, scale_idx))\n","    torch.save(netD.state_dict(), '%s/netDScale%d.pth' % (output_folder, scale_idx))\n","    # Pack outputs\n","    if plot_losses:\n","        loss_vectors = {'v_err_real': v_err_real,\n","                        'v_err_fake': v_err_fake,\n","                        'v_rec_loss': v_rec_loss,\n","                        'v_gp': v_gp}\n","    else:\n","        loss_vectors = []\n","    fake_signal = fake_signal.detach().cpu().numpy()[:, 0, :]\n","    reconstructed_signal = reconstructed_signal.detach().cpu().numpy()[:, 0, :]\n","    output_signals = {'fake_signal': fake_signal, 'reconstructed_signal': reconstructed_signal}\n","    del fake_signal, real_signal, netD, _grad_outputs, grad_pen_alpha_vec, input_signal, reconstructed_signal, prev_reconstructed_signal, reconstruction_noise\n","    netG = reset_grads(netG, False)\n","    netG.eval()\n","    if is_cuda:\n","        torch.cuda.empty_cache()\n","    print('*' * 30 + ' Finished working on scale ' + str(current_scale) + ' ' + '*' * 30)\n","    return output_signals, loss_vectors, netG, reconstruction_noise_list, noise_amp"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uzV4gMXQZR92","outputId":"b6a4dcaf-6048-4994-f89d-0c6c7d3363a1","executionInfo":{"status":"ok","timestamp":1663961132105,"user_tz":-60,"elapsed":4054771,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Working on file: S1_UMG.wav\n","Writing results to outputs_2\n","\n","Running on cuda:0\n","Signal in scale 15 has 5112 samples, sample rate is 320[Hz].\n","Total receptive field is 6378[msec] (39.9% of input).\n","[0/3000] D(real): -0.00. D(fake): -0.00. rec_loss: 0.0150. gp: 0.0067 \n","[100/3000] D(real): 0.03. D(fake): -0.02. rec_loss: 0.0120. gp: 0.0065 \n","[200/3000] D(real): 0.03. D(fake): -0.06. rec_loss: 0.0120. gp: 0.0100 \n","[300/3000] D(real): 0.05. D(fake): -0.09. rec_loss: 0.0124. gp: 0.1572 \n","[400/3000] D(real): 0.01. D(fake): -0.10. rec_loss: 0.0123. gp: 0.0202 \n","[500/3000] D(real): 0.05. D(fake): -0.10. rec_loss: 0.0136. gp: 0.0152 \n","[600/3000] D(real): 0.02. D(fake): -0.08. rec_loss: 0.0137. gp: 0.0320 \n","[700/3000] D(real): 0.09. D(fake): -0.12. rec_loss: 0.0137. gp: 0.0128 \n","[800/3000] D(real): 0.09. D(fake): -0.18. rec_loss: 0.0136. gp: 0.0891 \n","[900/3000] D(real): 0.10. D(fake): -0.19. rec_loss: 0.0132. gp: 0.0799 \n","[1000/3000] D(real): 0.13. D(fake): -0.14. rec_loss: 0.0141. gp: 0.1404 \n","[1100/3000] D(real): 0.08. D(fake): -0.21. rec_loss: 0.0136. gp: 0.3088 \n","[1200/3000] D(real): -0.10. D(fake): -0.22. rec_loss: 0.0134. gp: 0.0232 \n","[1300/3000] D(real): 0.19. D(fake): -0.15. rec_loss: 0.0141. gp: 0.2439 \n","[1400/3000] D(real): 0.18. D(fake): -0.13. rec_loss: 0.0132. gp: 0.0235 \n","[1500/3000] D(real): -0.01. D(fake): -0.20. rec_loss: 0.0136. gp: 0.0630 \n","[1600/3000] D(real): 0.12. D(fake): -0.22. rec_loss: 0.0139. gp: 0.2171 \n","[1700/3000] D(real): 0.12. D(fake): -0.19. rec_loss: 0.0147. gp: 0.1489 \n","[1800/3000] D(real): 0.21. D(fake): -0.19. rec_loss: 0.0134. gp: 0.0375 \n","[1900/3000] D(real): 0.15. D(fake): -0.23. rec_loss: 0.0133. gp: 0.0627 \n","[2000/3000] D(real): 0.13. D(fake): -0.23. rec_loss: 0.0139. gp: 0.0912 \n","[2100/3000] D(real): 0.17. D(fake): -0.28. rec_loss: 0.0139. gp: 0.0494 \n","[2200/3000] D(real): 0.20. D(fake): -0.30. rec_loss: 0.0137. gp: 0.0307 \n","[2300/3000] D(real): 0.18. D(fake): -0.29. rec_loss: 0.0135. gp: 0.0177 \n","[2400/3000] D(real): 0.22. D(fake): -0.27. rec_loss: 0.0138. gp: 0.0353 \n","[2500/3000] D(real): 0.21. D(fake): -0.29. rec_loss: 0.0134. gp: 0.0925 \n","[2600/3000] D(real): 0.18. D(fake): -0.30. rec_loss: 0.0131. gp: 0.5588 \n","[2700/3000] D(real): 0.17. D(fake): -0.28. rec_loss: 0.0132. gp: 0.0893 \n","[2800/3000] D(real): 0.19. D(fake): -0.27. rec_loss: 0.0132. gp: 0.0164 \n","[2900/3000] D(real): 0.20. D(fake): -0.26. rec_loss: 0.0133. gp: 0.0724 \n","Total time in scale 15: 166[sec] (0.06[sec]/epoch on avg.). D(real): 0.195522, D(fake): -0.262655, rec_loss: 0.0133. gp: 0.0724\n","****************************** Finished working on scale 15 ******************************\n","Signal in scale 14 has 6389 samples, sample rate is 400[Hz].\n","Total receptive field is 5102[msec] (31.9% of input).\n","[0/3000] D(real): 0.01. D(fake): 0.01. rec_loss: 0.0209. gp: 0.0080 \n","[100/3000] D(real): 0.09. D(fake): 0.06. rec_loss: 0.0191. gp: 0.0186 \n","[200/3000] D(real): 0.09. D(fake): 0.02. rec_loss: 0.0199. gp: 0.0316 \n","[300/3000] D(real): 0.10. D(fake): 0.05. rec_loss: 0.0191. gp: 0.0091 \n","[400/3000] D(real): 0.09. D(fake): 0.02. rec_loss: 0.0190. gp: 0.0442 \n","[500/3000] D(real): 0.11. D(fake): -0.00. rec_loss: 0.0190. gp: 0.0703 \n","[600/3000] D(real): 0.06. D(fake): -0.05. rec_loss: 0.0191. gp: 0.1043 \n","[700/3000] D(real): 0.11. D(fake): -0.05. rec_loss: 0.0179. gp: 0.0476 \n","[800/3000] D(real): 0.13. D(fake): -0.01. rec_loss: 0.0187. gp: 0.0086 \n","[900/3000] D(real): 0.16. D(fake): 0.03. rec_loss: 0.0179. gp: 0.0220 \n","[1000/3000] D(real): 0.12. D(fake): -0.03. rec_loss: 0.0179. gp: 0.1110 \n","[1100/3000] D(real): 0.17. D(fake): 0.03. rec_loss: 0.0181. gp: 0.0293 \n","[1200/3000] D(real): 0.19. D(fake): 0.01. rec_loss: 0.0181. gp: 0.2456 \n","[1300/3000] D(real): 0.06. D(fake): -0.08. rec_loss: 0.0175. gp: 0.1368 \n","[1400/3000] D(real): -0.02. D(fake): -0.07. rec_loss: 0.0182. gp: 0.0083 \n","[1500/3000] D(real): 0.22. D(fake): 0.01. rec_loss: 0.0182. gp: 0.0771 \n","[1600/3000] D(real): 0.03. D(fake): -0.07. rec_loss: 0.0188. gp: 0.0108 \n","[1700/3000] D(real): 0.11. D(fake): -0.09. rec_loss: 0.0170. gp: 0.0642 \n","[1800/3000] D(real): 0.20. D(fake): -0.10. rec_loss: 0.0173. gp: 0.1879 \n","[1900/3000] D(real): 0.20. D(fake): -0.23. rec_loss: 0.0176. gp: 0.0794 \n","[2000/3000] D(real): 0.09. D(fake): -0.18. rec_loss: 0.0171. gp: 0.0972 \n","[2100/3000] D(real): 0.11. D(fake): -0.19. rec_loss: 0.0166. gp: 0.0445 \n","[2200/3000] D(real): 0.14. D(fake): -0.21. rec_loss: 0.0164. gp: 0.0182 \n","[2300/3000] D(real): 0.14. D(fake): -0.23. rec_loss: 0.0164. gp: 0.0751 \n","[2400/3000] D(real): 0.19. D(fake): -0.21. rec_loss: 0.0164. gp: 0.0632 \n","[2500/3000] D(real): 0.21. D(fake): -0.23. rec_loss: 0.0164. gp: 0.4638 \n","[2600/3000] D(real): 0.21. D(fake): -0.23. rec_loss: 0.0166. gp: 0.4687 \n","[2700/3000] D(real): 0.18. D(fake): -0.23. rec_loss: 0.0165. gp: 0.0269 \n","[2800/3000] D(real): 0.15. D(fake): -0.25. rec_loss: 0.0164. gp: 0.0956 \n","[2900/3000] D(real): 0.25. D(fake): -0.20. rec_loss: 0.0162. gp: 0.0601 \n","Total time in scale 14: 218[sec] (0.07[sec]/epoch on avg.). D(real): 0.254719, D(fake): -0.202201, rec_loss: 0.0162. gp: 0.0601\n","****************************** Finished working on scale 14 ******************************\n","Signal in scale 13 has 7987 samples, sample rate is 500[Hz].\n","Total receptive field is 4082[msec] (25.6% of input).\n","[0/3000] D(real): -0.24. D(fake): -0.23. rec_loss: 0.0257. gp: 0.1896 \n","[100/3000] D(real): -0.02. D(fake): -0.13. rec_loss: 0.0223. gp: 0.0186 \n","[200/3000] D(real): 0.11. D(fake): -0.10. rec_loss: 0.0244. gp: 0.1254 \n","[300/3000] D(real): -0.09. D(fake): -0.15. rec_loss: 0.0215. gp: 0.0258 \n","[400/3000] D(real): 0.03. D(fake): -0.18. rec_loss: 0.0211. gp: 0.0185 \n","[500/3000] D(real): 0.15. D(fake): -0.21. rec_loss: 0.0208. gp: 0.0546 \n","[600/3000] D(real): 0.11. D(fake): -0.16. rec_loss: 0.0207. gp: 0.0916 \n","[700/3000] D(real): 0.19. D(fake): -0.24. rec_loss: 0.0201. gp: 0.2880 \n","[800/3000] D(real): 0.26. D(fake): -0.13. rec_loss: 0.0201. gp: 0.0504 \n","[900/3000] D(real): -0.16. D(fake): -0.23. rec_loss: 0.0204. gp: 0.0234 \n","[1000/3000] D(real): 0.19. D(fake): -0.16. rec_loss: 0.0192. gp: 0.0289 \n","[1100/3000] D(real): 0.09. D(fake): -0.16. rec_loss: 0.0199. gp: 0.0504 \n","[1200/3000] D(real): 0.18. D(fake): -0.21. rec_loss: 0.0188. gp: 0.1291 \n","[1300/3000] D(real): 0.13. D(fake): -0.18. rec_loss: 0.0189. gp: 0.1349 \n","[1400/3000] D(real): 0.09. D(fake): -0.18. rec_loss: 0.0186. gp: 0.0336 \n","[1500/3000] D(real): -0.12. D(fake): -0.25. rec_loss: 0.0194. gp: 0.0093 \n","[1600/3000] D(real): 0.24. D(fake): -0.08. rec_loss: 0.0185. gp: 0.2527 \n","[1700/3000] D(real): 0.19. D(fake): -0.17. rec_loss: 0.0183. gp: 0.0794 \n","[1800/3000] D(real): 0.13. D(fake): -0.26. rec_loss: 0.0180. gp: 0.1398 \n","[1900/3000] D(real): -0.16. D(fake): -0.27. rec_loss: 0.0180. gp: 0.0128 \n","[2000/3000] D(real): 0.00. D(fake): -0.36. rec_loss: 0.0174. gp: 0.1438 \n","[2100/3000] D(real): 0.20. D(fake): -0.37. rec_loss: 0.0166. gp: 0.0284 \n","[2200/3000] D(real): 0.12. D(fake): -0.36. rec_loss: 0.0164. gp: 0.0408 \n","[2300/3000] D(real): 0.14. D(fake): -0.36. rec_loss: 0.0165. gp: 0.0422 \n","[2400/3000] D(real): 0.20. D(fake): -0.36. rec_loss: 0.0163. gp: 0.0834 \n","[2500/3000] D(real): 0.12. D(fake): -0.41. rec_loss: 0.0167. gp: 0.0262 \n","[2600/3000] D(real): 0.17. D(fake): -0.39. rec_loss: 0.0171. gp: 0.5323 \n","[2700/3000] D(real): 0.23. D(fake): -0.30. rec_loss: 0.0175. gp: 0.4709 \n","[2800/3000] D(real): 0.21. D(fake): -0.34. rec_loss: 0.0165. gp: 0.0886 \n","[2900/3000] D(real): 0.23. D(fake): -0.37. rec_loss: 0.0162. gp: 0.0688 \n","Total time in scale 13: 276[sec] (0.09[sec]/epoch on avg.). D(real): 0.233388, D(fake): -0.369915, rec_loss: 0.0162. gp: 0.0688\n","****************************** Finished working on scale 13 ******************************\n","Signal in scale 12 has 10223 samples, sample rate is 640[Hz].\n","Total receptive field is 3189[msec] (20.0% of input).\n","[0/3000] D(real): -0.37. D(fake): -0.38. rec_loss: 0.0268. gp: 0.1524 \n","[100/3000] D(real): -0.11. D(fake): -0.21. rec_loss: 0.0217. gp: 0.0329 \n","[200/3000] D(real): 0.02. D(fake): -0.21. rec_loss: 0.0213. gp: 0.0590 \n","[300/3000] D(real): -0.06. D(fake): -0.22. rec_loss: 0.0206. gp: 0.0243 \n","[400/3000] D(real): -0.24. D(fake): -0.31. rec_loss: 0.0204. gp: 0.0186 \n","[500/3000] D(real): 0.10. D(fake): -0.16. rec_loss: 0.0206. gp: 0.0620 \n","[600/3000] D(real): -0.07. D(fake): -0.28. rec_loss: 0.0220. gp: 0.1565 \n","[700/3000] D(real): -0.32. D(fake): -0.35. rec_loss: 0.0205. gp: 0.0329 \n","[800/3000] D(real): -0.14. D(fake): -0.24. rec_loss: 0.0193. gp: 0.0136 \n","[900/3000] D(real): -0.07. D(fake): -0.23. rec_loss: 0.0189. gp: 0.0328 \n","[1000/3000] D(real): -0.04. D(fake): -0.24. rec_loss: 0.0183. gp: 0.0395 \n","[1100/3000] D(real): 0.05. D(fake): -0.22. rec_loss: 0.0185. gp: 0.0597 \n","[1200/3000] D(real): -0.03. D(fake): -0.25. rec_loss: 0.0180. gp: 0.0711 \n","[1300/3000] D(real): -0.09. D(fake): -0.23. rec_loss: 0.0177. gp: 0.0826 \n","[1400/3000] D(real): -0.23. D(fake): -0.33. rec_loss: 0.0174. gp: 0.0315 \n","[1500/3000] D(real): -0.01. D(fake): -0.26. rec_loss: 0.0173. gp: 0.0438 \n","[1600/3000] D(real): -0.28. D(fake): -0.44. rec_loss: 0.0177. gp: 0.1265 \n","[1700/3000] D(real): -0.15. D(fake): -0.20. rec_loss: 0.0189. gp: 0.0365 \n","[1800/3000] D(real): 0.03. D(fake): -0.31. rec_loss: 0.0170. gp: 0.4803 \n","[1900/3000] D(real): -0.30. D(fake): -0.35. rec_loss: 0.0173. gp: 0.0143 \n","[2000/3000] D(real): -0.02. D(fake): -0.30. rec_loss: 0.0160. gp: 0.1844 \n","[2100/3000] D(real): 0.08. D(fake): -0.30. rec_loss: 0.0156. gp: 0.0566 \n","[2200/3000] D(real): 0.09. D(fake): -0.33. rec_loss: 0.0156. gp: 0.4168 \n","[2300/3000] D(real): 0.08. D(fake): -0.32. rec_loss: 0.0154. gp: 0.0405 \n","[2400/3000] D(real): 0.08. D(fake): -0.34. rec_loss: 0.0153. gp: 0.0905 \n","[2500/3000] D(real): 0.11. D(fake): -0.38. rec_loss: 0.0156. gp: 0.2429 \n","[2600/3000] D(real): -0.03. D(fake): -0.40. rec_loss: 0.0154. gp: 0.1550 \n","[2700/3000] D(real): 0.03. D(fake): -0.39. rec_loss: 0.0151. gp: 0.0401 \n","[2800/3000] D(real): 0.09. D(fake): -0.42. rec_loss: 0.0150. gp: 0.1358 \n","[2900/3000] D(real): 0.02. D(fake): -0.36. rec_loss: 0.0157. gp: 0.2222 \n","Total time in scale 12: 346[sec] (0.12[sec]/epoch on avg.). D(real): 0.015527, D(fake): -0.359310, rec_loss: 0.0157. gp: 0.2222\n","****************************** Finished working on scale 12 ******************************\n","Signal in scale 11 has 12778 samples, sample rate is 800[Hz].\n","Total receptive field is 2551[msec] (16.0% of input).\n","[0/3000] D(real): -0.39. D(fake): -0.41. rec_loss: 0.0295. gp: 0.3911 \n","[100/3000] D(real): -0.21. D(fake): -0.34. rec_loss: 0.0197. gp: 0.0414 \n","[200/3000] D(real): -0.14. D(fake): -0.24. rec_loss: 0.0187. gp: 0.0258 \n","[300/3000] D(real): -0.00. D(fake): -0.20. rec_loss: 0.0192. gp: 0.0709 \n","[400/3000] D(real): 0.05. D(fake): -0.16. rec_loss: 0.0179. gp: 0.0291 \n","[500/3000] D(real): 0.11. D(fake): -0.07. rec_loss: 0.0182. gp: 0.0364 \n","[600/3000] D(real): 0.11. D(fake): -0.02. rec_loss: 0.0197. gp: 0.0623 \n","[700/3000] D(real): 0.26. D(fake): -0.02. rec_loss: 0.0180. gp: 0.1087 \n","[800/3000] D(real): 0.18. D(fake): 0.03. rec_loss: 0.0180. gp: 0.0443 \n","[900/3000] D(real): 0.13. D(fake): 0.07. rec_loss: 0.0168. gp: 0.0274 \n","[1000/3000] D(real): 0.29. D(fake): 0.16. rec_loss: 0.0165. gp: 0.0174 \n","[1100/3000] D(real): 0.35. D(fake): 0.14. rec_loss: 0.0163. gp: 0.0568 \n","[1200/3000] D(real): 0.43. D(fake): 0.22. rec_loss: 0.0169. gp: 0.0454 \n","[1300/3000] D(real): 0.33. D(fake): 0.07. rec_loss: 0.0165. gp: 0.0460 \n","[1400/3000] D(real): 0.36. D(fake): 0.06. rec_loss: 0.0165. gp: 0.0742 \n","[1500/3000] D(real): 0.39. D(fake): 0.29. rec_loss: 0.0170. gp: 0.0847 \n","[1600/3000] D(real): 0.38. D(fake): 0.09. rec_loss: 0.0172. gp: 0.1284 \n","[1700/3000] D(real): 0.45. D(fake): 0.15. rec_loss: 0.0169. gp: 0.1365 \n","[1800/3000] D(real): 0.38. D(fake): 0.20. rec_loss: 0.0157. gp: 0.0367 \n","[1900/3000] D(real): 0.48. D(fake): 0.23. rec_loss: 0.0167. gp: 0.0466 \n","[2000/3000] D(real): 0.35. D(fake): 0.15. rec_loss: 0.0165. gp: 0.0773 \n","[2100/3000] D(real): 0.51. D(fake): 0.17. rec_loss: 0.0157. gp: 0.0463 \n","[2200/3000] D(real): 0.57. D(fake): 0.16. rec_loss: 0.0163. gp: 0.0743 \n","[2300/3000] D(real): 0.47. D(fake): 0.17. rec_loss: 0.0159. gp: 0.1844 \n","[2400/3000] D(real): 0.52. D(fake): 0.17. rec_loss: 0.0155. gp: 0.0410 \n","[2500/3000] D(real): 0.58. D(fake): 0.20. rec_loss: 0.0171. gp: 0.0479 \n","[2600/3000] D(real): 0.54. D(fake): 0.20. rec_loss: 0.0151. gp: 0.0461 \n","[2700/3000] D(real): 0.58. D(fake): 0.16. rec_loss: 0.0151. gp: 0.0649 \n","[2800/3000] D(real): 0.55. D(fake): 0.12. rec_loss: 0.0157. gp: 0.1548 \n","[2900/3000] D(real): 0.50. D(fake): 0.14. rec_loss: 0.0157. gp: 0.1310 \n","Total time in scale 11: 438[sec] (0.15[sec]/epoch on avg.). D(real): 0.501641, D(fake): 0.137848, rec_loss: 0.0157. gp: 0.1310\n","****************************** Finished working on scale 11 ******************************\n","Signal in scale 10 has 15973 samples, sample rate is 1000[Hz].\n","Total receptive field is 2041[msec] (12.8% of input).\n","[0/3000] D(real): 0.07. D(fake): 0.08. rec_loss: 0.0318. gp: 0.1571 \n","[100/3000] D(real): 0.21. D(fake): 0.12. rec_loss: 0.0205. gp: 0.0240 \n","[200/3000] D(real): 0.25. D(fake): 0.18. rec_loss: 0.0186. gp: 0.0061 \n","[300/3000] D(real): 0.41. D(fake): 0.27. rec_loss: 0.0184. gp: 0.1356 \n","[400/3000] D(real): 0.28. D(fake): 0.25. rec_loss: 0.0178. gp: 0.0085 \n","[500/3000] D(real): 0.52. D(fake): 0.41. rec_loss: 0.0170. gp: 0.0272 \n","[600/3000] D(real): 0.52. D(fake): 0.39. rec_loss: 0.0177. gp: 0.0282 \n","[700/3000] D(real): 0.35. D(fake): 0.30. rec_loss: 0.0172. gp: 0.0214 \n","[800/3000] D(real): 0.68. D(fake): 0.42. rec_loss: 0.0169. gp: 0.0549 \n","[900/3000] D(real): 0.52. D(fake): 0.30. rec_loss: 0.0175. gp: 0.0515 \n","[1000/3000] D(real): 0.61. D(fake): 0.38. rec_loss: 0.0166. gp: 0.0396 \n","[1100/3000] D(real): 0.55. D(fake): 0.41. rec_loss: 0.0162. gp: 0.1006 \n","[1200/3000] D(real): 0.58. D(fake): 0.43. rec_loss: 0.0166. gp: 0.0258 \n","[1300/3000] D(real): 0.59. D(fake): 0.33. rec_loss: 0.0167. gp: 0.0386 \n","[1400/3000] D(real): 0.38. D(fake): 0.33. rec_loss: 0.0170. gp: 0.0132 \n","[1500/3000] D(real): 0.72. D(fake): 0.44. rec_loss: 0.0164. gp: 0.1339 \n","[1600/3000] D(real): 0.46. D(fake): 0.40. rec_loss: 0.0168. gp: 0.0162 \n","[1700/3000] D(real): 0.52. D(fake): 0.44. rec_loss: 0.0159. gp: 0.0378 \n","[1800/3000] D(real): 0.79. D(fake): 0.40. rec_loss: 0.0185. gp: 0.4230 \n","[1900/3000] D(real): 0.56. D(fake): 0.43. rec_loss: 0.0154. gp: 0.0569 \n","[2000/3000] D(real): 0.48. D(fake): 0.43. rec_loss: 0.0155. gp: 0.0170 \n","[2100/3000] D(real): 0.51. D(fake): 0.45. rec_loss: 0.0151. gp: 0.0225 \n","[2200/3000] D(real): 0.53. D(fake): 0.46. rec_loss: 0.0150. gp: 0.0207 \n","[2300/3000] D(real): 0.54. D(fake): 0.47. rec_loss: 0.0149. gp: 0.0114 \n","[2400/3000] D(real): 0.55. D(fake): 0.47. rec_loss: 0.0148. gp: 0.0489 \n","[2500/3000] D(real): 0.58. D(fake): 0.50. rec_loss: 0.0147. gp: 0.0304 \n","[2600/3000] D(real): 0.62. D(fake): 0.50. rec_loss: 0.0146. gp: 0.0395 \n","[2700/3000] D(real): 0.64. D(fake): 0.50. rec_loss: 0.0147. gp: 0.0253 \n","[2800/3000] D(real): 0.68. D(fake): 0.54. rec_loss: 0.0146. gp: 0.0219 \n","[2900/3000] D(real): 0.70. D(fake): 0.54. rec_loss: 0.0145. gp: 0.0320 \n","Total time in scale 10: 558[sec] (0.19[sec]/epoch on avg.). D(real): 0.696608, D(fake): 0.543580, rec_loss: 0.0145. gp: 0.0320\n","****************************** Finished working on scale 10 ******************************\n","Signal in scale 9 has 20445 samples, sample rate is 1280[Hz].\n","Total receptive field is 1594[msec] (10.0% of input).\n","[0/3000] D(real): 0.49. D(fake): 0.50. rec_loss: 0.0354. gp: 0.3227 \n","[100/3000] D(real): 0.61. D(fake): 0.59. rec_loss: 0.0193. gp: 0.0079 \n","[200/3000] D(real): 0.80. D(fake): 0.73. rec_loss: 0.0188. gp: 0.0371 \n","[300/3000] D(real): 0.97. D(fake): 0.88. rec_loss: 0.0186. gp: 0.0296 \n","[400/3000] D(real): 1.00. D(fake): 0.86. rec_loss: 0.0185. gp: 0.0352 \n","[500/3000] D(real): 1.11. D(fake): 0.97. rec_loss: 0.0180. gp: 0.0648 \n","[600/3000] D(real): 1.22. D(fake): 1.06. rec_loss: 0.0186. gp: 0.1109 \n","[700/3000] D(real): 1.34. D(fake): 1.14. rec_loss: 0.0182. gp: 0.1163 \n","[800/3000] D(real): 1.43. D(fake): 1.27. rec_loss: 0.0182. gp: 0.0269 \n","[900/3000] D(real): 1.30. D(fake): 1.24. rec_loss: 0.0177. gp: 0.0223 \n","[1000/3000] D(real): 1.65. D(fake): 1.40. rec_loss: 0.0178. gp: 0.0970 \n","[1100/3000] D(real): 1.57. D(fake): 1.32. rec_loss: 0.0199. gp: 0.1224 \n","[1200/3000] D(real): 1.60. D(fake): 1.36. rec_loss: 0.0178. gp: 0.1688 \n","[1300/3000] D(real): 1.41. D(fake): 1.31. rec_loss: 0.0183. gp: 0.0405 \n","[1400/3000] D(real): 1.56. D(fake): 1.33. rec_loss: 0.0178. gp: 0.1251 \n","[1500/3000] D(real): 1.39. D(fake): 1.28. rec_loss: 0.0175. gp: 0.0232 \n","[1600/3000] D(real): 1.31. D(fake): 1.22. rec_loss: 0.0175. gp: 0.0147 \n","[1700/3000] D(real): 1.70. D(fake): 1.35. rec_loss: 0.0174. gp: 0.0651 \n","[1800/3000] D(real): 1.65. D(fake): 1.36. rec_loss: 0.0186. gp: 0.1088 \n","[1900/3000] D(real): 1.45. D(fake): 1.39. rec_loss: 0.0172. gp: 0.0077 \n","[2000/3000] D(real): 1.41. D(fake): 1.35. rec_loss: 0.0172. gp: 0.0090 \n","[2100/3000] D(real): 1.45. D(fake): 1.38. rec_loss: 0.0169. gp: 0.0263 \n","[2200/3000] D(real): 1.46. D(fake): 1.37. rec_loss: 0.0167. gp: 0.0069 \n","[2300/3000] D(real): 1.49. D(fake): 1.37. rec_loss: 0.0166. gp: 0.0274 \n","[2400/3000] D(real): 1.54. D(fake): 1.38. rec_loss: 0.0165. gp: 0.0290 \n","[2500/3000] D(real): 1.59. D(fake): 1.44. rec_loss: 0.0165. gp: 0.1102 \n","[2600/3000] D(real): 1.62. D(fake): 1.41. rec_loss: 0.0165. gp: 0.0847 \n","[2700/3000] D(real): 1.66. D(fake): 1.44. rec_loss: 0.0165. gp: 0.1748 \n","[2800/3000] D(real): 1.70. D(fake): 1.42. rec_loss: 0.0166. gp: 0.0715 \n","[2900/3000] D(real): 1.73. D(fake): 1.44. rec_loss: 0.0167. gp: 0.1449 \n","Total time in scale 9: 720[sec] (0.24[sec]/epoch on avg.). D(real): 1.725112, D(fake): 1.440054, rec_loss: 0.0167. gp: 0.1449\n","****************************** Finished working on scale 9 ******************************\n","Signal in scale 8 has 25556 samples, sample rate is 1600[Hz].\n","Total receptive field is 1275[msec] (8.0% of input).\n","[0/3000] D(real): 1.31. D(fake): 1.33. rec_loss: 0.0400. gp: 0.1355 \n","[100/3000] D(real): 1.49. D(fake): 1.42. rec_loss: 0.0220. gp: 0.0208 \n","[200/3000] D(real): 1.77. D(fake): 1.68. rec_loss: 0.0216. gp: 0.0334 \n","[300/3000] D(real): 2.11. D(fake): 2.03. rec_loss: 0.0217. gp: 0.0256 \n","[400/3000] D(real): 2.33. D(fake): 2.22. rec_loss: 0.0214. gp: 0.0672 \n","[500/3000] D(real): 2.29. D(fake): 2.18. rec_loss: 0.0218. gp: 0.0315 \n","[600/3000] D(real): 2.31. D(fake): 2.14. rec_loss: 0.0224. gp: 0.0997 \n","[700/3000] D(real): 2.33. D(fake): 2.24. rec_loss: 0.0221. gp: 0.0131 \n","[800/3000] D(real): 2.29. D(fake): 2.15. rec_loss: 0.0221. gp: 0.0531 \n","[900/3000] D(real): 2.25. D(fake): 2.07. rec_loss: 0.0235. gp: 0.0411 \n","[1000/3000] D(real): 2.37. D(fake): 2.20. rec_loss: 0.0231. gp: 0.0508 \n","[1100/3000] D(real): 2.37. D(fake): 2.32. rec_loss: 0.0215. gp: 0.0312 \n","[1200/3000] D(real): 2.50. D(fake): 2.32. rec_loss: 0.0219. gp: 0.1132 \n","[1300/3000] D(real): 2.48. D(fake): 2.30. rec_loss: 0.0216. gp: 0.0449 \n","[1400/3000] D(real): 2.22. D(fake): 2.17. rec_loss: 0.0215. gp: 0.0236 \n","[1500/3000] D(real): 2.37. D(fake): 2.16. rec_loss: 0.0220. gp: 0.0605 \n","[1600/3000] D(real): 2.53. D(fake): 2.35. rec_loss: 0.0212. gp: 0.0567 \n","[1700/3000] D(real): 2.60. D(fake): 2.41. rec_loss: 0.0210. gp: 0.1027 \n","[1800/3000] D(real): 2.57. D(fake): 2.40. rec_loss: 0.0219. gp: 0.1553 \n","[1900/3000] D(real): 2.10. D(fake): 2.07. rec_loss: 0.0213. gp: 0.0088 \n","[2000/3000] D(real): 2.35. D(fake): 2.25. rec_loss: 0.0205. gp: 0.0164 \n","[2100/3000] D(real): 2.43. D(fake): 2.30. rec_loss: 0.0201. gp: 0.0464 \n","[2200/3000] D(real): 2.50. D(fake): 2.36. rec_loss: 0.0201. gp: 0.0396 \n","[2300/3000] D(real): 2.56. D(fake): 2.40. rec_loss: 0.0200. gp: 0.0403 \n","[2400/3000] D(real): 2.61. D(fake): 2.43. rec_loss: 0.0201. gp: 0.0505 \n","[2500/3000] D(real): 2.67. D(fake): 2.50. rec_loss: 0.0202. gp: 0.0830 \n","[2600/3000] D(real): 2.73. D(fake): 2.54. rec_loss: 0.0204. gp: 0.0793 \n","[2700/3000] D(real): 2.77. D(fake): 2.57. rec_loss: 0.0201. gp: 0.0615 \n","[2800/3000] D(real): 2.80. D(fake): 2.59. rec_loss: 0.0201. gp: 0.0987 \n","[2900/3000] D(real): 2.83. D(fake): 2.61. rec_loss: 0.0205. gp: 0.0630 \n","Total time in scale 8: 877[sec] (0.29[sec]/epoch on avg.). D(real): 2.833624, D(fake): 2.613645, rec_loss: 0.0205. gp: 0.0630\n","****************************** Finished working on scale 8 ******************************\n","Signal in scale 7 has 31945 samples, sample rate is 2000[Hz].\n","Total receptive field is 1020[msec] (6.4% of input).\n","[0/3000] D(real): 2.48. D(fake): 2.51. rec_loss: 0.0397. gp: 0.0809 \n","[100/3000] D(real): 1.81. D(fake): 1.78. rec_loss: 0.0271. gp: 0.0113 \n","[200/3000] D(real): 1.62. D(fake): 1.57. rec_loss: 0.0262. gp: 0.0129 \n","[300/3000] D(real): 1.68. D(fake): 1.62. rec_loss: 0.0257. gp: 0.0160 \n","[400/3000] D(real): 1.67. D(fake): 1.60. rec_loss: 0.0254. gp: 0.0413 \n","[500/3000] D(real): 1.86. D(fake): 1.81. rec_loss: 0.0251. gp: 0.0304 \n","[600/3000] D(real): 2.01. D(fake): 1.90. rec_loss: 0.0251. gp: 0.0475 \n","[700/3000] D(real): 1.90. D(fake): 1.87. rec_loss: 0.0249. gp: 0.0355 \n","[800/3000] D(real): 2.02. D(fake): 1.94. rec_loss: 0.0242. gp: 0.0226 \n","[900/3000] D(real): 2.17. D(fake): 2.05. rec_loss: 0.0246. gp: 0.0339 \n","[1000/3000] D(real): 2.01. D(fake): 1.90. rec_loss: 0.0241. gp: 0.0484 \n","[1100/3000] D(real): 2.17. D(fake): 2.07. rec_loss: 0.0241. gp: 0.0327 \n","[1200/3000] D(real): 2.38. D(fake): 2.23. rec_loss: 0.0242. gp: 0.2128 \n","[1300/3000] D(real): 2.40. D(fake): 2.30. rec_loss: 0.0247. gp: 0.0315 \n","[1400/3000] D(real): 2.45. D(fake): 2.35. rec_loss: 0.0237. gp: 0.0293 \n","[1500/3000] D(real): 2.57. D(fake): 2.46. rec_loss: 0.0241. gp: 0.0275 \n","[1600/3000] D(real): 2.62. D(fake): 2.44. rec_loss: 0.0254. gp: 0.0950 \n","[1700/3000] D(real): 1.95. D(fake): 1.83. rec_loss: 0.0229. gp: 0.0264 \n","[1800/3000] D(real): 2.29. D(fake): 2.11. rec_loss: 0.0230. gp: 0.0737 \n","[1900/3000] D(real): 2.40. D(fake): 2.25. rec_loss: 0.0225. gp: 0.0436 \n","[2000/3000] D(real): 2.46. D(fake): 2.34. rec_loss: 0.0226. gp: 0.0492 \n","[2100/3000] D(real): 2.63. D(fake): 2.46. rec_loss: 0.0221. gp: 0.0370 \n","[2200/3000] D(real): 2.70. D(fake): 2.49. rec_loss: 0.0223. gp: 0.0851 \n","[2300/3000] D(real): 2.77. D(fake): 2.58. rec_loss: 0.0223. gp: 0.1225 \n","[2400/3000] D(real): 2.81. D(fake): 2.59. rec_loss: 0.0220. gp: 0.0541 \n","[2500/3000] D(real): 2.85. D(fake): 2.64. rec_loss: 0.0229. gp: 0.0403 \n","[2600/3000] D(real): 2.81. D(fake): 2.63. rec_loss: 0.0228. gp: 0.0413 \n","[2700/3000] D(real): 2.87. D(fake): 2.66. rec_loss: 0.0221. gp: 0.0454 \n","[2800/3000] D(real): 2.90. D(fake): 2.71. rec_loss: 0.0220. gp: 0.0644 \n","[2900/3000] D(real): 2.91. D(fake): 2.73. rec_loss: 0.0218. gp: 0.0707 \n","Total time in scale 7: 1106[sec] (0.37[sec]/epoch on avg.). D(real): 2.911575, D(fake): 2.731956, rec_loss: 0.0218. gp: 0.0707\n","****************************** Finished working on scale 7 ******************************\n","Signal in scale 6 has 39931 samples, sample rate is 2500[Hz].\n","Total receptive field is 816[msec] (5.1% of input).\n","[0/3000] D(real): 2.63. D(fake): 2.66. rec_loss: 0.0449. gp: 0.1407 \n","[100/3000] D(real): 2.22. D(fake): 2.19. rec_loss: 0.0287. gp: 0.0067 \n","[200/3000] D(real): 1.83. D(fake): 1.79. rec_loss: 0.0277. gp: 0.0129 \n","[300/3000] D(real): 1.60. D(fake): 1.56. rec_loss: 0.0267. gp: 0.0227 \n","[400/3000] D(real): 1.33. D(fake): 1.32. rec_loss: 0.0234. gp: 0.0052 \n","[500/3000] D(real): 1.54. D(fake): 1.49. rec_loss: 0.0228. gp: 0.0307 \n","[600/3000] D(real): 1.70. D(fake): 1.65. rec_loss: 0.0220. gp: 0.0155 \n","[700/3000] D(real): 1.79. D(fake): 1.71. rec_loss: 0.0210. gp: 0.0219 \n","[800/3000] D(real): 1.80. D(fake): 1.71. rec_loss: 0.0205. gp: 0.0412 \n","[900/3000] D(real): 1.74. D(fake): 1.64. rec_loss: 0.0192. gp: 0.0429 \n","[1000/3000] D(real): 1.94. D(fake): 1.83. rec_loss: 0.0194. gp: 0.0310 \n","[1100/3000] D(real): 2.01. D(fake): 1.89. rec_loss: 0.0178. gp: 0.0581 \n","[1200/3000] D(real): 1.93. D(fake): 1.83. rec_loss: 0.0177. gp: 0.0266 \n","[1300/3000] D(real): 2.10. D(fake): 1.99. rec_loss: 0.0173. gp: 0.0226 \n","[1400/3000] D(real): 2.05. D(fake): 2.03. rec_loss: 0.0172. gp: 0.0188 \n","[1500/3000] D(real): 2.17. D(fake): 2.03. rec_loss: 0.0197. gp: 0.1299 \n","[1600/3000] D(real): 2.23. D(fake): 2.07. rec_loss: 0.0172. gp: 0.0927 \n","[1700/3000] D(real): 2.05. D(fake): 1.93. rec_loss: 0.0170. gp: 0.0520 \n","[1800/3000] D(real): 2.32. D(fake): 2.16. rec_loss: 0.0164. gp: 0.0456 \n","[1900/3000] D(real): 2.26. D(fake): 2.21. rec_loss: 0.0171. gp: 0.0398 \n","[2000/3000] D(real): 2.29. D(fake): 2.17. rec_loss: 0.0174. gp: 0.0244 \n","[2100/3000] D(real): 2.32. D(fake): 2.14. rec_loss: 0.0154. gp: 0.0592 \n","[2200/3000] D(real): 2.40. D(fake): 2.22. rec_loss: 0.0152. gp: 0.0841 \n","[2300/3000] D(real): 2.44. D(fake): 2.25. rec_loss: 0.0149. gp: 0.0329 \n","[2400/3000] D(real): 2.49. D(fake): 2.29. rec_loss: 0.0152. gp: 0.0525 \n","[2500/3000] D(real): 2.53. D(fake): 2.33. rec_loss: 0.0168. gp: 0.0357 \n","[2600/3000] D(real): 2.50. D(fake): 2.30. rec_loss: 0.0148. gp: 0.0733 \n","[2700/3000] D(real): 2.52. D(fake): 2.31. rec_loss: 0.0150. gp: 0.2043 \n","[2800/3000] D(real): 2.58. D(fake): 2.37. rec_loss: 0.0158. gp: 0.0440 \n","[2900/3000] D(real): 2.56. D(fake): 2.36. rec_loss: 0.0151. gp: 0.0654 \n","Total time in scale 6: 1350[sec] (0.45[sec]/epoch on avg.). D(real): 2.562003, D(fake): 2.357168, rec_loss: 0.0151. gp: 0.0654\n","****************************** Finished working on scale 6 ******************************\n","Signal in scale 5 has 63889 samples, sample rate is 4000[Hz].\n","Total receptive field is 510[msec] (3.2% of input).\n","[0/3000] D(real): 2.32. D(fake): 2.39. rec_loss: 0.0689. gp: 0.2013 \n","[100/3000] D(real): 1.74. D(fake): 1.73. rec_loss: 0.0267. gp: 0.0055 \n","[200/3000] D(real): 1.42. D(fake): 1.40. rec_loss: 0.0249. gp: 0.0071 \n","[300/3000] D(real): 1.03. D(fake): 1.00. rec_loss: 0.0239. gp: 0.0141 \n","[400/3000] D(real): 0.92. D(fake): 0.88. rec_loss: 0.0231. gp: 0.0179 \n","[500/3000] D(real): 0.84. D(fake): 0.81. rec_loss: 0.0222. gp: 0.0143 \n","[600/3000] D(real): 0.93. D(fake): 0.88. rec_loss: 0.0215. gp: 0.0107 \n","[700/3000] D(real): 1.06. D(fake): 1.01. rec_loss: 0.0214. gp: 0.0160 \n","[800/3000] D(real): 1.02. D(fake): 0.97. rec_loss: 0.0202. gp: 0.0162 \n","[900/3000] D(real): 1.16. D(fake): 1.10. rec_loss: 0.0202. gp: 0.0157 \n","[1000/3000] D(real): 1.15. D(fake): 1.09. rec_loss: 0.0197. gp: 0.0158 \n","[1100/3000] D(real): 1.12. D(fake): 1.06. rec_loss: 0.0197. gp: 0.0165 \n","[1200/3000] D(real): 1.25. D(fake): 1.18. rec_loss: 0.0193. gp: 0.0268 \n","[1300/3000] D(real): 1.26. D(fake): 1.23. rec_loss: 0.0191. gp: 0.0090 \n","[1400/3000] D(real): 1.30. D(fake): 1.22. rec_loss: 0.0186. gp: 0.0310 \n","[1500/3000] D(real): 1.38. D(fake): 1.30. rec_loss: 0.0184. gp: 0.0410 \n","[1600/3000] D(real): 1.31. D(fake): 1.24. rec_loss: 0.0183. gp: 0.0246 \n","[1700/3000] D(real): 1.35. D(fake): 1.28. rec_loss: 0.0186. gp: 0.0171 \n","[1800/3000] D(real): 1.53. D(fake): 1.44. rec_loss: 0.0178. gp: 0.0327 \n","[1900/3000] D(real): 1.49. D(fake): 1.38. rec_loss: 0.0176. gp: 0.0455 \n","[2000/3000] D(real): 1.56. D(fake): 1.45. rec_loss: 0.0177. gp: 0.0500 \n","[2100/3000] D(real): 1.64. D(fake): 1.54. rec_loss: 0.0166. gp: 0.0187 \n","[2200/3000] D(real): 1.71. D(fake): 1.58. rec_loss: 0.0165. gp: 0.0199 \n","[2300/3000] D(real): 1.73. D(fake): 1.62. rec_loss: 0.0164. gp: 0.0605 \n","[2400/3000] D(real): 1.77. D(fake): 1.65. rec_loss: 0.0164. gp: 0.0247 \n","[2500/3000] D(real): 1.78. D(fake): 1.67. rec_loss: 0.0163. gp: 0.0292 \n","[2600/3000] D(real): 1.79. D(fake): 1.65. rec_loss: 0.0166. gp: 0.0818 \n","[2700/3000] D(real): 1.81. D(fake): 1.69. rec_loss: 0.0164. gp: 0.0190 \n","[2800/3000] D(real): 1.86. D(fake): 1.73. rec_loss: 0.0163. gp: 0.0274 \n","[2900/3000] D(real): 1.87. D(fake): 1.75. rec_loss: 0.0163. gp: 0.0721 \n","Total time in scale 5: 2042[sec] (0.68[sec]/epoch on avg.). D(real): 1.873142, D(fake): 1.751620, rec_loss: 0.0163. gp: 0.0721\n","****************************** Finished working on scale 5 ******************************\n","Signal in scale 4 has 127778 samples, sample rate is 8000[Hz].\n","Total receptive field is 255[msec] (1.6% of input).\n","[0/3000] D(real): 1.69. D(fake): 1.71. rec_loss: 0.0611. gp: 0.0801 \n","[100/3000] D(real): 1.40. D(fake): 1.40. rec_loss: 0.0370. gp: 0.0045 \n","[200/3000] D(real): 1.08. D(fake): 1.07. rec_loss: 0.0349. gp: 0.0056 \n","[300/3000] D(real): 0.87. D(fake): 0.87. rec_loss: 0.0323. gp: 0.0042 \n","[400/3000] D(real): 0.84. D(fake): 0.82. rec_loss: 0.0313. gp: 0.0131 \n","[500/3000] D(real): 0.81. D(fake): 0.80. rec_loss: 0.0301. gp: 0.0062 \n","[600/3000] D(real): 0.78. D(fake): 0.74. rec_loss: 0.0298. gp: 0.0098 \n","[700/3000] D(real): 0.73. D(fake): 0.73. rec_loss: 0.0278. gp: 0.0049 \n","[800/3000] D(real): 0.77. D(fake): 0.73. rec_loss: 0.0274. gp: 0.0243 \n","[900/3000] D(real): 0.62. D(fake): 0.59. rec_loss: 0.0273. gp: 0.0201 \n","[1000/3000] D(real): 0.63. D(fake): 0.63. rec_loss: 0.0263. gp: 0.0053 \n","[1100/3000] D(real): 0.71. D(fake): 0.69. rec_loss: 0.0259. gp: 0.0090 \n","[1200/3000] D(real): 0.70. D(fake): 0.66. rec_loss: 0.0272. gp: 0.0129 \n","[1300/3000] D(real): 0.54. D(fake): 0.54. rec_loss: 0.0258. gp: 0.0082 \n","[1400/3000] D(real): 0.66. D(fake): 0.62. rec_loss: 0.0245. gp: 0.0148 \n","[1500/3000] D(real): 0.65. D(fake): 0.60. rec_loss: 0.0242. gp: 0.0149 \n","[1600/3000] D(real): 0.70. D(fake): 0.65. rec_loss: 0.0244. gp: 0.0102 \n","[1700/3000] D(real): 0.47. D(fake): 0.41. rec_loss: 0.0237. gp: 0.0121 \n","[1800/3000] D(real): 0.53. D(fake): 0.49. rec_loss: 0.0241. gp: 0.0106 \n","[1900/3000] D(real): 0.56. D(fake): 0.55. rec_loss: 0.0229. gp: 0.0086 \n","[2000/3000] D(real): 0.59. D(fake): 0.53. rec_loss: 0.0235. gp: 0.0136 \n","[2100/3000] D(real): 0.58. D(fake): 0.52. rec_loss: 0.0218. gp: 0.0331 \n","[2200/3000] D(real): 0.62. D(fake): 0.54. rec_loss: 0.0217. gp: 0.0135 \n","[2300/3000] D(real): 0.63. D(fake): 0.55. rec_loss: 0.0217. gp: 0.0435 \n","[2400/3000] D(real): 0.65. D(fake): 0.57. rec_loss: 0.0222. gp: 0.0655 \n","[2500/3000] D(real): 0.66. D(fake): 0.60. rec_loss: 0.0224. gp: 0.0324 \n","[2600/3000] D(real): 0.65. D(fake): 0.57. rec_loss: 0.0217. gp: 0.0131 \n","[2700/3000] D(real): 0.69. D(fake): 0.61. rec_loss: 0.0219. gp: 0.0139 \n","[2800/3000] D(real): 0.69. D(fake): 0.61. rec_loss: 0.0219. gp: 0.0286 \n","[2900/3000] D(real): 0.71. D(fake): 0.64. rec_loss: 0.0219. gp: 0.0097 \n","Total time in scale 4: 3710[sec] (1.24[sec]/epoch on avg.). D(real): 0.705863, D(fake): 0.644909, rec_loss: 0.0219. gp: 0.0097\n","****************************** Finished working on scale 4 ******************************\n","Signal in scale 3 has 159723 samples, sample rate is 10000[Hz].\n","Total receptive field is 204[msec] (1.3% of input).\n","[0/3000] D(real): 0.60. D(fake): 0.61. rec_loss: 0.0726. gp: 0.0222 \n","[100/3000] D(real): 0.20. D(fake): 0.18. rec_loss: 0.0331. gp: 0.0132 \n","[200/3000] D(real): 0.08. D(fake): 0.07. rec_loss: 0.0309. gp: 0.0103 \n","[300/3000] D(real): 0.05. D(fake): 0.02. rec_loss: 0.0297. gp: 0.0162 \n","[400/3000] D(real): -0.01. D(fake): -0.02. rec_loss: 0.0289. gp: 0.0041 \n","[500/3000] D(real): -0.03. D(fake): -0.04. rec_loss: 0.0284. gp: 0.0050 \n","[600/3000] D(real): -0.05. D(fake): -0.07. rec_loss: 0.0285. gp: 0.0271 \n","[700/3000] D(real): -0.10. D(fake): -0.11. rec_loss: 0.0280. gp: 0.0056 \n","[800/3000] D(real): -0.07. D(fake): -0.09. rec_loss: 0.0268. gp: 0.0091 \n","[900/3000] D(real): -0.10. D(fake): -0.12. rec_loss: 0.0270. gp: 0.0064 \n","[1000/3000] D(real): -0.08. D(fake): -0.13. rec_loss: 0.0264. gp: 0.0088 \n","[1100/3000] D(real): -0.12. D(fake): -0.13. rec_loss: 0.0260. gp: 0.0072 \n","[1200/3000] D(real): -0.12. D(fake): -0.13. rec_loss: 0.0262. gp: 0.0067 \n","[1300/3000] D(real): -0.07. D(fake): -0.11. rec_loss: 0.0264. gp: 0.0088 \n","[1400/3000] D(real): -0.07. D(fake): -0.10. rec_loss: 0.0255. gp: 0.0079 \n","[1500/3000] D(real): -0.04. D(fake): -0.04. rec_loss: 0.0255. gp: 0.0045 \n","[1600/3000] D(real): -0.09. D(fake): -0.12. rec_loss: 0.0250. gp: 0.0117 \n","[1700/3000] D(real): 0.03. D(fake): -0.02. rec_loss: 0.0250. gp: 0.0103 \n","[1800/3000] D(real): 0.07. D(fake): 0.03. rec_loss: 0.0248. gp: 0.0210 \n","[1900/3000] D(real): -0.03. D(fake): -0.08. rec_loss: 0.0258. gp: 0.0183 \n","[2000/3000] D(real): 0.07. D(fake): 0.01. rec_loss: 0.0250. gp: 0.0108 \n","[2100/3000] D(real): 0.10. D(fake): 0.03. rec_loss: 0.0239. gp: 0.0771 \n","[2200/3000] D(real): 0.14. D(fake): 0.08. rec_loss: 0.0239. gp: 0.0152 \n","[2300/3000] D(real): 0.16. D(fake): 0.10. rec_loss: 0.0238. gp: 0.0099 \n","[2400/3000] D(real): 0.21. D(fake): 0.14. rec_loss: 0.0239. gp: 0.0100 \n","[2500/3000] D(real): 0.21. D(fake): 0.15. rec_loss: 0.0238. gp: 0.0110 \n","[2600/3000] D(real): 0.22. D(fake): 0.15. rec_loss: 0.0238. gp: 0.0119 \n","[2700/3000] D(real): 0.23. D(fake): 0.15. rec_loss: 0.0238. gp: 0.0135 \n","[2800/3000] D(real): 0.25. D(fake): 0.17. rec_loss: 0.0239. gp: 0.0197 \n","[2900/3000] D(real): 0.25. D(fake): 0.17. rec_loss: 0.0238. gp: 0.0137 \n","Total time in scale 3: 4795[sec] (1.60[sec]/epoch on avg.). D(real): 0.254987, D(fake): 0.167956, rec_loss: 0.0238. gp: 0.0137\n","****************************** Finished working on scale 3 ******************************\n","Signal in scale 2 has 191667 samples, sample rate is 12000[Hz].\n","Total receptive field is 170[msec] (1.1% of input).\n","[0/3000] D(real): 0.15. D(fake): 0.16. rec_loss: 0.0471. gp: 0.0222 \n","[100/3000] D(real): 0.18. D(fake): 0.18. rec_loss: 0.0342. gp: 0.0042 \n","[200/3000] D(real): 0.13. D(fake): 0.12. rec_loss: 0.0333. gp: 0.0070 \n","[300/3000] D(real): 0.01. D(fake): 0.01. rec_loss: 0.0323. gp: 0.0051 \n","[400/3000] D(real): 0.05. D(fake): 0.03. rec_loss: 0.0314. gp: 0.0084 \n","[500/3000] D(real): 0.03. D(fake): 0.01. rec_loss: 0.0310. gp: 0.0238 \n","[600/3000] D(real): 0.03. D(fake): 0.01. rec_loss: 0.0305. gp: 0.0073 \n","[700/3000] D(real): 0.04. D(fake): 0.01. rec_loss: 0.0304. gp: 0.0092 \n","[800/3000] D(real): 0.06. D(fake): 0.04. rec_loss: 0.0300. gp: 0.0058 \n","[900/3000] D(real): 0.01. D(fake): -0.02. rec_loss: 0.0296. gp: 0.0070 \n","[1000/3000] D(real): 0.01. D(fake): -0.01. rec_loss: 0.0293. gp: 0.0078 \n","[1100/3000] D(real): 0.08. D(fake): 0.05. rec_loss: 0.0288. gp: 0.0097 \n","[1200/3000] D(real): 0.15. D(fake): 0.15. rec_loss: 0.0287. gp: 0.0058 \n","[1300/3000] D(real): 0.13. D(fake): 0.09. rec_loss: 0.0286. gp: 0.0312 \n","[1400/3000] D(real): 0.07. D(fake): 0.02. rec_loss: 0.0281. gp: 0.0255 \n","[1500/3000] D(real): 0.11. D(fake): 0.06. rec_loss: 0.0291. gp: 0.0172 \n","[1600/3000] D(real): 0.07. D(fake): 0.06. rec_loss: 0.0280. gp: 0.0055 \n","[1700/3000] D(real): 0.19. D(fake): 0.16. rec_loss: 0.0278. gp: 0.0154 \n","[1800/3000] D(real): 0.25. D(fake): 0.20. rec_loss: 0.0277. gp: 0.0102 \n","[1900/3000] D(real): 0.21. D(fake): 0.17. rec_loss: 0.0273. gp: 0.0198 \n","[2000/3000] D(real): 0.18. D(fake): 0.14. rec_loss: 0.0274. gp: 0.0375 \n","[2100/3000] D(real): 0.19. D(fake): 0.14. rec_loss: 0.0265. gp: 0.0206 \n","[2200/3000] D(real): 0.18. D(fake): 0.12. rec_loss: 0.0265. gp: 0.0085 \n","[2300/3000] D(real): 0.21. D(fake): 0.16. rec_loss: 0.0265. gp: 0.0092 \n","[2400/3000] D(real): 0.25. D(fake): 0.20. rec_loss: 0.0265. gp: 0.0087 \n","[2500/3000] D(real): 0.26. D(fake): 0.21. rec_loss: 0.0265. gp: 0.0075 \n","[2600/3000] D(real): 0.26. D(fake): 0.20. rec_loss: 0.0265. gp: 0.0085 \n","[2700/3000] D(real): 0.31. D(fake): 0.26. rec_loss: 0.0265. gp: 0.0238 \n","[2800/3000] D(real): 0.33. D(fake): 0.27. rec_loss: 0.0265. gp: 0.0075 \n","[2900/3000] D(real): 0.33. D(fake): 0.28. rec_loss: 0.0266. gp: 0.0091 \n","Total time in scale 2: 5831[sec] (1.94[sec]/epoch on avg.). D(real): 0.333510, D(fake): 0.282977, rec_loss: 0.0266. gp: 0.0091\n","****************************** Finished working on scale 2 ******************************\n","Signal in scale 1 has 230001 samples, sample rate is 14400[Hz].\n","Total receptive field is 141[msec] (0.9% of input).\n","[0/3000] D(real): 0.26. D(fake): 0.26. rec_loss: 0.0463. gp: 0.0276 \n","[100/3000] D(real): 0.19. D(fake): 0.18. rec_loss: 0.0373. gp: 0.0114 \n","[200/3000] D(real): 0.07. D(fake): 0.07. rec_loss: 0.0362. gp: 0.0041 \n","[300/3000] D(real): 0.02. D(fake): -0.00. rec_loss: 0.0355. gp: 0.0284 \n","[400/3000] D(real): -0.07. D(fake): -0.10. rec_loss: 0.0344. gp: 0.0089 \n","[500/3000] D(real): -0.06. D(fake): -0.08. rec_loss: 0.0337. gp: 0.0070 \n","[600/3000] D(real): -0.08. D(fake): -0.08. rec_loss: 0.0332. gp: 0.0045 \n","[700/3000] D(real): 0.01. D(fake): -0.02. rec_loss: 0.0331. gp: 0.0059 \n","[800/3000] D(real): -0.01. D(fake): -0.02. rec_loss: 0.0326. gp: 0.0054 \n","[900/3000] D(real): -0.05. D(fake): -0.07. rec_loss: 0.0323. gp: 0.0057 \n","[1000/3000] D(real): -0.01. D(fake): -0.03. rec_loss: 0.0320. gp: 0.0092 \n","[1100/3000] D(real): -0.11. D(fake): -0.12. rec_loss: 0.0316. gp: 0.0063 \n","[1200/3000] D(real): -0.03. D(fake): -0.04. rec_loss: 0.0313. gp: 0.0083 \n","[1300/3000] D(real): -0.02. D(fake): -0.06. rec_loss: 0.0313. gp: 0.0066 \n","[1400/3000] D(real): 0.00. D(fake): -0.04. rec_loss: 0.0310. gp: 0.0215 \n","[1500/3000] D(real): -0.01. D(fake): -0.03. rec_loss: 0.0307. gp: 0.0110 \n","[1600/3000] D(real): -0.01. D(fake): -0.06. rec_loss: 0.0307. gp: 0.0500 \n","[1700/3000] D(real): -0.05. D(fake): -0.09. rec_loss: 0.0304. gp: 0.0247 \n","[1800/3000] D(real): -0.01. D(fake): -0.02. rec_loss: 0.0301. gp: 0.0082 \n","[1900/3000] D(real): -0.07. D(fake): -0.11. rec_loss: 0.0304. gp: 0.0086 \n","[2000/3000] D(real): -0.05. D(fake): -0.08. rec_loss: 0.0298. gp: 0.0238 \n","[2100/3000] D(real): -0.07. D(fake): -0.11. rec_loss: 0.0292. gp: 0.0078 \n","[2200/3000] D(real): -0.05. D(fake): -0.10. rec_loss: 0.0291. gp: 0.0292 \n","[2300/3000] D(real): -0.05. D(fake): -0.09. rec_loss: 0.0290. gp: 0.0176 \n","[2400/3000] D(real): -0.05. D(fake): -0.09. rec_loss: 0.0291. gp: 0.0227 \n","[2500/3000] D(real): -0.02. D(fake): -0.08. rec_loss: 0.0290. gp: 0.0613 \n","[2600/3000] D(real): 0.02. D(fake): -0.03. rec_loss: 0.0290. gp: 0.0097 \n","[2700/3000] D(real): 0.03. D(fake): -0.03. rec_loss: 0.0290. gp: 0.0105 \n","[2800/3000] D(real): 0.04. D(fake): -0.03. rec_loss: 0.0290. gp: 0.0810 \n","[2900/3000] D(real): 0.06. D(fake): 0.00. rec_loss: 0.0290. gp: 0.0093 \n","Total time in scale 1: 7025[sec] (2.34[sec]/epoch on avg.). D(real): 0.064111, D(fake): 0.000043, rec_loss: 0.0290. gp: 0.0093\n","****************************** Finished working on scale 1 ******************************\n","Signal in scale 0 has 255556 samples, sample rate is 16000[Hz].\n","Total receptive field is 127[msec] (0.8% of input).\n","[0/3000] D(real): -0.00. D(fake): -0.03. rec_loss: 0.0479. gp: 0.0397 \n","[100/3000] D(real): -0.10. D(fake): -0.12. rec_loss: 0.0382. gp: 0.0126 \n","[200/3000] D(real): -0.08. D(fake): -0.09. rec_loss: 0.0372. gp: 0.0051 \n","[300/3000] D(real): -0.21. D(fake): -0.23. rec_loss: 0.0366. gp: 0.0061 \n","[400/3000] D(real): -0.23. D(fake): -0.23. rec_loss: 0.0360. gp: 0.0044 \n","[500/3000] D(real): -0.23. D(fake): -0.25. rec_loss: 0.0354. gp: 0.0167 \n","[600/3000] D(real): -0.14. D(fake): -0.15. rec_loss: 0.0352. gp: 0.0060 \n","[700/3000] D(real): -0.12. D(fake): -0.13. rec_loss: 0.0346. gp: 0.0063 \n","[800/3000] D(real): -0.23. D(fake): -0.26. rec_loss: 0.0341. gp: 0.0068 \n","[900/3000] D(real): -0.11. D(fake): -0.11. rec_loss: 0.0341. gp: 0.0048 \n","[1000/3000] D(real): -0.17. D(fake): -0.20. rec_loss: 0.0336. gp: 0.0125 \n","[1100/3000] D(real): -0.22. D(fake): -0.25. rec_loss: 0.0333. gp: 0.0057 \n","[1200/3000] D(real): -0.07. D(fake): -0.07. rec_loss: 0.0329. gp: 0.0044 \n","[1300/3000] D(real): -0.08. D(fake): -0.08. rec_loss: 0.0325. gp: 0.0045 \n","[1400/3000] D(real): -0.16. D(fake): -0.17. rec_loss: 0.0322. gp: 0.0065 \n","[1500/3000] D(real): -0.19. D(fake): -0.21. rec_loss: 0.0321. gp: 0.0063 \n","[1600/3000] D(real): -0.15. D(fake): -0.19. rec_loss: 0.0318. gp: 0.0406 \n","[1700/3000] D(real): -0.15. D(fake): -0.17. rec_loss: 0.0315. gp: 0.0056 \n","[1800/3000] D(real): -0.13. D(fake): -0.15. rec_loss: 0.0313. gp: 0.0120 \n","[1900/3000] D(real): -0.09. D(fake): -0.09. rec_loss: 0.0309. gp: 0.0048 \n","[2000/3000] D(real): -0.06. D(fake): -0.07. rec_loss: 0.0308. gp: 0.0090 \n","[2100/3000] D(real): -0.03. D(fake): -0.06. rec_loss: 0.0302. gp: 0.0074 \n","[2200/3000] D(real): -0.07. D(fake): -0.12. rec_loss: 0.0302. gp: 0.0081 \n","[2300/3000] D(real): -0.08. D(fake): -0.13. rec_loss: 0.0301. gp: 0.0288 \n","[2400/3000] D(real): -0.07. D(fake): -0.12. rec_loss: 0.0301. gp: 0.0069 \n","[2500/3000] D(real): -0.05. D(fake): -0.10. rec_loss: 0.0301. gp: 0.0191 \n","[2600/3000] D(real): -0.06. D(fake): -0.12. rec_loss: 0.0301. gp: 0.0070 \n","[2700/3000] D(real): -0.05. D(fake): -0.10. rec_loss: 0.0301. gp: 0.0231 \n","[2800/3000] D(real): -0.04. D(fake): -0.08. rec_loss: 0.0301. gp: 0.0154 \n","[2900/3000] D(real): -0.04. D(fake): -0.10. rec_loss: 0.0302. gp: 0.0185 \n","Total time in scale 0: 7803[sec] (2.60[sec]/epoch on avg.). D(real): -0.038661, D(fake): -0.101200, rec_loss: 0.0302. gp: 0.0185\n","****************************** Finished working on scale 0 ******************************\n"]}],"source":["#training\n","startTime = time.time()\n","\n","if len(inpainting_indices)%2 != 0:\n","    raise Exception('Provide START and END indices of each hole!')\n","\n","if is_cuda:\n","    torch.cuda.set_device(gpu_num)\n","    device = torch.device(\"cuda:%d\" % gpu_num)\n","\n","if manual_random_seed != -1:\n","    random.seed(manual_random_seed)\n","    torch.manual_seed(manual_random_seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","samples, Fs = get_input_signal(input_file, max_length)\n","\n","fs_list = [f for f in fs_list if f <= Fs]\n","if fs_list[-1] != Fs:\n","    fs_list.append(Fs)\n","\n","scales = [Fs / f for f in fs_list]\n","\n","print('Working on file: %s' % input_file)\n","\n","scheduler_milestones = [int(num_epochs * 2 / 3)]\n","\n","alpha1 = 0\n","alpha2 = 1e-4\n","add_cond_noise = True\n","\n","dilation_factors = [2 ** i for i in range(num_layers)]\n","\n","if not os.path.exists(output_folder):\n","    os.mkdir(output_folder)\n","\n","if os.path.exists(output_folder):\n","    dirs = glob.glob(output_folder + '*')\n","    output_folder = output_folder + '_' + str(len(dirs) + 1)\n","\n","os.mkdir(output_folder)\n","print('Writing results to %s\\n' % output_folder)\n","\n","signals_list, fs_list = create_input_signals(scales, set_first_scale_by_energy, min_energy_th,  filter_size, torch.tensor(samples), Fs)\n","if len(signals_list) == 0:\n","    set_first_scale_by_energy = False\n","    scales = scales[2:]  # Manually start from 500\n","    signals_list, fs_list = create_input_signals(scales, set_first_scale_by_energy, min_energy_th,  filter_size, torch.tensor(samples), Fs)\n","scales = [Fs / f for f in fs_list]\n","\n","fs_list = fs_list\n","inputs_lengths = [len(s) for s in signals_list]\n","\n","print('Running on ' + str(device))\n","\n","output_signals, loss_vectors, generators_list, noise_amp_list, energy_list, reconstruction_noise_list = train(\n","                          manual_random_seed, fs_list, scales, growing_hidden_channels_factor,learning_rate, beta1, scheduler_lr_decay,\n","                          plot_losses, initial_noise_amp, noise_amp_factor, signals_list, dilation_factors, output_folder, inputs_lengths)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bBY1GNTZpbgP"},"outputs":[],"source":["#!zip -r outputs.zip outputs_2"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"pfWK4VohjG0Z","executionInfo":{"status":"ok","timestamp":1663961151275,"user_tz":-60,"elapsed":327,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}}},"outputs":[],"source":["class AudioGenerator(object):\n","    def __init__(self, output_folder, fs_list, dilation_factors, filter_size, device, generators_list=None, noise_amp_list=None, reconstruction_noise_list=None):\n","        super(AudioGenerator, self).__init__()\n","        self.generators_list = generators_list\n","        self.noise_amp_list = noise_amp_list\n","        self.reconstruction_noise_list = reconstruction_noise_list\n","        self.output_folder = output_folder\n","        self.fs_list= fs_list\n","        self.device = device\n","        self.dilation_factors = dilation_factors\n","        self.filter_size = filter_size\n","        if not os.path.exists(os.path.join(output_folder, 'GeneratedSignals')):\n","            os.mkdir(os.path.join(output_folder, 'GeneratedSignals'))\n","\n","    def generate(self, nSignals=1, length=20, generate_all_scales=False):\n","        for sig_idx in range(nSignals):\n","            # Draws a signal up to current scale, using learned generators\n","            output_signals_list = draw_signal(self.generators_list,\n","                                              [round(f * length) for f in self.fs_list], self.fs_list,\n","                                              self.noise_amp_list,  self.filter_size, self.dilation_factors, self.device, \n","                                              output_all_scales=generate_all_scales)\n","            # Write signals\n","            if generate_all_scales:\n","                for scale_idx, sig in enumerate(output_signals_list):\n","                    write_signal(\n","                        os.path.join(self.output_folder, 'GeneratedSignals',\n","                                     'generated@%dHz.wav' % self.fs_list[scale_idx]),\n","                        sig, self.fs_list[scale_idx], overwrite=False)\n","            else:\n","                write_signal(\n","                    os.path.join(self.output_folder, 'GeneratedSignals',\n","                                 'generated@%dHz.wav' % self.fs_list[-1]),\n","                    output_signals_list, self.fs_list[-1], overwrite=False)\n","\n","    def condition(self, condition, write=True):\n","        condition[\"condition_scale_idx\"] = np.where(np.array(self.fs_list) <= condition[\"condition_fs\"])[0][\n","                                               -1] + 1\n","        condition[\"condition_signal\"] = torch.Tensor(condition[\"condition_signal\"]).expand(1, 1, -1).to(\n","            self.device)\n","        lengths = [int(condition[\"condition_signal\"].shape[2] / condition[\"condition_fs\"] * fs) for fs in\n","                   self.fs_list]\n","        conditioned_signal = draw_signal(self.generators_list, lengths, self.fs_list, self.noise_amp_list, \n","                                         self.filter_size, self.dilation_factors, self.device,\n","                                         condition=condition)\n","        if write:\n","            output_file = os.path.join(self.output_folder, 'GeneratedSignals',\n","                                       'conditioned_on_' + condition['name'])\n","            write_signal(output_file, conditioned_signal, self.params.Fs)\n","        else:\n","            return conditioned_signal"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"2XgcJ9x-zq06","executionInfo":{"status":"ok","timestamp":1663961165645,"user_tz":-60,"elapsed":1576,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}}},"outputs":[],"source":["nSignals=1\n","length=25\n","generate_all_scales=False\n","\n","audio_generator = AudioGenerator(output_folder, fs_list, dilation_factors, filter_size, device, generators_list, noise_amp_list,\n","                                 reconstruction_noise_list=reconstruction_noise_list)\n","\n","audio_generator.generate(nSignals=nSignals, length=length,generate_all_scales=generate_all_scales)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"W_HyF3o0kkHQ","executionInfo":{"status":"ok","timestamp":1663961168896,"user_tz":-60,"elapsed":476,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}}},"outputs":[],"source":["path = \"/content/outputs_2/GeneratedSignals\""]},{"cell_type":"code","execution_count":21,"metadata":{"id":"wLMleJ-vk2oB","executionInfo":{"status":"ok","timestamp":1663961172859,"user_tz":-60,"elapsed":702,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3001188c-20f6-464e-ab7a-f70243b6366b"},"outputs":[{"output_type":"stream","name":"stdout","text":["We have 1 .Wav Files with 0.76 Mb in size\n"]}],"source":["paths = []\n","size = 0\n","for root, dirs, files in os.walk(path):\n","    for file in files:\n","        if (file.endswith(\".wav\") and  (not (file.startswith(\".\") or file.startswith(\"noise\")))):\n","             paths.append(os.path.join(root, file))\n","             size += os.path.getsize(os.path.join(root, file))\n","             \n","\n","\n","print(f'We have {len(paths)} .Wav Files with {size/1024**2:.2f} Mb in size')"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"wWO_wzVto6df","executionInfo":{"status":"ok","timestamp":1663961177743,"user_tz":-60,"elapsed":455,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0db39ce-127c-413a-f381-0c6db3eac4ad"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['  adding: content/outputs_2/GeneratedSignals/ (stored 0%)',\n"," '  adding: content/outputs_2/GeneratedSignals/generated@16000Hz.wav (deflated 7%)']"]},"metadata":{},"execution_count":22}],"source":["!!zip -r /content/outputs_2/GeneratedSignals.zip /content/outputs_2/GeneratedSignals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T112qY4jpfmr"},"outputs":[],"source":["from google.colab import files\n","files.download('/content/outputs_2/GeneratedSignals.zip')"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"7aCyKbHdaDnH","executionInfo":{"status":"ok","timestamp":1663961182639,"user_tz":-60,"elapsed":492,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}}},"outputs":[],"source":["!cp /content/outputs_2/GeneratedSignals.zip  /content/drive/MyDrive/FinalProject/CAW_outputs"]},{"cell_type":"markdown","metadata":{"id":"-Du88qF5aFIw"},"source":["# Remember to change this path everything you do generation ok"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"ruT6IPL-aKZn","executionInfo":{"status":"ok","timestamp":1663961205586,"user_tz":-60,"elapsed":7505,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"36647d4c-4fa8-46e1-f706-2cf0122cded5"},"outputs":[{"output_type":"stream","name":"stdout","text":["We have 7 .Wav Files with 5.34 Mb in size\n","updating: content/outputs_2/GeneratedSignals/ (stored 0%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_1.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_2.wav (deflated 8%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_3.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_4.wav (deflated 10%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_5.wav (deflated 7%)\n","  adding: content/outputs_2/GeneratedSignals/generated@16000Hz_6.wav (deflated 7%)\n","We have 8 .Wav Files with 6.10 Mb in size\n","updating: content/outputs_2/GeneratedSignals/ (stored 0%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_1.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_2.wav (deflated 8%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_3.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_4.wav (deflated 10%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_5.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_6.wav (deflated 7%)\n","  adding: content/outputs_2/GeneratedSignals/generated@16000Hz_7.wav (deflated 7%)\n","We have 9 .Wav Files with 6.87 Mb in size\n","updating: content/outputs_2/GeneratedSignals/ (stored 0%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_1.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_2.wav (deflated 8%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_3.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_4.wav (deflated 10%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_5.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_6.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_7.wav (deflated 7%)\n","  adding: content/outputs_2/GeneratedSignals/generated@16000Hz_8.wav (deflated 7%)\n","We have 10 .Wav Files with 7.63 Mb in size\n","updating: content/outputs_2/GeneratedSignals/ (stored 0%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_1.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_2.wav (deflated 8%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_3.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_4.wav (deflated 10%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_5.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_6.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_7.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_8.wav (deflated 7%)\n","  adding: content/outputs_2/GeneratedSignals/generated@16000Hz_9.wav (deflated 7%)\n","We have 11 .Wav Files with 8.39 Mb in size\n","updating: content/outputs_2/GeneratedSignals/ (stored 0%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_1.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_2.wav (deflated 8%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_3.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_4.wav (deflated 10%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_5.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_6.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_7.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_8.wav (deflated 7%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_9.wav (deflated 7%)\n","  adding: content/outputs_2/GeneratedSignals/generated@16000Hz_10.wav (deflated 9%)\n","Don't forget to change n according to the number of samples you want\n"]}],"source":["n = 5\n","\n","\n","for i in range (n):\n","  nSignals=1\n","  length=25\n","  generate_all_scales=False\n","\n","  audio_generator = AudioGenerator(output_folder, fs_list, dilation_factors, filter_size, device, generators_list, noise_amp_list,\n","                                 reconstruction_noise_list=reconstruction_noise_list)\n","\n","  audio_generator.generate(nSignals=nSignals, length=length,generate_all_scales=generate_all_scales)\n","\n","  paths = []\n","  size = 0\n","  for root, dirs, files in os.walk(path):\n","    for file in files:\n","        if (file.endswith(\".wav\") and  (not (file.startswith(\".\") or file.startswith(\"noise\")))):\n","             paths.append(os.path.join(root, file))\n","             size += os.path.getsize(os.path.join(root, file))\n","             \n","\n","\n","  print(f'We have {len(paths)} .Wav Files with {size/1024**2:.2f} Mb in size')\n","  !zip -r /content/outputs_2/GeneratedSignals.zip /content/outputs_2/GeneratedSignals\n","  !cp /content/outputs_2/GeneratedSignals.zip  /content/drive/MyDrive/FinalProject/CAW_outputs\n","\n","print(\"Don't forget to change n according to the number of samples you want\")\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[],"background_execution":"on"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}