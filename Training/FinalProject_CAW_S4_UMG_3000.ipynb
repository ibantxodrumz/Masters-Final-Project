{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111327,"status":"ok","timestamp":1664180248086,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"},"user_tz":-60},"id":"dg-Zaz71X5ao","outputId":"2f865d5d-0bcf-4e19-ee0d-047fb8465296"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.9.0\n","  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[K     |████████████████████████████████| 831.4 MB 2.3 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (4.1.1)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.9.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: librosa==0.8.1 in /usr/local/lib/python3.7/dist-packages (0.8.1)\n","Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (3.0.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (21.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (1.21.6)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (1.7.3)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (1.6.0)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (1.0.2)\n","Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (0.4.0)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (4.4.2)\n","Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (0.56.2)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (1.1.0)\n","Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (0.10.3.post1)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa==0.8.1) (0.39.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa==0.8.1) (4.12.0)\n","Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa==0.8.1) (57.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa==0.8.1) (3.0.9)\n","Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.1) (1.4.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.1) (2.23.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (3.0.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.1) (3.1.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa==0.8.1) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.8.1) (2.21)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa==0.8.1) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa==0.8.1) (4.1.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting soundfile==0.10.2\n","  Downloading SoundFile-0.10.2-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile==0.10.2) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile==0.10.2) (2.21)\n","Installing collected packages: soundfile\n","  Attempting uninstall: soundfile\n","    Found existing installation: SoundFile 0.10.3.post1\n","    Uninstalling SoundFile-0.10.3.post1:\n","      Successfully uninstalled SoundFile-0.10.3.post1\n","Successfully installed soundfile-0.10.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting bokeh==2.3.0\n","  Downloading bokeh-2.3.0.tar.gz (10.6 MB)\n","\u001b[K     |████████████████████████████████| 10.6 MB 3.6 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (6.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (2.8.2)\n","Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (2.11.3)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (1.21.6)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (7.1.2)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (21.3)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (5.1.1)\n","Requirement already satisfied: typing_extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (4.1.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.7->bokeh==2.3.0) (2.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh==2.3.0) (3.0.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh==2.3.0) (1.15.0)\n","Building wheels for collected packages: bokeh\n","  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bokeh: filename=bokeh-2.3.0-py3-none-any.whl size=11292273 sha256=5420ce4a2255774bb7a1110bab994822b4745dd621cf9896491b0df55b90b962\n","  Stored in directory: /root/.cache/pip/wheels/fe/2b/67/993b844d1b11a6129b91880955c1b315438d00fc39d2dcf489\n","Successfully built bokeh\n","Installing collected packages: bokeh\n","  Attempting uninstall: bokeh\n","    Found existing installation: bokeh 2.3.3\n","    Uninstalling bokeh-2.3.3:\n","      Successfully uninstalled bokeh-2.3.3\n","Successfully installed bokeh-2.3.0\n"]}],"source":["!pip install torch==1.9.0\n","!pip install librosa==0.8.1\n","!pip install soundfile==0.10.2\n","!pip install bokeh==2.3.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jkAW3HWGajZj"},"outputs":[],"source":["import torch\n","import librosa\n","import soundfile as sf\n","import torch.nn as nn\n","import numpy as np\n","from torch.nn.utils import weight_norm\n","from torch import optim\n","from math import ceil\n","import glob\n","import time\n","import random\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31185,"status":"ok","timestamp":1664180281192,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"},"user_tz":-60},"id":"LqIHLYluDlVe","outputId":"87b9aa2e-bde9-415f-bb07-4b69e73bd96b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["#Connect colab to your google drive\n","from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3785,"status":"ok","timestamp":1664180284972,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"},"user_tz":-60},"id":"2-dJwYs_iAkN","outputId":"d06803df-e3a3-46ca-d9d6-e2c5ab1a2554"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SajXpcQKDUgM"},"outputs":[],"source":["#prepare input folder\n","input_folder='inputs'\n","if not os.path.exists(input_folder):\n","    os.mkdir(input_folder)\n","#copy file from drive to colab --remember it has to be from mydrive---for whatever reason it does not go any deeper!\n","!cp /content/drive/MyDrive/S4_U.wav /content/inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4q0K6-KXcxmO"},"outputs":[],"source":["#paramaters\n","inpainting_indices= [0, 1]\n","is_cuda = torch.cuda.is_available()\n","gpu_num = 0\n","manual_random_seed = -1\n","input_file = 'S4_U.wav'\n","segments_to_train = []\n","start_time = 0\n","init_sample_rate =  16000\n","fs_list = [320, 400, 500, 640, 800, 1000, 1280, 1600, 2000, 2500, 4000, 8000, 10000, 12000, 14400, 16000]\n","max_length = 25\n","run_mode = 'normal' #['normal', 'inpainting', 'denoising']\n","num_epochs = 3000\n","learning_rate = 0.0015\n","scheduler_lr_decay = 0.1\n","beta1 = 0.5\n","speech = False\n","num_layers = 8\n","output_folder = 'outputs'\n","filter_size = 9\n","set_first_scale_by_energy = True\n","min_energy_th = 0.0025\n","hidden_channels_init = 16\n","growing_hidden_channels_factor = 6\n","plot_losses = False\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","initial_noise_amp = 1\n","noise_amp_factor = 0.01"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bELoeAltdEdB"},"outputs":[],"source":["#functions\n","def get_input_signal(input_file, max_length):\n","    file_name = input_file.split('.')\n","    if len(file_name) < 2:\n","        input_file = '.'.join([input_file, 'wav'])\n","    output_folder = file_name[0].replace(' ', '_')\n","    if len(segments_to_train) == 0:\n","        samples, Fs = librosa.load(os.path.join('inputs', input_file), sr=None,\n","                                   offset=start_time, duration=2 * max_length)\n","\n","    if samples.shape[0] / Fs > max_length:\n","        n_samples = int(max_length * Fs)\n","        samples = samples[:n_samples]\n","\n","    output_folder = output_folder\n","    output_folder = os.path.join('outputs', output_folder)\n","    Fs = Fs\n","    if init_sample_rate < Fs:\n","        hr_samples = samples.copy()\n","        samples = librosa.resample(hr_samples, Fs, init_sample_rate)\n","        Fs = init_sample_rate\n","    norm_factor = max(abs(samples.reshape(-1)))\n","    samples = samples / norm_factor\n","    return samples, Fs\n","\n","def create_input_signals(scales, set_first_scale_by_energy, min_energy_th,  filter_size, input_signal, Fs):\n","    # Performs downscaling for desired scales and outputs list of signals\n","    signals_list = []\n","    fs_list = []\n","    n_scales = len(scales)\n","    set_first_scale = False\n","    rf = calc_receptive_field(filter_size, dilation_factors)\n","    for k in range(n_scales):\n","        downsample = scales[k]\n","        fs = int(Fs / downsample)\n","        if downsample == 1:\n","            coarse_sig = input_signal\n","        else:\n","            coarse_sig = torch.Tensor(librosa.resample(input_signal.squeeze().numpy(), Fs, fs))\n","        if speech and fs < 500:\n","            continue\n","        if set_first_scale_by_energy and not speech:\n","            e = (coarse_sig ** 2).mean()\n","            if e < min_energy_th and not set_first_scale:\n","                continue\n","        set_first_scale = True\n","        signals_list.append(coarse_sig)\n","        assert np.mod(fs, 1) == 0, 'Sampling rate is not integer'\n","        fs_list.append(int(fs))\n","\n","        # Write downsampled real sound\n","        filename = 'real@%dHz.wav' % fs\n","        write_signal(os.path.join(output_folder, filename), coarse_sig.cpu(), fs)\n","\n","    return signals_list, fs_list\n","\n","def calc_receptive_field(filter_size, dilation_factors, Fs=None):\n","    if Fs is None:\n","        # in samples\n","        return (filter_size * dilation_factors[0] + sum(dilation_factors[1:]) * (filter_size - 1))\n","    else:\n","        # in [ms]\n","        return (filter_size * dilation_factors[0] + sum(dilation_factors[1:]) * (filter_size - 1)) / Fs * 1e3\n","\n","def write_signal(path, signal, fs, overwrite=False, subtype='PCM_16'):\n","    if signal is None:\n","        return\n","    if torch.is_tensor(signal):\n","        signal = signal.squeeze().detach().cpu().numpy()\n","    if not path.endswith('.wav'):\n","        path = path + '.wav'\n","    if not overwrite:\n","        if os.path.exists(path):\n","            files = glob.glob(path[:-4].replace('[Hz]', '[[]Hz[]]') + '*')\n","            path = path[:-4] + '_' + str(len(files)) + path[-4:]\n","    maxAmp = max(abs(signal.reshape(-1)))\n","    if maxAmp > 1:\n","        signal = signal / maxAmp  # normalize to avoid clipping\n","    sf.write(path, signal, fs, subtype=subtype)\n","\n","def calc_pad_size(dilation_factors, filter_size):\n","    return int(np.ceil(sum(dilation_factors) * (filter_size - 1) / 2))\n","\n","def get_noise(device, shape):\n","    return torch.randn(shape, device=device)\n","\n","def draw_signal(generators_list, signals_lengths_list, fs_list, noise_amp_list, filter_size, dilation_factors, device, reconstruction_noise_list=None,\n","                condition=None, output_all_scales=False):\n","    # Draws a signal up to current scale, using learned generators\n","    pad_size = calc_pad_size(dilation_factors, filter_size)\n","    if output_all_scales:\n","        signals_all_scales = []\n","    for scale_idx, (netG, noise_amp) in enumerate(zip(generators_list, noise_amp_list)):\n","        signal_padder = nn.ConstantPad1d(pad_size, 0)\n","        if condition is None:\n","            n_samples = signals_lengths_list[scale_idx]\n","            if reconstruction_noise_list is not None:\n","                noise_signal = reconstruction_noise_list[scale_idx]\n","            else:\n","                noise_signal = get_noise(device, (1, 1, n_samples))\n","                noise_signal = noise_signal * noise_amp\n","\n","            if scale_idx == 0:\n","                prev_sig = torch.full(noise_signal.shape, 0, device=device, dtype=noise_signal.dtype)\n","            else:\n","                prev_sig = signal_padder(prev_sig)\n","\n","            # pad noise with zeros, to match signal after filtering\n","            if reconstruction_noise_list is None:\n","                # reconstruction_noise is already padded\n","                noise_signal = signal_padder(noise_signal)\n","                if scale_idx == 0:\n","                    prev_sig = signal_padder(prev_sig)\n","        else:\n","            if scale_idx < condition[\"condition_scale_idx\"]:\n","                continue\n","            elif scale_idx == condition[\"condition_scale_idx\"]:\n","                prev_sig = resample_sig(device, condition[\"condition_signal\"], condition['condition_fs'],\n","                                        fs_list[scale_idx]).expand(1, 1, -1)\n","            noise_signal = get_noise(device, prev_sig.shape[2]).expand(1, 1, -1)\n","            noise_signal = signal_padder(noise_signal)\n","            noise_signal = noise_signal * noise_amp\n","            prev_sig = signal_padder(prev_sig)\n","\n","        # Generate this scale signal\n","        cur_sig = netG((noise_signal + prev_sig).detach(), prev_sig)\n","\n","        if output_all_scales:\n","            signals_all_scales.append(torch.squeeze(cur_sig).detach().cpu().numpy())\n","\n","        # Upsample for next scale\n","        if scale_idx < len(fs_list) - 1:\n","            up_sig = resample_sig( device, cur_sig, orig_fs=fs_list[scale_idx], target_fs=fs_list[scale_idx + 1])\n","            if up_sig.shape[2] > signals_lengths_list[scale_idx + 1]:\n","                assert abs(\n","                    up_sig.shape[2] > signals_lengths_list[scale_idx + 1]) < 20, 'Should not happen, check this!'\n","                up_sig = up_sig[:, :, :signals_lengths_list[scale_idx + 1]]\n","            elif up_sig.shape[2] < signals_lengths_list[scale_idx + 1]:\n","                assert abs(\n","                    up_sig.shape[2] < signals_lengths_list[scale_idx + 1]) < 20, 'Should not happen, check this!'\n","                up_sig = torch.cat(\n","                    (up_sig, up_sig.new_zeros(1, 1, signals_lengths_list[scale_idx + 1] - up_sig.shape[2])),\n","                    dim=2)\n","        else:\n","            up_sig = cur_sig\n","        prev_sig = up_sig\n","        prev_sig = prev_sig.detach()\n","\n","        del up_sig, cur_sig, noise_signal, netG\n","\n","    if output_all_scales:\n","        return signals_all_scales\n","    else:\n","        return prev_sig\n","\n","def resample_sig(device,input_signal, orig_fs=None, target_fs=None, resamplers=None):\n","    if resamplers == None:\n","        resamplers = {}\n","    if (orig_fs, target_fs) in resamplers.keys() and resamplers[(orig_fs, target_fs)].in_shape[2] == \\\n","            input_signal.shape[2]:\n","        resampler = resamplers[(orig_fs, target_fs)]\n","    else:\n","        in_shape = input_signal.shape\n","        scale_factors = (1, 1, target_fs / orig_fs)\n","        resampler = ResizeLayer(in_shape, scale_factors=scale_factors, device=device)\n","        resamplers[(orig_fs, target_fs)] = resampler\n","    new_sig = resampler(input_signal)\n","\n","    return new_sig\n","\n","def support_sz(sz):\n","    def wrapper(f):\n","        f.support_sz = sz\n","        return f\n","    return wrapper\n","\n","@support_sz(4)\n","def cubic(x):\n","    fw, to_dtype, eps = set_framework_dependencies(x)\n","    absx = fw.abs(x)\n","    absx2 = absx ** 2\n","    absx3 = absx ** 3\n","    return ((1.5 * absx3 - 2.5 * absx2 + 1.) * to_dtype(absx <= 1.) +\n","            (-0.5 * absx3 + 2.5 * absx2 - 4. * absx + 2.) *\n","            to_dtype((1. < absx) & (absx <= 2.)))\n","\n","class ResizeLayer(nn.Module):\n","    def __init__(self, in_shape, scale_factors=None, out_shape=None,\n","                 interp_method=cubic, support_sz=None,\n","                 antialiasing=True, device=None):\n","        super(ResizeLayer, self).__init__()\n","\n","        # fw stands for framework, that can be either numpy or torch. since\n","        # this is a torch layer, only one option in this case.\n","        fw = torch\n","        eps = fw.finfo(fw.float32).eps\n","\n","        # set missing scale factors or output shapem one according to another,\n","        # scream if both missing\n","        scale_factors, out_shape = set_scale_and_out_sz(in_shape, out_shape,\n","                                                        scale_factors, fw)\n","        \n","        # unless support size is specified by the user, it is an attribute\n","        # of the interpolation method\n","        if support_sz is None:\n","            support_sz = interp_method.support_sz\n","        \n","        self.n_dims = len(in_shape)       \n","\n","        # sort indices of dimensions according to scale of each dimension.\n","        # since we are going dim by dim this is efficient\n","        self.sorted_filtered_dims_and_scales = [(dim, scale_factors[dim])\n","                                                for dim in\n","                                                sorted(range(self.n_dims),\n","                                                key=lambda ind:\n","                                                scale_factors[ind])\n","                                                if scale_factors[dim] != 1.]\n","\n","        # iterate over dims\n","        field_of_view_list = []\n","        weights_list = []\n","        for dim, scale_factor in self.sorted_filtered_dims_and_scales:\n","\n","            # get 1d set of weights and fields of view for each output\n","            # location along this dim\n","            field_of_view, weights = prepare_weights_and_field_of_view_1d(\n","                dim, scale_factor, in_shape[dim], out_shape[dim],\n","                interp_method, support_sz, antialiasing, fw, eps, device)\n","\n","            # keep weights and fields of views for all dims\n","            weights_list.append(nn.Parameter(weights, requires_grad=False))\n","            field_of_view_list.append(nn.Parameter(field_of_view,\n","                                      requires_grad=False))\n","\n","        self.field_of_view = nn.ParameterList(field_of_view_list)\n","        self.weights = nn.ParameterList(weights_list)\n","        self.in_shape = in_shape\n","\n","    def forward(self, input):\n","        # output begins identical to input and changes with each iteration\n","        output = input\n","\n","        for (dim, scale_factor), field_of_view, weights in zip(\n","                self.sorted_filtered_dims_and_scales,\n","                self.field_of_view,\n","                self.weights):\n","            # multiply the weights by the values in the field of view and\n","            # aggreagate\n","            output = apply_weights(output, field_of_view, weights, dim,\n","                                   self.n_dims, torch)\n","        return output\n","\n","def prepare_weights_and_field_of_view_1d(dim, scale_factor, in_sz, out_sz,\n","                                         interp_method, support_sz, \n","                                         antialiasing, fw, eps, device=None):\n","    # If antialiasing is taking place, we modify the window size and the\n","    # interpolation method (see inside function)\n","    interp_method, cur_support_sz = apply_antialiasing_if_needed(\n","                                                             interp_method,\n","                                                             support_sz,\n","                                                             scale_factor,\n","                                                             antialiasing)\n","\n","    # STEP 1- PROJECTED GRID: The non-integer locations of the projection of\n","    # output pixel locations to the input tensor\n","    projected_grid = get_projected_grid(in_sz, out_sz, scale_factor, fw, device)\n","\n","    # STEP 2- FIELDS OF VIEW: for each output pixels, map the input pixels\n","    # that influence it\n","    field_of_view = get_field_of_view(projected_grid, cur_support_sz, in_sz,\n","                                      fw, eps)\n","\n","    # STEP 3- CALCULATE WEIGHTS: Match a set of weights to the pixels in the\n","    # field of view for each output pixel\n","    weights = get_weights(interp_method, projected_grid, field_of_view)\n","\n","    return field_of_view, weights\n","\n","def apply_weights(input, field_of_view, weights, dim, n_dims, fw):\n","    # STEP 4- APPLY WEIGHTS: Each output pixel is calculated by multiplying\n","    # its set of weights with the pixel values in its field of view.\n","    # We now multiply the fields of view with their matching weights.\n","    # We do this by tensor multiplication and broadcasting.\n","    # this step is separated to a different function, so that it can be\n","    # repeated with the same calculated weights and fields.\n","\n","    # for this operations we assume the resized dim is the first one.\n","    # so we transpose and will transpose back after multiplying\n","    tmp_input = fw_swapaxes(input, dim, 0, fw)\n","\n","    # field_of_view is a tensor of order 2: for each output (1d location\n","    # along cur dim)- a list of 1d neighbors locations.\n","    # note that this whole operations is applied to each dim separately,\n","    # this is why it is all in 1d.\n","    # neighbors = tmp_input[field_of_view] is a tensor of order image_dims+1:\n","    # for each output pixel (this time indicated in all dims), these are the\n","    # values of the neighbors in the 1d field of view. note that we only\n","    # consider neighbors along the current dim, but such set exists for every\n","    # multi-dim location, hence the final tensor order is image_dims+1.\n","    neighbors = tmp_input[field_of_view]\n","\n","    # weights is an order 2 tensor: for each output location along 1d- a list\n","    # of weighs matching the field of view. we augment it with ones, for\n","    # broadcasting, so that when multiplies some tensor the weights affect\n","    # only its first dim.\n","    tmp_weights = fw.reshape(weights, (*weights.shape, * [1] * (n_dims - 1)))\n","\n","    # now we simply multiply the weights with the neighbors, and then sum\n","    # along the field of view, to get a single value per out pixel\n","    tmp_output = (neighbors * tmp_weights).sum(1)\n","\n","    # we transpose back the resized dim to its original position\n","    return fw_swapaxes(tmp_output, 0, dim, fw)\n","\n","def get_weights(interp_method, projected_grid, field_of_view):\n","    # the set of weights per each output pixels is the result of the chosen\n","    # interpolation method applied to the distances between projected grid\n","    # locations and the pixel-centers in the field of view (distances are\n","    # directed, can be positive or negative)\n","    weights = interp_method(projected_grid[:, None] - field_of_view)\n","\n","    # we now carefully normalize the weights to sum to 1 per each output pixel\n","    sum_weights = weights.sum(1, keepdims=True)\n","    sum_weights[sum_weights == 0] = 1\n","    return weights / sum_weights\n","\n","def fw_ceil(x, fw):\n","    return x.ceil().long()\n","\n","\n","def fw_cat(x, fw):\n","    return fw.cat(x)\n","\n","\n","def fw_swapaxes(x, ax_1, ax_2, fw):\n","    return x.transpose(ax_1, ax_2)\n","    \n","def fw_set_device(x, device, fw):\n","    return x.to(device)\n","\n","def set_scale_and_out_sz(in_shape, out_shape, scale_factors, fw):\n","    # eventually we must have both scale-factors and out-sizes for all in/out\n","    # dims. however, we support many possible partial arguments\n","    if scale_factors is None and out_shape is None:\n","        raise ValueError(\"either scale_factors or out_shape should be \"\n","                         \"provided\")\n","    if out_shape is not None:\n","        # if out_shape has less dims than in_shape, we defaultly resize the\n","        # first dims for numpy and last dims for torch\n","        out_shape = list(out_shape) + list(in_shape[:-len(out_shape)])\n","        if scale_factors is None:\n","            # if no scale given, we calculate it as the out to in ratio\n","            # (not recomended)\n","            scale_factors = [out_sz / in_sz for out_sz, in_sz\n","                             in zip(out_shape, in_shape)]\n","    if scale_factors is not None:\n","        # by default, if a single number is given as scale, we assume resizing\n","        # two dims (most common are images with 2 spatial dims)\n","        scale_factors = (scale_factors\n","                         if isinstance(scale_factors, (list, tuple))\n","                         else [scale_factors, scale_factors])\n","        # if less scale_factors than in_shape dims, we defaultly resize the\n","        # first dims for numpy and last dims for torch\n","        scale_factors = list(scale_factors) + [1] * (len(in_shape) - len(scale_factors)) \n","        if out_shape is None:\n","            # when no out_shape given, it is calculated by multiplying the\n","            # scale by the in_shape (not recomended)\n","            out_shape = [ceil(scale_factor * in_sz)\n","                         for scale_factor, in_sz in\n","                         zip(scale_factors, in_shape)]\n","        # next line intentionally after out_shape determined for stability\n","        scale_factors = [float(sf) for sf in scale_factors]\n","    return scale_factors, out_shape\n","\n","def apply_antialiasing_if_needed(interp_method, support_sz, scale_factor,\n","                                 antialiasing):\n","    # antialiasing is \"stretching\" the field of view according to the scale\n","    # factor (only for downscaling). this is low-pass filtering. this\n","    # requires modifying both the interpolation (stretching the 1d\n","    # function and multiplying by the scale-factor) and the window size.\n","    if scale_factor >= 1.0 or not antialiasing:\n","        return interp_method, support_sz\n","    cur_interp_method = (lambda arg: scale_factor *\n","                         interp_method(scale_factor * arg))\n","    cur_support_sz = support_sz / scale_factor\n","    return cur_interp_method, cur_support_sz\n","\n","def get_projected_grid(in_sz, out_sz, scale_factor, fw, device=None):\n","    # we start by having the ouput coordinates which are just integer locations\n","    out_coordinates = fw.arange(out_sz)\n","    \n","    # if using torch we need to match the grid tensor device to the input device\n","    out_coordinates = fw_set_device(out_coordinates, device, fw)\n","        \n","    # This is projecting the ouput pixel locations in 1d to the input tensor,\n","    # as non-integer locations.\n","    # the following fomrula is derived in the paper\n","    # \"From Discrete to Continuous Convolutions\" by Shocher et al.\n","    return (out_coordinates / scale_factor +\n","            (in_sz - 1) / 2 - (out_sz - 1) / (2 * scale_factor))\n","\n","\n","def get_field_of_view(projected_grid, cur_support_sz, in_sz, fw, eps):\n","    # for each output pixel, map which input pixels influence it, in 1d.\n","    # we start by calculating the leftmost neighbor, using half of the window\n","    # size (eps is for when boundary is exact int)\n","    left_boundaries = fw_ceil(projected_grid - cur_support_sz / 2 - eps, fw)\n","\n","    # then we simply take all the pixel centers in the field by counting\n","    # window size pixels from the left boundary\n","    ordinal_numbers = fw.arange(ceil(cur_support_sz - eps))\n","    # in case using torch we need to match the device\n","    ordinal_numbers = fw_set_device(ordinal_numbers, projected_grid.device, fw)\n","    field_of_view = left_boundaries[:, None] + ordinal_numbers\n","\n","    # next we do a trick instead of padding, we map the field of view so that\n","    # it would be like mirror padding, without actually padding\n","    # (which would require enlarging the input tensor)\n","    mirror = fw_cat((fw.arange(in_sz), fw.arange(in_sz - 1, -1, step=-1)), fw)\n","    field_of_view = mirror[fw.remainder(field_of_view, mirror.shape[0])]\n","    field_of_view = fw_set_device(field_of_view,projected_grid.device, fw)\n","    return field_of_view\n","\n","def set_framework_dependencies(x):\n","    if type(x) is np.ndarray:\n","        to_dtype = lambda a: a\n","        fw = np\n","    else:\n","        to_dtype = lambda a: a.to(x.dtype)\n","        fw = torch\n","    eps = fw.finfo(fw.float32).eps\n","    return fw, to_dtype, eps\n","\n","def calc_gradient_penalty(run_mode, current_holes, netD, real_data, fake_data, LAMBDA, alpha=None, _grad_outputs=None, mask_ratio=None, not_valid_idx_start=None, not_valid_idx_end=None):\n","    # Gradient penalty method for WGAN\n","    if alpha is None:\n","        alpha = torch.rand(1, 1)\n","        alpha = alpha.expand(real_data.size())\n","        if torch.cuda.is_available():\n","            alpha = alpha.cuda(real_data.get_device())  # gpu) #if use_cuda else alpha\n","    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","    interpolates = torch.autograd.Variable(interpolates, requires_grad=True)\n","    use_mask = False\n","    mask_ratio = 1\n","    disc_interpolates = netD(interpolates, use_mask)\n","    if _grad_outputs is None:\n","        _grad_outputs = torch.ones(disc_interpolates.size())\n","        if torch.cuda.is_available():\n","            _grad_outputs = _grad_outputs.cuda(real_data.get_device())\n","    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n","                                    grad_outputs=_grad_outputs,\n","                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n","    gradient_penalty = ((mask_ratio * gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n","    del gradients, interpolates, _grad_outputs, disc_interpolates\n","    return gradient_penalty\n","\n","def stft(sig, n_fft, hop_length, window_size):\n","    s = torch.stft(sig, n_fft, hop_length, win_length=window_size,\n","                   window=torch.hann_window(window_size, device=sig.device), return_complex=False)\n","    return s\n","\n","def spec(x, n_fft, hop_length, window_size):\n","    s = stft(x, n_fft, hop_length, window_size)\n","    n = torch.norm(s, p=2, dim=-1)\n","    return n\n","\n","def norm(x):\n","    return (x.view(x.shape[0], -1) ** 2).sum(dim=-1).sqrt()\n","\n","\n","def squeeze(x):\n","    if len(x.shape) == 3:\n","        assert x.shape[-1] in [1, 2]\n","        x = torch.mean(x, -1)\n","    if len(x.shape) != 2:\n","        raise ValueError(f'Unknown input shape {x.shape}')\n","    return x\n","\n","def multi_scale_spectrogram_loss(multispec_loss_n_fft, multispec_loss_hop_length, multispec_loss_window_size, current_holes, x_in, x_out):\n","    losses = []\n","    args = [multispec_loss_n_fft,\n","            multispec_loss_hop_length,\n","            multispec_loss_window_size]\n","    for n_fft, hop_length, window_size in zip(*args):\n","        if window_size == -1:\n","            window_size = x_in.shape[1]\n","            hop_length = window_size + 1\n","            n_fft = int(2 ** np.ceil(np.log2(window_size)))\n","        spec_in = spec(squeeze(x_in.float()), n_fft, hop_length, window_size)\n","        spec_out = spec(squeeze(x_out.float()), n_fft, hop_length, window_size)\n","        losses.append(norm(spec_in - spec_out))\n","    return sum(losses) / len(losses)\n","\n","def reset_grads(model, require_grad):\n","    for p in model.parameters():\n","        p.requires_grad_(require_grad)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uv_GjFaJDb6G"},"outputs":[],"source":["#model \n","class Generator(nn.Module):\n","    def __init__(self, filter_size, hidden_channels, current_fs ):\n","        super(Generator, self).__init__()\n","        self.head = ConvBlock(filter_size, 1, hidden_channels, dilation_factors[0])\n","        self.body = nn.Sequential()\n","        self.Fs = current_fs\n","        for i in range(num_layers - 2):\n","            block = ConvBlock(filter_size, hidden_channels, hidden_channels, dilation_factors[i + 1])\n","            self.body.add_module('block%d' % (i + 1), block)\n","        self.tail = nn.Sequential()\n","        self.tail.add_module('tail0',\n","                             NormConv1d(in_channels=hidden_channels, out_channels=hidden_channels,\n","                                        kernel_size=filter_size,\n","                                        dilation=dilation_factors[-1]))\n","        self.filter = nn.Sequential(\n","            NormConv1d(in_channels=hidden_channels, out_channels=hidden_channels,\n","                       kernel_size=filter_size, padding=int((filter_size - 1) / 2)),\n","            nn.Tanh()\n","        )\n","        self.gate = nn.Sequential(\n","            NormConv1d(in_channels=hidden_channels, out_channels=hidden_channels,\n","                       kernel_size=filter_size, padding=int((filter_size - 1) / 2)),\n","            nn.Sigmoid()\n","        )\n","        self.out_conv = NormConv1d(hidden_channels, 1, kernel_size=1)\n","        self.pe_filter = PreEmphasisFilter(device)\n","\n","    def forward(self, noise_plus_sig, prev_sig):\n","        out_head = self.head(noise_plus_sig)\n","        out_body = self.body(out_head)\n","        out_tail = self.tail(out_body)\n","        filter = self.filter(out_tail)\n","        gate = self.gate(out_tail)\n","        out_tail = filter * gate\n","        out_tail = self.out_conv(out_tail)\n","        out_filt = self.pe_filter(out_tail)\n","        ind = int((prev_sig.shape[2] - out_filt.shape[2]) / 2)\n","        prev_sig = prev_sig[:, :, ind:(prev_sig.shape[2] - ind)]\n","        output = out_filt + prev_sig\n","        return output\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, run_mode, current_holes, hidden_channels, dilation_factors, num_layers, device,filter_size ):\n","        super(Discriminator, self).__init__()\n","        if run_mode == 'inpainting':\n","            mask = current_holes\n","        else:\n","            mask = None\n","        self.head = ConvBlock(filter_size, 1, hidden_channels, dilation_factors[0], mask=mask)\n","        mask = self.head.mask_out\n","        self.body = nn.ModuleList()\n","        for i in range(num_layers - 2):\n","            block = ConvBlock(filter_size, hidden_channels, hidden_channels,\n","                              dilation_factors[i + 1], mask=mask)\n","            mask = block.mask_out\n","            self.body.add_module('block%d' % (i + 1), block)\n","        self.mask_out = mask\n","        self.tail = NormConv1d(hidden_channels, 1, kernel_size=filter_size,\n","                               dilation=dilation_factors[-1])\n","        self.pe_filter = PreEmphasisFilter(device)\n","\n","    def forward(self, sig, use_mask=False):\n","        out_head = self.head(sig, use_mask)\n","        out_body = out_head\n","        for b in self.body:\n","            out_body = b(out_body, use_mask)\n","        out_tail = self.tail(out_body)\n","        output = self.pe_filter(out_tail)\n","        return output\n","\n","\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1 and classname.find('ConvBlock') == -1 and hasattr(m, 'weight'):\n","        if m.weight.numel() > 1 and m.weight.requires_grad:  # scalar blocks are initiailized upon creation\n","            m.weight.data.normal_(0.0, 0.02)\n","\n","    elif classname.find('Norm') != -1 and hasattr(m, 'weight'):\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n","\n","class PreEmphasisFilter(nn.Module):\n","    def __init__(self, device):\n","        super(PreEmphasisFilter, self).__init__()\n","        self.alpha = torch.Tensor([0.97]).to(device)\n","        self.alpha.requires_grad = False\n","\n","    def forward(self, x):\n","        output = torch.cat((x[:, :, 0].view(x.shape[0], x.shape[1], 1), x[:, :, 1:] - self.alpha * x[:, :, :-1]), dim=2)\n","        return output\n","\n","\n","class NormConv1d(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=True):\n","        super(NormConv1d, self).__init__()\n","        self.conv = weight_norm(nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n","                                          stride=stride, padding=padding, dilation=dilation, bias=bias))\n","\n","    def forward(self, x):\n","        output = self.conv(x)\n","        return output\n","\n","\n","class ConvBlock(nn.Sequential):\n","    def __init__(self, filter_size, in_channels, out_channels, dilation=1, mask=None):\n","        super(ConvBlock, self).__init__()\n","        if filter_size is None:\n","            filter_size = filter_size\n","        if mask is not None:\n","            self.mask_in = mask\n","            self.mask_out = []\n","            self.rf = int((filter_size - 1) * dilation)\n","            for hole in self.mask_in:\n","                self.mask_out.append([hole[0] - self.rf, hole[1]])\n","            # ???\n","            # for idx in range(len(self.mask_out) - 1):\n","            #     if self.mask_out[idx+1][0] < self.mask_out[idx][1]:\n","            #         self.mask_out[idx+1][0] = self.mask_out[idx][1] + 1\n","\n","        else:\n","            self.mask_out = None\n","        self.conv = NormConv1d(in_channels, out_channels, filter_size, dilation=dilation)\n","        self.norm = nn.BatchNorm1d(out_channels)\n","        self.activation = nn.LeakyReLU(0.2, inplace=True)\n","\n","    def forward(self, x, use_mask=False):\n","        out_conv = self.conv(x)\n","        if use_mask:\n","            #tmp = torch.cat((out_conv[:, :, :int(self.mask_out[0][0])], out_conv[:, :, int(self.mask_out[0][1] + 1):]), dim=2)\n","            tmp = out_conv[:, :, :int(self.mask_out[0][0])].clone()\n","            cut_idx = []\n","            cut_idx.append(tmp.shape[2])\n","            for idx in range(len(self.mask_out)-1):\n","                tmp = torch.cat((tmp, out_conv[:, :, int(self.mask_out[idx][1] + 1):int(self.mask_out[idx+1][0])]), dim=2)\n","                cut_idx.append(tmp.shape[2])\n","            tmp = torch.cat((tmp, out_conv[:, :, int(self.mask_out[-1][1] + 1):]), dim=2)\n","\n","            tmp_norm = self.norm(tmp)\n","            out_norm = out_conv\n","            out_norm[:, :, :int(self.mask_out[0][0])] = tmp_norm[:, :, :int(cut_idx[0])]\n","            for idx in range(len(self.mask_out) - 1):\n","                out_norm[:, :, int(self.mask_out[idx][1] + 1):int(self.mask_out[idx+1][0])] = tmp_norm[:, :, int(cut_idx[idx]):int(cut_idx[idx+1])] #tmp_norm[:, :, int(self.mask_out[idx][0]):int(self.mask_out[idx+1][0])]\n","                #out_norm[:, :, :int(self.mask_out[idx+1][0])] = tmp_norm[:, :, :int(self.mask_out[idx+1][0])]\n","            out_norm[:, :, int(self.mask_out[-1][1] + 1):] = tmp_norm[:, :, int(cut_idx[-1]):]\n","\n","        else:\n","            out_norm = self.norm(out_conv)\n","        return self.activation(out_norm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_r37Na9OBSA8"},"outputs":[],"source":["#training functions\n","def train(manual_random_seed, fs_list, scales, growing_hidden_channels_factor,learning_rate, beta1, scheduler_lr_decay, plot_losses,\n","          initial_noise_amp, noise_amp_factor, signals_list, dilation_factors, output_folder, inputs_lengths):\n","    if manual_random_seed != -1:\n","        random.seed(manual_random_seed)\n","        torch.manual_seed(manual_random_seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","    fs_list = fs_list\n","    n_scales = len(scales)\n","    generators_list = []\n","    noise_amp_list = []\n","    if run_mode == 'inpainting':\n","        energy_list = [(sig[mask] ** 2).mean().item() for sig, mask in zip(signals_list, masks)]\n","    else:\n","        energy_list = [(sig ** 2).mean().item() for sig in signals_list]\n","    reconstruction_noise_list = []\n","    output_signals = []\n","    loss_vectors = []\n","\n","    for scale_idx in range(n_scales):\n","        output_signals_single_scale, loss_vectors_single_scale, netG, reconstruction_noise_list, noise_amp = train_single_scale(\n","                      scales, device, run_mode, hidden_channels_init, growing_hidden_channels_factor,  learning_rate, beta1, \n","                      scheduler_lr_decay, plot_losses, initial_noise_amp, noise_amp_factor, signals_list, fs_list, \n","                      generators_list, noise_amp_list, energy_list, reconstruction_noise_list, dilation_factors, output_folder, inputs_lengths)\n","\n","        # Write fake sound\n","        fake_sound = output_signals_single_scale['fake_signal'].squeeze()\n","        filename = 'fake@%dHz.wav' % fs_list[scale_idx]\n","        write_signal(os.path.join(output_folder, filename), fake_sound,\n","                     fs_list[scale_idx], overwrite=False)\n","\n","        # Write reconstructed sound\n","        reconstructed_sound = output_signals_single_scale['reconstructed_signal'].squeeze()\n","        filename = 'reconstructed@%dHz.wav' % fs_list[scale_idx]\n","        write_signal(os.path.join(output_folder, filename),\n","                     reconstructed_sound, fs_list[scale_idx], overwrite=False)\n","        torch.save(reconstruction_noise_list,\n","                   os.path.join(output_folder, 'reconstruction_noise_list.pt'))\n","\n","        generators_list.append(netG)\n","        noise_amp_list.append(noise_amp)\n","        output_signals.append(output_signals_single_scale)\n","        loss_vectors.append(loss_vectors_single_scale)\n","\n","    return output_signals, loss_vectors, generators_list, noise_amp_list, energy_list, reconstruction_noise_list\n","\n","\n","def train_single_scale(scales, device, run_mode, hidden_channels_init, growing_hidden_channels_factor,\n","                       learning_rate, beta1, scheduler_lr_decay, plot_losses, initial_noise_amp, noise_amp_factor, signals_list,\n","                        fs_list, generators_list, noise_amp_list, energy_list, reconstruction_noise_list, dilation_factors, output_folder, inputs_lengths):\n","    # Terminology: 0 is the higher scale (original signal, no downsampling). Higher scale means larger downsampling, e.g shorter signals\n","    n_scales = len(scales)\n","    current_scale = n_scales - len(generators_list) - 1\n","    scale_idx = n_scales - current_scale - 1\n","    input_signal = signals_list[scale_idx].to(device)\n","    current_fs = fs_list[scale_idx]\n","    N = len(input_signal)\n","\n","    if run_mode == 'inpainting':\n","        current_mask = masks[scale_idx]\n","        current_mask = current_mask\n","        current_holes = torch.Tensor([(int(idx[0] / Fs * current_fs), int(idx[1] / Fs * current_fs)) for idx in inpainting_indices]).to(device)\n","    else:\n","        current_holes = None\n","\n","    # Create inputs\n","    real_signal = input_signal.reshape(1, 1, N)\n","\n","    hidden_channels = hidden_channels_init if scale_idx == 0 else int(\n","        hidden_channels_init * growing_hidden_channels_factor)\n","\n","    scale_num = n_scales - scale_idx - 1\n","    pad_size = calc_pad_size(dilation_factors, filter_size)\n","    signal_padder = nn.ConstantPad1d(pad_size, 0)\n","\n","    # Initialize models\n","    netD = Discriminator(run_mode, current_holes, hidden_channels, dilation_factors, num_layers, device, filter_size).to(device)\n","    netD.apply(weights_init)\n","    netG = Generator(filter_size, hidden_channels, current_fs).to(device)\n","    netG.apply(weights_init)\n","    receptive_field = calc_receptive_field(filter_size, dilation_factors, current_fs)\n","    receptive_field_percent = 100 * receptive_field / 1e3 / (N / current_fs)\n","    print('Signal in scale %d has %d samples, sample rate is %d[Hz].' % (\n","        scale_num, N, current_fs))\n","    print('Total receptive field is %d[msec] (%.1f%% of input).' % (receptive_field, receptive_field_percent))\n","    with open(os.path.join(output_folder, 'log.txt'), 'a') as f:\n","        f.write('*' * 30 + ' Scale ' + str(scale_num) + ' (' + str(current_fs) + ' [Hz]) ' + '*' * 30)\n","        f.write('\\nreceptive_field = %d[msec] (%.1f%% of input)' % (receptive_field, receptive_field_percent))\n","        f.write('\\nsignal_energy = %.4f' % energy_list[scale_idx])\n","\n","    if scale_idx == 0:\n","        reconstruction_noise = get_noise(device, real_signal.shape)\n","    else:\n","        reconstruction_noise = torch.zeros(real_signal.shape, device=device)\n","        if run_mode == 'inpainting':\n","            reconstruction_noise[:, :, torch.logical_not(current_mask)] = get_noise(device, torch.nonzero(\n","                torch.logical_not(current_mask)).shape[0]).expand(1, 1, -1).to(device)\n","\n","    reconstruction_noise = signal_padder(reconstruction_noise)\n","\n","    if scale_idx > 1:\n","        netG.load_state_dict(\n","            torch.load('%s/netGScale%d.pth' % (output_folder, scale_idx - 1), map_location=device))\n","        netD.load_state_dict(\n","            torch.load('%s/netDScale%d.pth' % (output_folder, scale_idx - 1), map_location=device))\n","\n","    output_folder = output_folder\n","\n","    # Create optimizers\n","    optimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n","    optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n","    schedulerD = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizerD, milestones=scheduler_milestones,\n","                                                      gamma=scheduler_lr_decay)\n","    schedulerG = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizerG, milestones=scheduler_milestones,\n","                                                      gamma=scheduler_lr_decay)\n","\n","    # Initialize error vectors\n","    v_err_real = np.zeros(num_epochs, )\n","    v_err_fake = np.zeros(num_epochs, )\n","    v_gp = np.zeros(num_epochs, )\n","    v_rec_loss = np.zeros(num_epochs, )\n","\n","    epochs_start_time = time.time()\n","    # prepare inputs for gradient penalty\n","    if not run_mode == 'inpainting':\n","        D_out_shape = torch.Size((1, 1, N - 2 * pad_size))\n","        _grad_outputs = torch.ones(D_out_shape, device=device)\n","    grad_pen_alpha_vec = torch.rand(num_epochs).to(device)\n","\n","    inputs_lengths = inputs_lengths\n","    for epoch_num in range(num_epochs):\n","        print_progress = epoch_num % 100 == 0\n","        # Create noise\n","        noise_signal = get_noise(device, real_signal.shape)\n","        noise_signal = signal_padder(noise_signal)\n","        #################################################################\n","        # Optimize D by maximizing D(realSignal)+(1-D(G(noise_signal))) #\n","        #################################################################\n","        netD.zero_grad()\n","        # Run on real signal\n","        not_valid_idx_start = []\n","        not_valid_idx_end = []\n","        if run_mode == 'inpainting':\n","            out_D_real = netD(real_signal, use_mask=True)\n","            tot_samples = out_D_real.shape[2]\n","            not_valid_idx_start = [int(idx[0] - receptive_field / 1e3 * current_fs + 1) for idx in current_holes]\n","            not_valid_idx_end = [int(idx[1] + 1) for idx in current_holes]  # +1 is because of pe filter\n","            out_D_real_cp = out_D_real.clone()\n","            out_D_real = out_D_real_cp[:, :, :not_valid_idx_start[0]]\n","            if len(current_holes) > 1:\n","                for i in range(len(current_holes) - 1):\n","                    out_D_real = torch.cat((out_D_real, out_D_real_cp[:, :, not_valid_idx_end[i] + 1:not_valid_idx_start[i+1]]), dim=2)\n","            out_D_real = torch.cat((out_D_real, out_D_real_cp[:, :, not_valid_idx_end[-1] + 1:]), dim=2)\n","            mask_ratio = tot_samples / out_D_real.shape[2]\n","        else:\n","            mask_ratio = 1\n","            out_D_real = netD(real_signal)\n","        err_real_D = -out_D_real.mean()\n","        err_real_D.backward(retain_graph=True)\n","        err_real_D = err_real_D.detach()\n","        if print_progress or plot_losses:\n","            err_real_D_val = err_real_D.item()\n","\n","        if epoch_num == 0:\n","            if run_mode == 'inpainting':\n","                D_out_shape = out_D_real.shape\n","                _grad_outputs = torch.ones(D_out_shape, device=device)\n","            if scale_idx == 0:  # We are at coarsest scale\n","                prev_signal = torch.full(noise_signal.shape, 0, device=device, dtype=noise_signal.dtype)\n","                prev_reconstructed_signal = torch.zeros(reconstruction_noise.shape, device=device)\n","                noise_amp = initial_noise_amp\n","            else:\n","                prev_signal = draw_signal(generators_list, inputs_lengths, fs_list, noise_amp_list, filter_size, dilation_factors, device)\n","                prev_signal = signal_padder(prev_signal)\n","                prev_reconstructed_signal = draw_signal(generators_list, inputs_lengths,\n","                                                        fs_list,\n","                                                        noise_amp_list, filter_size, dilation_factors, device,\n","                                                        reconstruction_noise_list)\n","                prev_reconstructed_signal = signal_padder(prev_reconstructed_signal)\n","                innovation = energy_list[scale_idx] - energy_list[scale_idx - 1]\n","                energy_diff = torch.sqrt(torch.Tensor([innovation])).to(device)\n","                noise_amp = noise_amp_factor * max(torch.Tensor([0]).to(device),\n","                                                          energy_diff)\n","\n","            if scale_idx == 1 and add_cond_noise:\n","                noise_amp = prev_reconstructed_signal.std()\n","\n","            with open(os.path.join(output_folder, 'log.txt'), 'a') as f:\n","                f.write('\\nnoise_amp: %.6f' % noise_amp)\n","\n","            reconstruction_noise = reconstruction_noise * noise_amp\n","            reconstruction_noise_list.append(reconstruction_noise)\n","        else:\n","            if scale_idx > 0:\n","                prev_signal = draw_signal(generators_list, inputs_lengths, fs_list, noise_amp_list, filter_size, dilation_factors, device)\n","                prev_signal = signal_padder(prev_signal)\n","\n","        input_noise = noise_signal * noise_amp\n","\n","        # Run on fake signal\n","        fake_signal = netG((input_noise + prev_signal).detach(), prev_signal)\n","        out_D_fake = netD(fake_signal.detach())\n","        err_fake_D = out_D_fake.mean()\n","        del out_D_real, out_D_fake\n","        err_fake_D.backward(retain_graph=True)\n","        err_fake_D = err_fake_D.detach()\n","        if print_progress or plot_losses:\n","            err_fake_D_val = err_fake_D.item()\n","\n","        lambda_grad=0.01\n","        gradient_penalty = calc_gradient_penalty(run_mode, current_holes, netD, real_signal, fake_signal, lambda_grad,\n","                                                 grad_pen_alpha_vec[epoch_num], _grad_outputs, mask_ratio)\n","        gradient_penalty.backward()\n","        if print_progress or plot_losses:\n","            gradient_penalty_val = gradient_penalty.item()\n","        del gradient_penalty\n","\n","        optimizerD.step()\n","\n","        if plot_losses:\n","            v_err_real[epoch_num] = err_real_D_val\n","            v_err_fake[epoch_num] = err_fake_D_val\n","            v_gp[epoch_num] = gradient_penalty_val\n","\n","        #############################################\n","        # Update G by maximizing D(G(noise_signal)) #\n","        #############################################\n","        netG.zero_grad()\n","        output = netD(fake_signal)\n","        errG = -output.mean()\n","        del output\n","        errG.backward(retain_graph=True)\n","        errG = errG.detach()\n","        if print_progress or plot_losses:\n","            errG_val = errG.item()\n","        if scale_idx == 0:\n","            reconstructed_signal = netG((reconstruction_noise + prev_reconstructed_signal).detach(),\n","                                        prev_reconstructed_signal)\n","        else:\n","            reconstructed_signal = netG((reconstruction_noise + prev_reconstructed_signal).detach(),\n","                                        prev_reconstructed_signal)\n","        if alpha1 > 0:\n","            if run_mode == 'inpainting':\n","                rec_loss_t = alpha1 * torch.mean(\n","                    (real_signal[:, :, current_mask] - reconstructed_signal[:, :, current_mask]) ** 2)\n","            else:\n","                rec_loss_t = alpha1 * torch.mean((real_signal - reconstructed_signal) ** 2)\n","        else:\n","            rec_loss_t = 0\n","        if alpha2 > 0:\n","            multispec_loss_n_fft = (2048, 1024, 512)\n","            multispec_loss_hop_length = (240, 120, 50)\n","            multispec_loss_window_size = (1200, 600, 240)\n","            rec_loss_f = alpha2 * multi_scale_spectrogram_loss(multispec_loss_n_fft, multispec_loss_hop_length, multispec_loss_window_size,\n","                                                               current_holes, real_signal.permute(0, 2, 1),reconstructed_signal.permute(0, 2, 1))\n","        else:\n","            rec_loss_f = 0\n","        rec_loss = rec_loss_t + rec_loss_f\n","        rec_loss.backward(retain_graph=True)\n","        rec_loss = rec_loss.detach()\n","        if alpha1 > 0:\n","            rec_loss_t = rec_loss_t.detach()\n","        if alpha2 > 0:\n","            rec_loss_f = rec_loss_f.detach()\n","        if print_progress or plot_losses:\n","            rec_loss_val = rec_loss.item()\n","\n","        optimizerG.step()\n","\n","        if plot_losses:\n","            v_rec_loss[epoch_num] = rec_loss_val\n","\n","        if print_progress:\n","            print('[%d/%d] D(real): %.2f. D(fake): %.2f. rec_loss: %.4f. gp: %.4f ' % (\n","                epoch_num, num_epochs, -err_real_D_val, err_fake_D_val, rec_loss_val, gradient_penalty_val))\n","\n","        schedulerD.step()\n","        schedulerG.step()\n","\n","        # Some memory cleanup\n","        fake_signal = fake_signal.detach()\n","        reconstructed_signal = reconstructed_signal.detach()\n","        if epoch_num < num_epochs - 1:\n","            del fake_signal, reconstructed_signal, rec_loss, rec_loss_t, rec_loss_f\n","        del noise_signal, input_noise\n","        if scale_idx > 0:\n","            del prev_signal\n","\n","    epochs_stop_time = time.time()\n","    runtime_msg = 'Total time in scale %d: %d[sec] (%.2f[sec]/epoch on avg.). D(real): %f, D(fake): %f, rec_loss: %.4f. gp: %.4f' % (\n","        current_scale, epochs_stop_time - epochs_start_time,\n","        (epochs_stop_time - epochs_start_time) / num_epochs,\n","        -err_real_D_val, err_fake_D_val, rec_loss_val, gradient_penalty_val)\n","    print(runtime_msg)\n","    with open(os.path.join(output_folder, 'log.txt'), 'a') as f:\n","        f.write('\\n%s\\n' % runtime_msg)\n","\n","    # Save this scale models\n","    torch.save(netG.state_dict(), '%s/netGScale%d.pth' % (output_folder, scale_idx))\n","    torch.save(netD.state_dict(), '%s/netDScale%d.pth' % (output_folder, scale_idx))\n","    # Pack outputs\n","    if plot_losses:\n","        loss_vectors = {'v_err_real': v_err_real,\n","                        'v_err_fake': v_err_fake,\n","                        'v_rec_loss': v_rec_loss,\n","                        'v_gp': v_gp}\n","    else:\n","        loss_vectors = []\n","    fake_signal = fake_signal.detach().cpu().numpy()[:, 0, :]\n","    reconstructed_signal = reconstructed_signal.detach().cpu().numpy()[:, 0, :]\n","    output_signals = {'fake_signal': fake_signal, 'reconstructed_signal': reconstructed_signal}\n","    del fake_signal, real_signal, netD, _grad_outputs, grad_pen_alpha_vec, input_signal, reconstructed_signal, prev_reconstructed_signal, reconstruction_noise\n","    netG = reset_grads(netG, False)\n","    netG.eval()\n","    if is_cuda:\n","        torch.cuda.empty_cache()\n","    print('*' * 30 + ' Finished working on scale ' + str(current_scale) + ' ' + '*' * 30)\n","    return output_signals, loss_vectors, netG, reconstruction_noise_list, noise_amp"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uzV4gMXQZR92","outputId":"9fa0de1c-a948-4d3c-9e03-866dc6b673b5","executionInfo":{"status":"ok","timestamp":1664231685471,"user_tz":-60,"elapsed":8956015,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Working on file: S4_U.wav\n","Writing results to outputs_2\n","\n","Running on cuda:0\n","Signal in scale 15 has 7440 samples, sample rate is 320[Hz].\n","Total receptive field is 6378[msec] (27.4% of input).\n","[0/3000] D(real): 0.01. D(fake): 0.01. rec_loss: 0.0660. gp: 0.0092 \n","[100/3000] D(real): 0.08. D(fake): -0.01. rec_loss: 0.0376. gp: 0.0078 \n","[200/3000] D(real): 0.14. D(fake): 0.04. rec_loss: 0.0484. gp: 0.0068 \n","[300/3000] D(real): 0.16. D(fake): 0.02. rec_loss: 0.0425. gp: 0.0464 \n","[400/3000] D(real): 0.19. D(fake): 0.03. rec_loss: 0.0492. gp: 0.0091 \n","[500/3000] D(real): 0.33. D(fake): 0.28. rec_loss: 0.0535. gp: 0.0062 \n","[600/3000] D(real): 0.52. D(fake): 0.40. rec_loss: 0.0382. gp: 0.0225 \n","[700/3000] D(real): 0.42. D(fake): 0.32. rec_loss: 0.0404. gp: 0.0355 \n","[800/3000] D(real): 0.67. D(fake): 0.49. rec_loss: 0.0414. gp: 0.0053 \n","[900/3000] D(real): 0.64. D(fake): 0.46. rec_loss: 0.0430. gp: 0.0714 \n","[1000/3000] D(real): 0.77. D(fake): 0.46. rec_loss: 0.0426. gp: 0.0785 \n","[1100/3000] D(real): 0.64. D(fake): 0.36. rec_loss: 0.0588. gp: 0.0069 \n","[1200/3000] D(real): 0.58. D(fake): 0.30. rec_loss: 0.0536. gp: 0.4684 \n","[1300/3000] D(real): 0.75. D(fake): 0.53. rec_loss: 0.0501. gp: 0.0125 \n","[1400/3000] D(real): 0.85. D(fake): 0.48. rec_loss: 0.0487. gp: 0.1287 \n","[1500/3000] D(real): 0.67. D(fake): 0.42. rec_loss: 0.0504. gp: 0.0815 \n","[1600/3000] D(real): 0.77. D(fake): 0.33. rec_loss: 0.0582. gp: 0.1479 \n","[1700/3000] D(real): 0.97. D(fake): 0.39. rec_loss: 0.0666. gp: 0.0290 \n","[1800/3000] D(real): 0.58. D(fake): 0.12. rec_loss: 0.0643. gp: 0.0978 \n","[1900/3000] D(real): 0.80. D(fake): 0.22. rec_loss: 0.0559. gp: 0.0455 \n","[2000/3000] D(real): 0.89. D(fake): 0.68. rec_loss: 0.0560. gp: 0.0253 \n","[2100/3000] D(real): 0.89. D(fake): 0.56. rec_loss: 0.0595. gp: 0.0351 \n","[2200/3000] D(real): 0.96. D(fake): 0.54. rec_loss: 0.0585. gp: 0.1078 \n","[2300/3000] D(real): 0.99. D(fake): 0.49. rec_loss: 0.0581. gp: 0.0304 \n","[2400/3000] D(real): 1.00. D(fake): 0.47. rec_loss: 0.0589. gp: 0.0778 \n","[2500/3000] D(real): 1.08. D(fake): 0.48. rec_loss: 0.0610. gp: 0.2502 \n","[2600/3000] D(real): 1.14. D(fake): 0.45. rec_loss: 0.0602. gp: 0.1372 \n","[2700/3000] D(real): 1.20. D(fake): 0.42. rec_loss: 0.0597. gp: 0.2685 \n","[2800/3000] D(real): 1.14. D(fake): 0.42. rec_loss: 0.0589. gp: 0.1018 \n","[2900/3000] D(real): 1.11. D(fake): 0.31. rec_loss: 0.0554. gp: 0.0701 \n","Total time in scale 15: 108[sec] (0.04[sec]/epoch on avg.). D(real): 1.112388, D(fake): 0.305230, rec_loss: 0.0554. gp: 0.0701\n","****************************** Finished working on scale 15 ******************************\n","Signal in scale 14 has 9300 samples, sample rate is 400[Hz].\n","Total receptive field is 5102[msec] (21.9% of input).\n","[0/3000] D(real): -0.00. D(fake): -0.00. rec_loss: 0.0682. gp: 0.0087 \n","[100/3000] D(real): 0.07. D(fake): -0.10. rec_loss: 0.0731. gp: 0.0733 \n","[200/3000] D(real): 0.04. D(fake): -0.11. rec_loss: 0.0664. gp: 0.0274 \n","[300/3000] D(real): 0.04. D(fake): -0.29. rec_loss: 0.0661. gp: 0.1936 \n","[400/3000] D(real): -0.06. D(fake): -0.27. rec_loss: 0.0655. gp: 0.0510 \n","[500/3000] D(real): 0.21. D(fake): 0.12. rec_loss: 0.0674. gp: 0.0058 \n","[600/3000] D(real): 0.07. D(fake): 0.08. rec_loss: 0.0697. gp: 0.0133 \n","[700/3000] D(real): 0.11. D(fake): -0.05. rec_loss: 0.0796. gp: 0.0985 \n","[800/3000] D(real): 0.31. D(fake): 0.22. rec_loss: 0.0730. gp: 0.0149 \n","[900/3000] D(real): 0.24. D(fake): -0.01. rec_loss: 0.0734. gp: 0.0348 \n","[1000/3000] D(real): 0.69. D(fake): 0.42. rec_loss: 0.0706. gp: 0.0846 \n","[1100/3000] D(real): 0.51. D(fake): 0.42. rec_loss: 0.0624. gp: 0.0087 \n","[1200/3000] D(real): 0.59. D(fake): 0.30. rec_loss: 0.0636. gp: 0.0522 \n","[1300/3000] D(real): 0.95. D(fake): 0.75. rec_loss: 0.0621. gp: 0.0298 \n","[1400/3000] D(real): 0.91. D(fake): 0.81. rec_loss: 0.0657. gp: 0.0063 \n","[1500/3000] D(real): 1.37. D(fake): 1.32. rec_loss: 0.0660. gp: 0.0353 \n","[1600/3000] D(real): 1.41. D(fake): 1.25. rec_loss: 0.0597. gp: 0.0602 \n","[1700/3000] D(real): 1.52. D(fake): 1.11. rec_loss: 0.0590. gp: 0.0241 \n","[1800/3000] D(real): 1.28. D(fake): 0.90. rec_loss: 0.0557. gp: 0.0325 \n","[1900/3000] D(real): 1.84. D(fake): 1.41. rec_loss: 0.0574. gp: 0.0561 \n","[2000/3000] D(real): 1.97. D(fake): 1.59. rec_loss: 0.0559. gp: 0.2340 \n","[2100/3000] D(real): 2.00. D(fake): 1.70. rec_loss: 0.0536. gp: 0.1363 \n","[2200/3000] D(real): 2.01. D(fake): 1.61. rec_loss: 0.0529. gp: 0.1443 \n","[2300/3000] D(real): 2.04. D(fake): 1.60. rec_loss: 0.0527. gp: 0.0947 \n","[2400/3000] D(real): 2.09. D(fake): 1.68. rec_loss: 0.0520. gp: 0.0074 \n","[2500/3000] D(real): 2.10. D(fake): 1.68. rec_loss: 0.0530. gp: 0.3780 \n","[2600/3000] D(real): 2.10. D(fake): 1.67. rec_loss: 0.0510. gp: 0.1294 \n","[2700/3000] D(real): 2.17. D(fake): 1.66. rec_loss: 0.0505. gp: 0.0058 \n","[2800/3000] D(real): 2.18. D(fake): 1.66. rec_loss: 0.0503. gp: 0.1797 \n","[2900/3000] D(real): 2.25. D(fake): 1.73. rec_loss: 0.0504. gp: 0.2189 \n","Total time in scale 14: 280[sec] (0.09[sec]/epoch on avg.). D(real): 2.249746, D(fake): 1.729366, rec_loss: 0.0504. gp: 0.2189\n","****************************** Finished working on scale 14 ******************************\n","Signal in scale 13 has 11625 samples, sample rate is 500[Hz].\n","Total receptive field is 4082[msec] (17.6% of input).\n","[0/3000] D(real): 1.79. D(fake): 1.73. rec_loss: 0.0788. gp: 0.2613 \n","[100/3000] D(real): 1.64. D(fake): 1.52. rec_loss: 0.0598. gp: 0.0232 \n","[200/3000] D(real): 1.48. D(fake): 1.25. rec_loss: 0.0579. gp: 0.0667 \n","[300/3000] D(real): 1.44. D(fake): 1.09. rec_loss: 0.0562. gp: 0.0716 \n","[400/3000] D(real): 1.61. D(fake): 1.29. rec_loss: 0.0566. gp: 0.1475 \n","[500/3000] D(real): 1.44. D(fake): 1.10. rec_loss: 0.0555. gp: 0.0653 \n","[600/3000] D(real): 1.65. D(fake): 1.24. rec_loss: 0.0541. gp: 0.3013 \n","[700/3000] D(real): 1.88. D(fake): 1.42. rec_loss: 0.0554. gp: 0.2175 \n","[800/3000] D(real): 1.76. D(fake): 1.56. rec_loss: 0.0559. gp: 0.0297 \n","[900/3000] D(real): 2.30. D(fake): 1.80. rec_loss: 0.0542. gp: 0.0811 \n","[1000/3000] D(real): 2.21. D(fake): 1.76. rec_loss: 0.0527. gp: 0.1894 \n","[1100/3000] D(real): 2.49. D(fake): 1.92. rec_loss: 0.0527. gp: 0.2587 \n","[1200/3000] D(real): 2.79. D(fake): 2.25. rec_loss: 0.0529. gp: 0.2950 \n","[1300/3000] D(real): 2.14. D(fake): 1.91. rec_loss: 0.0530. gp: 0.0685 \n","[1400/3000] D(real): 2.19. D(fake): 1.94. rec_loss: 0.0527. gp: 0.0988 \n","[1500/3000] D(real): 2.91. D(fake): 2.26. rec_loss: 0.0499. gp: 0.2924 \n","[1600/3000] D(real): 2.71. D(fake): 2.34. rec_loss: 0.0509. gp: 0.2054 \n","[1700/3000] D(real): 2.83. D(fake): 2.37. rec_loss: 0.0491. gp: 0.0748 \n","[1800/3000] D(real): 2.60. D(fake): 2.07. rec_loss: 0.0521. gp: 0.1326 \n","[1900/3000] D(real): 2.27. D(fake): 2.10. rec_loss: 0.0495. gp: 0.0704 \n","[2000/3000] D(real): 3.18. D(fake): 2.20. rec_loss: 0.0500. gp: 0.7358 \n","[2100/3000] D(real): 2.50. D(fake): 2.03. rec_loss: 0.0474. gp: 0.1325 \n","[2200/3000] D(real): 3.27. D(fake): 2.19. rec_loss: 0.0468. gp: 0.2395 \n","[2300/3000] D(real): 3.43. D(fake): 2.08. rec_loss: 0.0468. gp: 0.1763 \n","[2400/3000] D(real): 3.48. D(fake): 2.30. rec_loss: 0.0463. gp: 0.2320 \n","[2500/3000] D(real): 3.48. D(fake): 2.21. rec_loss: 0.0464. gp: 0.2126 \n","[2600/3000] D(real): 3.52. D(fake): 2.29. rec_loss: 0.0469. gp: 0.3488 \n","[2700/3000] D(real): 3.47. D(fake): 2.27. rec_loss: 0.0506. gp: 0.5492 \n","[2800/3000] D(real): 3.27. D(fake): 2.30. rec_loss: 0.0470. gp: 0.2078 \n","[2900/3000] D(real): 3.50. D(fake): 2.49. rec_loss: 0.0491. gp: 0.4690 \n","Total time in scale 13: 367[sec] (0.12[sec]/epoch on avg.). D(real): 3.497501, D(fake): 2.485282, rec_loss: 0.0491. gp: 0.4690\n","****************************** Finished working on scale 13 ******************************\n","Signal in scale 12 has 14880 samples, sample rate is 640[Hz].\n","Total receptive field is 3189[msec] (13.7% of input).\n","[0/3000] D(real): 2.33. D(fake): 2.31. rec_loss: 0.0775. gp: 0.8127 \n","[100/3000] D(real): 2.86. D(fake): 2.64. rec_loss: 0.0600. gp: 0.0489 \n","[200/3000] D(real): 3.07. D(fake): 2.78. rec_loss: 0.0582. gp: 0.0818 \n","[300/3000] D(real): 3.22. D(fake): 2.88. rec_loss: 0.0574. gp: 0.1902 \n","[400/3000] D(real): 3.29. D(fake): 2.92. rec_loss: 0.0568. gp: 0.2088 \n","[500/3000] D(real): 3.29. D(fake): 3.02. rec_loss: 0.0569. gp: 0.0598 \n","[600/3000] D(real): 3.19. D(fake): 2.89. rec_loss: 0.0548. gp: 0.0466 \n","[700/3000] D(real): 3.13. D(fake): 2.89. rec_loss: 0.0587. gp: 0.0705 \n","[800/3000] D(real): 3.08. D(fake): 2.76. rec_loss: 0.0545. gp: 0.1736 \n","[900/3000] D(real): 2.95. D(fake): 2.71. rec_loss: 0.0540. gp: 0.2094 \n","[1000/3000] D(real): 3.32. D(fake): 2.92. rec_loss: 0.0543. gp: 0.0350 \n","[1100/3000] D(real): 3.63. D(fake): 3.07. rec_loss: 0.0551. gp: 0.1683 \n","[1200/3000] D(real): 3.42. D(fake): 2.83. rec_loss: 0.0529. gp: 0.2168 \n","[1300/3000] D(real): 2.97. D(fake): 2.71. rec_loss: 0.0537. gp: 0.0160 \n","[1400/3000] D(real): 3.68. D(fake): 2.95. rec_loss: 0.0516. gp: 0.2005 \n","[1500/3000] D(real): 3.76. D(fake): 3.03. rec_loss: 0.0534. gp: 0.1873 \n","[1600/3000] D(real): 3.20. D(fake): 2.86. rec_loss: 0.0521. gp: 0.2061 \n","[1700/3000] D(real): 3.75. D(fake): 3.12. rec_loss: 0.0517. gp: 0.2062 \n","[1800/3000] D(real): 3.42. D(fake): 3.24. rec_loss: 0.0568. gp: 0.0896 \n","[1900/3000] D(real): 3.57. D(fake): 2.90. rec_loss: 0.0509. gp: 0.2120 \n","[2000/3000] D(real): 3.62. D(fake): 2.88. rec_loss: 0.0504. gp: 0.1471 \n","[2100/3000] D(real): 3.96. D(fake): 2.92. rec_loss: 0.0494. gp: 0.4132 \n","[2200/3000] D(real): 3.63. D(fake): 2.74. rec_loss: 0.0494. gp: 0.2168 \n","[2300/3000] D(real): 4.12. D(fake): 2.92. rec_loss: 0.0547. gp: 0.6392 \n","[2400/3000] D(real): 4.19. D(fake): 3.10. rec_loss: 0.0488. gp: 0.9758 \n","[2500/3000] D(real): 4.13. D(fake): 2.91. rec_loss: 0.0513. gp: 0.3973 \n","[2600/3000] D(real): 4.07. D(fake): 2.96. rec_loss: 0.0579. gp: 1.0655 \n","[2700/3000] D(real): 4.22. D(fake): 2.93. rec_loss: 0.0516. gp: 0.3849 \n","[2800/3000] D(real): 4.26. D(fake): 2.96. rec_loss: 0.0508. gp: 1.2045 \n","[2900/3000] D(real): 3.91. D(fake): 2.90. rec_loss: 0.0528. gp: 0.2626 \n","Total time in scale 12: 463[sec] (0.15[sec]/epoch on avg.). D(real): 3.908088, D(fake): 2.899742, rec_loss: 0.0528. gp: 0.2626\n","****************************** Finished working on scale 12 ******************************\n","Signal in scale 11 has 18600 samples, sample rate is 800[Hz].\n","Total receptive field is 2551[msec] (11.0% of input).\n","[0/3000] D(real): 2.82. D(fake): 2.87. rec_loss: 0.0785. gp: 0.2615 \n","[100/3000] D(real): 3.00. D(fake): 2.82. rec_loss: 0.0631. gp: 0.0849 \n","[200/3000] D(real): 2.68. D(fake): 2.54. rec_loss: 0.0700. gp: 0.0680 \n","[300/3000] D(real): 2.84. D(fake): 2.61. rec_loss: 0.0612. gp: 0.1241 \n","[400/3000] D(real): 2.89. D(fake): 2.60. rec_loss: 0.0589. gp: 0.1784 \n","[500/3000] D(real): 3.26. D(fake): 2.76. rec_loss: 0.0598. gp: 0.1587 \n","[600/3000] D(real): 2.86. D(fake): 2.79. rec_loss: 0.0596. gp: 0.0301 \n","[700/3000] D(real): 3.34. D(fake): 3.01. rec_loss: 0.0575. gp: 0.0647 \n","[800/3000] D(real): 3.23. D(fake): 2.96. rec_loss: 0.0571. gp: 0.0932 \n","[900/3000] D(real): 3.20. D(fake): 2.89. rec_loss: 0.0559. gp: 0.0540 \n","[1000/3000] D(real): 3.69. D(fake): 3.14. rec_loss: 0.0561. gp: 0.2739 \n","[1100/3000] D(real): 3.56. D(fake): 3.27. rec_loss: 0.0555. gp: 0.1214 \n","[1200/3000] D(real): 3.22. D(fake): 2.93. rec_loss: 0.0555. gp: 0.0849 \n","[1300/3000] D(real): 3.68. D(fake): 3.31. rec_loss: 0.0546. gp: 0.0555 \n","[1400/3000] D(real): 3.56. D(fake): 2.82. rec_loss: 0.0552. gp: 0.1968 \n","[1500/3000] D(real): 3.84. D(fake): 3.22. rec_loss: 0.0558. gp: 0.2653 \n","[1600/3000] D(real): 3.79. D(fake): 3.35. rec_loss: 0.0537. gp: 0.1802 \n","[1700/3000] D(real): 3.87. D(fake): 3.36. rec_loss: 0.0583. gp: 0.1213 \n","[1800/3000] D(real): 4.20. D(fake): 3.52. rec_loss: 0.0534. gp: 0.1378 \n","[1900/3000] D(real): 4.25. D(fake): 3.48. rec_loss: 0.0531. gp: 0.4887 \n","[2000/3000] D(real): 4.44. D(fake): 3.77. rec_loss: 0.0531. gp: 0.1531 \n","[2100/3000] D(real): 4.62. D(fake): 3.72. rec_loss: 0.0519. gp: 0.1685 \n","[2200/3000] D(real): 4.64. D(fake): 3.72. rec_loss: 0.0515. gp: 0.2632 \n","[2300/3000] D(real): 4.76. D(fake): 3.88. rec_loss: 0.0514. gp: 0.2005 \n","[2400/3000] D(real): 4.80. D(fake): 3.71. rec_loss: 0.0517. gp: 0.4520 \n","[2500/3000] D(real): 4.74. D(fake): 3.60. rec_loss: 0.0513. gp: 0.2856 \n","[2600/3000] D(real): 4.68. D(fake): 3.65. rec_loss: 0.0514. gp: 0.1723 \n","[2700/3000] D(real): 4.69. D(fake): 3.47. rec_loss: 0.0524. gp: 0.4326 \n","[2800/3000] D(real): 4.51. D(fake): 3.57. rec_loss: 0.0515. gp: 0.2302 \n","[2900/3000] D(real): 4.63. D(fake): 3.51. rec_loss: 0.0533. gp: 0.3479 \n","Total time in scale 11: 604[sec] (0.20[sec]/epoch on avg.). D(real): 4.628652, D(fake): 3.514981, rec_loss: 0.0533. gp: 0.3479\n","****************************** Finished working on scale 11 ******************************\n","Signal in scale 10 has 23250 samples, sample rate is 1000[Hz].\n","Total receptive field is 2041[msec] (8.8% of input).\n","[0/3000] D(real): 3.31. D(fake): 3.33. rec_loss: 0.0859. gp: 0.3835 \n","[100/3000] D(real): 3.36. D(fake): 3.21. rec_loss: 0.0669. gp: 0.0378 \n","[200/3000] D(real): 3.26. D(fake): 3.14. rec_loss: 0.0668. gp: 0.2183 \n","[300/3000] D(real): 2.99. D(fake): 2.81. rec_loss: 0.0647. gp: 0.0419 \n","[400/3000] D(real): 3.09. D(fake): 2.86. rec_loss: 0.0648. gp: 0.1304 \n","[500/3000] D(real): 3.01. D(fake): 2.63. rec_loss: 0.0632. gp: 0.2644 \n","[600/3000] D(real): 2.79. D(fake): 2.44. rec_loss: 0.0622. gp: 0.1162 \n","[700/3000] D(real): 3.08. D(fake): 2.75. rec_loss: 0.0619. gp: 0.3645 \n","[800/3000] D(real): 2.95. D(fake): 2.61. rec_loss: 0.0611. gp: 0.1767 \n","[900/3000] D(real): 2.64. D(fake): 2.45. rec_loss: 0.0609. gp: 0.1084 \n","[1000/3000] D(real): 2.68. D(fake): 2.37. rec_loss: 0.0597. gp: 0.0899 \n","[1100/3000] D(real): 3.01. D(fake): 2.51. rec_loss: 0.0591. gp: 0.3670 \n","[1200/3000] D(real): 3.03. D(fake): 2.68. rec_loss: 0.0594. gp: 0.1706 \n","[1300/3000] D(real): 3.10. D(fake): 2.63. rec_loss: 0.0584. gp: 0.2457 \n","[1400/3000] D(real): 2.86. D(fake): 2.49. rec_loss: 0.0583. gp: 0.0659 \n","[1500/3000] D(real): 2.87. D(fake): 2.53. rec_loss: 0.0580. gp: 0.1248 \n","[1600/3000] D(real): 3.13. D(fake): 2.53. rec_loss: 0.0570. gp: 0.1876 \n","[1700/3000] D(real): 3.16. D(fake): 2.74. rec_loss: 0.0561. gp: 0.1395 \n","[1800/3000] D(real): 3.26. D(fake): 2.52. rec_loss: 0.0569. gp: 0.3108 \n","[1900/3000] D(real): 3.24. D(fake): 2.66. rec_loss: 0.0572. gp: 0.3150 \n","[2000/3000] D(real): 3.13. D(fake): 2.76. rec_loss: 0.0555. gp: 0.1308 \n","[2100/3000] D(real): 3.23. D(fake): 2.62. rec_loss: 0.0549. gp: 0.1176 \n","[2200/3000] D(real): 3.28. D(fake): 2.79. rec_loss: 0.0545. gp: 0.1771 \n","[2300/3000] D(real): 3.30. D(fake): 2.64. rec_loss: 0.0543. gp: 0.1378 \n","[2400/3000] D(real): 3.32. D(fake): 2.72. rec_loss: 0.0543. gp: 0.3290 \n","[2500/3000] D(real): 3.34. D(fake): 2.73. rec_loss: 0.0542. gp: 0.1399 \n","[2600/3000] D(real): 3.37. D(fake): 2.61. rec_loss: 0.0541. gp: 0.4678 \n","[2700/3000] D(real): 3.37. D(fake): 2.74. rec_loss: 0.0541. gp: 0.6159 \n","[2800/3000] D(real): 3.31. D(fake): 2.57. rec_loss: 0.0539. gp: 0.2653 \n","[2900/3000] D(real): 3.32. D(fake): 2.70. rec_loss: 0.0538. gp: 0.4431 \n","Total time in scale 10: 753[sec] (0.25[sec]/epoch on avg.). D(real): 3.324632, D(fake): 2.700172, rec_loss: 0.0538. gp: 0.4431\n","****************************** Finished working on scale 10 ******************************\n","Signal in scale 9 has 29760 samples, sample rate is 1280[Hz].\n","Total receptive field is 1594[msec] (6.9% of input).\n","[0/3000] D(real): 2.37. D(fake): 2.39. rec_loss: 0.1067. gp: 0.2126 \n","[100/3000] D(real): 2.36. D(fake): 2.27. rec_loss: 0.0721. gp: 0.1093 \n","[200/3000] D(real): 2.12. D(fake): 2.04. rec_loss: 0.0699. gp: 0.0664 \n","[300/3000] D(real): 1.89. D(fake): 1.72. rec_loss: 0.0669. gp: 0.1184 \n","[400/3000] D(real): 1.82. D(fake): 1.36. rec_loss: 0.0679. gp: 0.1434 \n","[500/3000] D(real): 1.80. D(fake): 1.65. rec_loss: 0.0648. gp: 0.1113 \n","[600/3000] D(real): 1.98. D(fake): 1.76. rec_loss: 0.0647. gp: 0.1141 \n","[700/3000] D(real): 1.85. D(fake): 1.56. rec_loss: 0.0637. gp: 0.0930 \n","[800/3000] D(real): 1.96. D(fake): 1.76. rec_loss: 0.0631. gp: 0.1537 \n","[900/3000] D(real): 2.01. D(fake): 1.69. rec_loss: 0.0635. gp: 0.1788 \n","[1000/3000] D(real): 1.82. D(fake): 1.54. rec_loss: 0.0635. gp: 0.0783 \n","[1100/3000] D(real): 1.80. D(fake): 1.50. rec_loss: 0.0635. gp: 0.1093 \n","[1200/3000] D(real): 1.95. D(fake): 1.67. rec_loss: 0.0633. gp: 0.1701 \n","[1300/3000] D(real): 1.67. D(fake): 1.52. rec_loss: 0.0615. gp: 0.1207 \n","[1400/3000] D(real): 1.60. D(fake): 1.36. rec_loss: 0.0621. gp: 0.0933 \n","[1500/3000] D(real): 2.01. D(fake): 1.46. rec_loss: 0.0627. gp: 0.3228 \n","[1600/3000] D(real): 1.95. D(fake): 1.41. rec_loss: 0.0613. gp: 0.3898 \n","[1700/3000] D(real): 1.84. D(fake): 1.62. rec_loss: 0.0592. gp: 0.1390 \n","[1800/3000] D(real): 2.27. D(fake): 1.87. rec_loss: 0.0611. gp: 0.1193 \n","[1900/3000] D(real): 1.67. D(fake): 1.52. rec_loss: 0.0597. gp: 0.0353 \n","[2000/3000] D(real): 2.27. D(fake): 1.87. rec_loss: 0.0579. gp: 0.1247 \n","[2100/3000] D(real): 2.36. D(fake): 1.95. rec_loss: 0.0569. gp: 0.1023 \n","[2200/3000] D(real): 2.38. D(fake): 1.95. rec_loss: 0.0566. gp: 0.4202 \n","[2300/3000] D(real): 2.35. D(fake): 1.81. rec_loss: 0.0566. gp: 0.0947 \n","[2400/3000] D(real): 2.40. D(fake): 1.92. rec_loss: 0.0564. gp: 0.1076 \n","[2500/3000] D(real): 2.38. D(fake): 1.77. rec_loss: 0.0565. gp: 0.2924 \n","[2600/3000] D(real): 2.43. D(fake): 1.91. rec_loss: 0.0563. gp: 0.1113 \n","[2700/3000] D(real): 2.41. D(fake): 1.98. rec_loss: 0.0562. gp: 0.2661 \n","[2800/3000] D(real): 2.44. D(fake): 1.90. rec_loss: 0.0562. gp: 0.3046 \n","[2900/3000] D(real): 2.44. D(fake): 1.83. rec_loss: 0.0563. gp: 0.2596 \n","Total time in scale 9: 940[sec] (0.31[sec]/epoch on avg.). D(real): 2.436994, D(fake): 1.832091, rec_loss: 0.0563. gp: 0.2596\n","****************************** Finished working on scale 9 ******************************\n","Signal in scale 8 has 37200 samples, sample rate is 1600[Hz].\n","Total receptive field is 1275[msec] (5.5% of input).\n","[0/3000] D(real): 1.64. D(fake): 1.65. rec_loss: 0.1007. gp: 0.3348 \n","[100/3000] D(real): 1.69. D(fake): 1.60. rec_loss: 0.0775. gp: 0.0553 \n","[200/3000] D(real): 1.56. D(fake): 1.43. rec_loss: 0.0738. gp: 0.0245 \n","[300/3000] D(real): 1.64. D(fake): 1.54. rec_loss: 0.0718. gp: 0.1342 \n","[400/3000] D(real): 1.71. D(fake): 1.55. rec_loss: 0.0702. gp: 0.0963 \n","[500/3000] D(real): 1.76. D(fake): 1.49. rec_loss: 0.0702. gp: 0.0817 \n","[600/3000] D(real): 1.76. D(fake): 1.56. rec_loss: 0.0692. gp: 0.0508 \n","[700/3000] D(real): 1.51. D(fake): 1.38. rec_loss: 0.0676. gp: 0.0844 \n","[800/3000] D(real): 1.62. D(fake): 1.32. rec_loss: 0.0688. gp: 0.1394 \n","[900/3000] D(real): 1.48. D(fake): 1.21. rec_loss: 0.0672. gp: 0.1091 \n","[1000/3000] D(real): 1.50. D(fake): 1.23. rec_loss: 0.0662. gp: 0.0623 \n","[1100/3000] D(real): 1.58. D(fake): 1.26. rec_loss: 0.0661. gp: 0.1346 \n","[1200/3000] D(real): 1.79. D(fake): 1.37. rec_loss: 0.0666. gp: 0.1522 \n","[1300/3000] D(real): 1.32. D(fake): 1.16. rec_loss: 0.0657. gp: 0.0200 \n","[1400/3000] D(real): 1.35. D(fake): 0.92. rec_loss: 0.0648. gp: 0.1293 \n","[1500/3000] D(real): 1.56. D(fake): 1.16. rec_loss: 0.0649. gp: 0.0957 \n","[1600/3000] D(real): 1.51. D(fake): 1.16. rec_loss: 0.0638. gp: 0.1778 \n","[1700/3000] D(real): 1.32. D(fake): 1.11. rec_loss: 0.0640. gp: 0.0443 \n","[1800/3000] D(real): 1.64. D(fake): 1.24. rec_loss: 0.0627. gp: 0.1140 \n","[1900/3000] D(real): 1.73. D(fake): 1.39. rec_loss: 0.0626. gp: 0.1218 \n","[2000/3000] D(real): 1.51. D(fake): 1.19. rec_loss: 0.0622. gp: 0.0670 \n","[2100/3000] D(real): 1.60. D(fake): 1.16. rec_loss: 0.0613. gp: 0.2274 \n","[2200/3000] D(real): 1.65. D(fake): 1.19. rec_loss: 0.0610. gp: 0.1639 \n","[2300/3000] D(real): 1.71. D(fake): 1.31. rec_loss: 0.0610. gp: 0.2088 \n","[2400/3000] D(real): 1.76. D(fake): 1.35. rec_loss: 0.0608. gp: 0.1291 \n","[2500/3000] D(real): 1.79. D(fake): 1.25. rec_loss: 0.0607. gp: 0.1628 \n","[2600/3000] D(real): 1.70. D(fake): 1.16. rec_loss: 0.0605. gp: 0.1673 \n","[2700/3000] D(real): 1.67. D(fake): 1.18. rec_loss: 0.0604. gp: 0.1447 \n","[2800/3000] D(real): 1.85. D(fake): 1.29. rec_loss: 0.0604. gp: 0.5904 \n","[2900/3000] D(real): 1.65. D(fake): 1.17. rec_loss: 0.0605. gp: 0.1372 \n","Total time in scale 8: 1164[sec] (0.39[sec]/epoch on avg.). D(real): 1.647696, D(fake): 1.166555, rec_loss: 0.0605. gp: 0.1372\n","****************************** Finished working on scale 8 ******************************\n","Signal in scale 7 has 46500 samples, sample rate is 2000[Hz].\n","Total receptive field is 1020[msec] (4.4% of input).\n","[0/3000] D(real): 1.04. D(fake): 1.06. rec_loss: 0.1167. gp: 0.1383 \n","[100/3000] D(real): 1.00. D(fake): 0.93. rec_loss: 0.0860. gp: 0.0542 \n","[200/3000] D(real): 0.84. D(fake): 0.75. rec_loss: 0.0832. gp: 0.0249 \n","[300/3000] D(real): 1.01. D(fake): 0.83. rec_loss: 0.0809. gp: 0.0713 \n","[400/3000] D(real): 1.19. D(fake): 1.07. rec_loss: 0.0813. gp: 0.0225 \n","[500/3000] D(real): 1.36. D(fake): 1.13. rec_loss: 0.0796. gp: 0.0709 \n","[600/3000] D(real): 1.31. D(fake): 1.07. rec_loss: 0.0780. gp: 0.0992 \n","[700/3000] D(real): 1.21. D(fake): 0.92. rec_loss: 0.0776. gp: 0.2020 \n","[800/3000] D(real): 1.17. D(fake): 0.92. rec_loss: 0.0776. gp: 0.0742 \n","[900/3000] D(real): 1.16. D(fake): 0.89. rec_loss: 0.0760. gp: 0.0428 \n","[1000/3000] D(real): 1.07. D(fake): 0.89. rec_loss: 0.0761. gp: 0.0687 \n","[1100/3000] D(real): 0.96. D(fake): 0.78. rec_loss: 0.0761. gp: 0.0917 \n","[1200/3000] D(real): 0.78. D(fake): 0.72. rec_loss: 0.0760. gp: 0.0208 \n","[1300/3000] D(real): 1.08. D(fake): 0.87. rec_loss: 0.0733. gp: 0.0948 \n","[1400/3000] D(real): 0.99. D(fake): 0.72. rec_loss: 0.0751. gp: 0.1553 \n","[1500/3000] D(real): 0.92. D(fake): 0.48. rec_loss: 0.0735. gp: 0.0897 \n","[1600/3000] D(real): 1.08. D(fake): 0.66. rec_loss: 0.0728. gp: 0.1433 \n","[1700/3000] D(real): 0.80. D(fake): 0.62. rec_loss: 0.0732. gp: 0.0268 \n","[1800/3000] D(real): 1.13. D(fake): 0.80. rec_loss: 0.0729. gp: 0.1366 \n","[1900/3000] D(real): 1.20. D(fake): 0.66. rec_loss: 0.0717. gp: 0.3418 \n","[2000/3000] D(real): 0.97. D(fake): 0.55. rec_loss: 0.0722. gp: 0.1064 \n","[2100/3000] D(real): 1.22. D(fake): 0.66. rec_loss: 0.0708. gp: 0.1409 \n","[2200/3000] D(real): 1.08. D(fake): 0.53. rec_loss: 0.0707. gp: 0.1298 \n","[2300/3000] D(real): 1.16. D(fake): 0.51. rec_loss: 0.0710. gp: 0.0957 \n","[2400/3000] D(real): 1.06. D(fake): 0.65. rec_loss: 0.0705. gp: 0.1065 \n","[2500/3000] D(real): 1.20. D(fake): 0.61. rec_loss: 0.0701. gp: 0.1534 \n","[2600/3000] D(real): 1.21. D(fake): 0.58. rec_loss: 0.0704. gp: 0.4465 \n","[2700/3000] D(real): 1.12. D(fake): 0.57. rec_loss: 0.0707. gp: 0.4266 \n","[2800/3000] D(real): 1.16. D(fake): 0.64. rec_loss: 0.0703. gp: 0.1287 \n","[2900/3000] D(real): 1.16. D(fake): 0.56. rec_loss: 0.0703. gp: 0.0953 \n","Total time in scale 7: 1527[sec] (0.51[sec]/epoch on avg.). D(real): 1.158827, D(fake): 0.562816, rec_loss: 0.0703. gp: 0.0953\n","****************************** Finished working on scale 7 ******************************\n","Signal in scale 6 has 58125 samples, sample rate is 2500[Hz].\n","Total receptive field is 816[msec] (3.5% of input).\n","[0/3000] D(real): 0.44. D(fake): 0.43. rec_loss: 0.1231. gp: 0.1817 \n","[100/3000] D(real): 0.71. D(fake): 0.60. rec_loss: 0.0991. gp: 0.0821 \n","[200/3000] D(real): 0.97. D(fake): 0.78. rec_loss: 0.0956. gp: 0.0918 \n","[300/3000] D(real): 0.87. D(fake): 0.77. rec_loss: 0.0938. gp: 0.0912 \n","[400/3000] D(real): 0.71. D(fake): 0.60. rec_loss: 0.0942. gp: 0.0298 \n","[500/3000] D(real): 0.96. D(fake): 0.77. rec_loss: 0.0917. gp: 0.0513 \n","[600/3000] D(real): 0.90. D(fake): 0.72. rec_loss: 0.0907. gp: 0.0839 \n","[700/3000] D(real): 1.24. D(fake): 1.00. rec_loss: 0.0893. gp: 0.0859 \n","[800/3000] D(real): 1.31. D(fake): 1.00. rec_loss: 0.0891. gp: 0.1246 \n","[900/3000] D(real): 1.21. D(fake): 1.11. rec_loss: 0.0879. gp: 0.0271 \n","[1000/3000] D(real): 1.30. D(fake): 1.14. rec_loss: 0.0886. gp: 0.0692 \n","[1100/3000] D(real): 1.53. D(fake): 1.24. rec_loss: 0.0872. gp: 0.1443 \n","[1200/3000] D(real): 1.53. D(fake): 1.35. rec_loss: 0.0866. gp: 0.1634 \n","[1300/3000] D(real): 1.25. D(fake): 1.16. rec_loss: 0.0860. gp: 0.0244 \n","[1400/3000] D(real): 1.41. D(fake): 1.24. rec_loss: 0.0843. gp: 0.0970 \n","[1500/3000] D(real): 1.64. D(fake): 1.32. rec_loss: 0.0858. gp: 0.0618 \n","[1600/3000] D(real): 1.43. D(fake): 1.31. rec_loss: 0.1096. gp: 0.0195 \n","[1700/3000] D(real): 1.72. D(fake): 1.33. rec_loss: 0.0838. gp: 0.1352 \n","[1800/3000] D(real): 1.43. D(fake): 1.32. rec_loss: 0.0831. gp: 0.0347 \n","[1900/3000] D(real): 1.75. D(fake): 1.53. rec_loss: 0.0814. gp: 0.1056 \n","[2000/3000] D(real): 1.53. D(fake): 1.25. rec_loss: 0.0817. gp: 0.0916 \n","[2100/3000] D(real): 1.75. D(fake): 1.39. rec_loss: 0.0802. gp: 0.1087 \n","[2200/3000] D(real): 1.66. D(fake): 1.28. rec_loss: 0.0801. gp: 0.1387 \n","[2300/3000] D(real): 1.74. D(fake): 1.30. rec_loss: 0.0798. gp: 0.0963 \n","[2400/3000] D(real): 1.74. D(fake): 1.24. rec_loss: 0.0805. gp: 0.4980 \n","[2500/3000] D(real): 1.78. D(fake): 1.37. rec_loss: 0.0800. gp: 0.3670 \n","[2600/3000] D(real): 1.85. D(fake): 1.36. rec_loss: 0.0800. gp: 0.1764 \n","[2700/3000] D(real): 1.90. D(fake): 1.39. rec_loss: 0.0811. gp: 0.0890 \n","[2800/3000] D(real): 1.82. D(fake): 1.36. rec_loss: 0.0799. gp: 0.1304 \n","[2900/3000] D(real): 1.68. D(fake): 1.36. rec_loss: 0.0801. gp: 0.0795 \n","Total time in scale 6: 1886[sec] (0.63[sec]/epoch on avg.). D(real): 1.681636, D(fake): 1.359770, rec_loss: 0.0801. gp: 0.0795\n","****************************** Finished working on scale 6 ******************************\n","Signal in scale 5 has 93000 samples, sample rate is 4000[Hz].\n","Total receptive field is 510[msec] (2.2% of input).\n","[0/3000] D(real): 1.22. D(fake): 1.31. rec_loss: 0.1736. gp: 0.2524 \n","[100/3000] D(real): 1.31. D(fake): 1.18. rec_loss: 0.1375. gp: 0.1700 \n","[200/3000] D(real): 1.33. D(fake): 1.23. rec_loss: 0.1295. gp: 0.0509 \n","[300/3000] D(real): 1.36. D(fake): 1.27. rec_loss: 0.1277. gp: 0.0272 \n","[400/3000] D(real): 1.29. D(fake): 1.18. rec_loss: 0.1246. gp: 0.0420 \n","[500/3000] D(real): 1.37. D(fake): 1.28. rec_loss: 0.1233. gp: 0.0294 \n","[600/3000] D(real): 1.39. D(fake): 1.25. rec_loss: 0.1219. gp: 0.0627 \n","[700/3000] D(real): 1.25. D(fake): 1.04. rec_loss: 0.1212. gp: 0.1062 \n","[800/3000] D(real): 1.14. D(fake): 1.07. rec_loss: 0.1172. gp: 0.0184 \n","[900/3000] D(real): 1.23. D(fake): 1.16. rec_loss: 0.1148. gp: 0.0158 \n","[1000/3000] D(real): 1.33. D(fake): 1.17. rec_loss: 0.1138. gp: 0.0892 \n","[1100/3000] D(real): 1.24. D(fake): 1.11. rec_loss: 0.1118. gp: 0.0491 \n","[1200/3000] D(real): 1.35. D(fake): 1.12. rec_loss: 0.1102. gp: 0.1432 \n","[1300/3000] D(real): 1.33. D(fake): 1.20. rec_loss: 0.1080. gp: 0.0734 \n","[1400/3000] D(real): 1.14. D(fake): 1.06. rec_loss: 0.1062. gp: 0.0316 \n","[1500/3000] D(real): 1.22. D(fake): 1.08. rec_loss: 0.1068. gp: 0.0356 \n","[1600/3000] D(real): 1.11. D(fake): 0.98. rec_loss: 0.1052. gp: 0.0947 \n","[1700/3000] D(real): 1.11. D(fake): 0.89. rec_loss: 0.1031. gp: 0.0709 \n","[1800/3000] D(real): 1.21. D(fake): 1.02. rec_loss: 0.1021. gp: 0.0357 \n","[1900/3000] D(real): 1.04. D(fake): 0.95. rec_loss: 0.0987. gp: 0.0229 \n","[2000/3000] D(real): 1.22. D(fake): 1.10. rec_loss: 0.0984. gp: 0.0467 \n","[2100/3000] D(real): 1.23. D(fake): 1.00. rec_loss: 0.0964. gp: 0.1128 \n","[2200/3000] D(real): 1.23. D(fake): 0.96. rec_loss: 0.0960. gp: 0.0960 \n","[2300/3000] D(real): 1.20. D(fake): 0.94. rec_loss: 0.0959. gp: 0.1870 \n","[2400/3000] D(real): 1.14. D(fake): 0.85. rec_loss: 0.0958. gp: 0.0563 \n","[2500/3000] D(real): 1.07. D(fake): 0.83. rec_loss: 0.0959. gp: 0.1598 \n","[2600/3000] D(real): 1.17. D(fake): 0.84. rec_loss: 0.0968. gp: 0.0823 \n","[2700/3000] D(real): 1.13. D(fake): 0.82. rec_loss: 0.0959. gp: 0.0460 \n","[2800/3000] D(real): 1.20. D(fake): 0.88. rec_loss: 0.0960. gp: 0.0589 \n","[2900/3000] D(real): 1.17. D(fake): 0.84. rec_loss: 0.0952. gp: 0.0612 \n","Total time in scale 5: 2742[sec] (0.91[sec]/epoch on avg.). D(real): 1.171519, D(fake): 0.841880, rec_loss: 0.0952. gp: 0.0612\n","****************************** Finished working on scale 5 ******************************\n","Signal in scale 4 has 186000 samples, sample rate is 8000[Hz].\n","Total receptive field is 255[msec] (1.1% of input).\n","[0/3000] D(real): 0.76. D(fake): 0.77. rec_loss: 0.2505. gp: 0.2015 \n","[100/3000] D(real): 1.03. D(fake): 1.01. rec_loss: 0.1871. gp: 0.0165 \n","[200/3000] D(real): 1.00. D(fake): 0.95. rec_loss: 0.1755. gp: 0.0181 \n","[300/3000] D(real): 0.82. D(fake): 0.76. rec_loss: 0.1664. gp: 0.0208 \n","[400/3000] D(real): 0.70. D(fake): 0.68. rec_loss: 0.1607. gp: 0.0257 \n","[500/3000] D(real): 0.45. D(fake): 0.42. rec_loss: 0.1574. gp: 0.0124 \n","[600/3000] D(real): 0.48. D(fake): 0.38. rec_loss: 0.1525. gp: 0.0418 \n","[700/3000] D(real): 0.36. D(fake): 0.28. rec_loss: 0.1486. gp: 0.0247 \n","[800/3000] D(real): 0.36. D(fake): 0.19. rec_loss: 0.1433. gp: 0.0596 \n","[900/3000] D(real): 0.16. D(fake): 0.07. rec_loss: 0.1384. gp: 0.0218 \n","[1000/3000] D(real): 0.12. D(fake): 0.05. rec_loss: 0.1360. gp: 0.0327 \n","[1100/3000] D(real): 0.14. D(fake): 0.01. rec_loss: 0.1330. gp: 0.0664 \n","[1200/3000] D(real): 0.13. D(fake): -0.01. rec_loss: 0.1284. gp: 0.0287 \n","[1300/3000] D(real): 0.00. D(fake): -0.06. rec_loss: 0.1262. gp: 0.0095 \n","[1400/3000] D(real): 0.13. D(fake): 0.11. rec_loss: 0.1264. gp: 0.0111 \n","[1500/3000] D(real): 0.13. D(fake): 0.03. rec_loss: 0.1180. gp: 0.0254 \n","[1600/3000] D(real): -0.21. D(fake): -0.22. rec_loss: 0.1146. gp: 0.0090 \n","[1700/3000] D(real): -0.08. D(fake): -0.09. rec_loss: 0.1112. gp: 0.0056 \n","[1800/3000] D(real): -0.06. D(fake): -0.15. rec_loss: 0.1171. gp: 0.0359 \n","[1900/3000] D(real): -0.08. D(fake): -0.11. rec_loss: 0.1068. gp: 0.0128 \n","[2000/3000] D(real): -0.23. D(fake): -0.29. rec_loss: 0.1051. gp: 0.0120 \n","[2100/3000] D(real): -0.10. D(fake): -0.24. rec_loss: 0.1028. gp: 0.0478 \n","[2200/3000] D(real): -0.19. D(fake): -0.33. rec_loss: 0.1032. gp: 0.0200 \n","[2300/3000] D(real): -0.16. D(fake): -0.34. rec_loss: 0.1030. gp: 0.0208 \n","[2400/3000] D(real): -0.15. D(fake): -0.36. rec_loss: 0.1032. gp: 0.1544 \n","[2500/3000] D(real): -0.13. D(fake): -0.34. rec_loss: 0.1033. gp: 0.0250 \n","[2600/3000] D(real): -0.10. D(fake): -0.40. rec_loss: 0.1068. gp: 0.0308 \n","[2700/3000] D(real): -0.11. D(fake): -0.35. rec_loss: 0.1042. gp: 0.0368 \n","[2800/3000] D(real): -0.12. D(fake): -0.35. rec_loss: 0.1037. gp: 0.0396 \n","[2900/3000] D(real): -0.23. D(fake): -0.37. rec_loss: 0.1053. gp: 0.0171 \n","Total time in scale 4: 5180[sec] (1.73[sec]/epoch on avg.). D(real): -0.226049, D(fake): -0.373935, rec_loss: 0.1053. gp: 0.0171\n","****************************** Finished working on scale 4 ******************************\n","Signal in scale 3 has 232500 samples, sample rate is 10000[Hz].\n","Total receptive field is 204[msec] (0.9% of input).\n","[0/3000] D(real): -0.38. D(fake): -0.42. rec_loss: 0.2686. gp: 0.0860 \n","[100/3000] D(real): -0.14. D(fake): -0.22. rec_loss: 0.1869. gp: 0.0261 \n","[200/3000] D(real): -0.36. D(fake): -0.38. rec_loss: 0.1730. gp: 0.0120 \n","[300/3000] D(real): -0.32. D(fake): -0.42. rec_loss: 0.1630. gp: 0.0213 \n","[400/3000] D(real): -0.32. D(fake): -0.42. rec_loss: 0.1571. gp: 0.0279 \n","[500/3000] D(real): -0.41. D(fake): -0.47. rec_loss: 0.1473. gp: 0.0239 \n","[600/3000] D(real): -0.41. D(fake): -0.53. rec_loss: 0.1437. gp: 0.0513 \n","[700/3000] D(real): -0.51. D(fake): -0.62. rec_loss: 0.1585. gp: 0.0283 \n","[800/3000] D(real): -0.54. D(fake): -0.60. rec_loss: 0.1340. gp: 0.0169 \n","[900/3000] D(real): -0.46. D(fake): -0.54. rec_loss: 0.1276. gp: 0.0405 \n","[1000/3000] D(real): -0.68. D(fake): -0.73. rec_loss: 0.1259. gp: 0.0096 \n","[1100/3000] D(real): -0.52. D(fake): -0.72. rec_loss: 0.1234. gp: 0.0511 \n","[1200/3000] D(real): -0.73. D(fake): -0.78. rec_loss: 0.1185. gp: 0.0239 \n","[1300/3000] D(real): -0.70. D(fake): -0.80. rec_loss: 0.1172. gp: 0.0201 \n","[1400/3000] D(real): -0.76. D(fake): -0.86. rec_loss: 0.1204. gp: 0.0641 \n","[1500/3000] D(real): -0.66. D(fake): -0.79. rec_loss: 0.1106. gp: 0.0259 \n","[1600/3000] D(real): -0.74. D(fake): -0.81. rec_loss: 0.1095. gp: 0.0267 \n","[1700/3000] D(real): -0.64. D(fake): -0.80. rec_loss: 0.1103. gp: 0.0325 \n","[1800/3000] D(real): -0.61. D(fake): -0.78. rec_loss: 0.1105. gp: 0.0559 \n","[1900/3000] D(real): -0.69. D(fake): -0.80. rec_loss: 0.1119. gp: 0.0407 \n","[2000/3000] D(real): -0.71. D(fake): -0.78. rec_loss: 0.1016. gp: 0.0279 \n","[2100/3000] D(real): -0.60. D(fake): -0.74. rec_loss: 0.0988. gp: 0.0483 \n","[2200/3000] D(real): -0.62. D(fake): -0.79. rec_loss: 0.0984. gp: 0.0334 \n","[2300/3000] D(real): -0.60. D(fake): -0.83. rec_loss: 0.0985. gp: 0.0416 \n","[2400/3000] D(real): -0.62. D(fake): -0.85. rec_loss: 0.0982. gp: 0.0399 \n","[2500/3000] D(real): -0.74. D(fake): -0.89. rec_loss: 0.0991. gp: 0.0210 \n","[2600/3000] D(real): -0.60. D(fake): -0.81. rec_loss: 0.0982. gp: 0.0284 \n","[2700/3000] D(real): -0.67. D(fake): -0.87. rec_loss: 0.0994. gp: 0.2013 \n","[2800/3000] D(real): -0.65. D(fake): -0.84. rec_loss: 0.0981. gp: 0.0404 \n","[2900/3000] D(real): -0.62. D(fake): -0.90. rec_loss: 0.0994. gp: 0.1382 \n","Total time in scale 3: 6702[sec] (2.23[sec]/epoch on avg.). D(real): -0.615553, D(fake): -0.898265, rec_loss: 0.0994. gp: 0.1382\n","****************************** Finished working on scale 3 ******************************\n","Signal in scale 2 has 279000 samples, sample rate is 12000[Hz].\n","Total receptive field is 170[msec] (0.7% of input).\n","[0/3000] D(real): -0.90. D(fake): -0.92. rec_loss: 0.2602. gp: 0.0269 \n","[100/3000] D(real): -0.63. D(fake): -0.75. rec_loss: 0.1982. gp: 0.0707 \n","[200/3000] D(real): -0.59. D(fake): -0.70. rec_loss: 0.1802. gp: 0.0257 \n","[300/3000] D(real): -0.78. D(fake): -0.82. rec_loss: 0.1695. gp: 0.0105 \n","[400/3000] D(real): -0.67. D(fake): -0.75. rec_loss: 0.1588. gp: 0.0319 \n","[500/3000] D(real): -0.58. D(fake): -0.73. rec_loss: 0.1520. gp: 0.0734 \n","[600/3000] D(real): -0.71. D(fake): -0.73. rec_loss: 0.1447. gp: 0.0104 \n","[700/3000] D(real): -0.68. D(fake): -0.89. rec_loss: 0.1406. gp: 0.0315 \n","[800/3000] D(real): -0.54. D(fake): -0.66. rec_loss: 0.1350. gp: 0.0340 \n","[900/3000] D(real): -0.55. D(fake): -0.61. rec_loss: 0.1302. gp: 0.0258 \n","[1000/3000] D(real): -0.55. D(fake): -0.65. rec_loss: 0.1285. gp: 0.0330 \n","[1100/3000] D(real): -0.59. D(fake): -0.70. rec_loss: 0.1221. gp: 0.0312 \n","[1200/3000] D(real): -0.61. D(fake): -0.69. rec_loss: 0.1209. gp: 0.0227 \n","[1300/3000] D(real): -0.71. D(fake): -0.76. rec_loss: 0.1179. gp: 0.0174 \n","[1400/3000] D(real): -0.65. D(fake): -0.79. rec_loss: 0.1166. gp: 0.1326 \n","[1500/3000] D(real): -0.64. D(fake): -0.79. rec_loss: 0.1185. gp: 0.0857 \n","[1600/3000] D(real): -0.88. D(fake): -0.90. rec_loss: 0.1160. gp: 0.0125 \n","[1700/3000] D(real): -0.53. D(fake): -0.70. rec_loss: 0.1125. gp: 0.0297 \n","[1800/3000] D(real): -0.58. D(fake): -0.74. rec_loss: 0.1195. gp: 0.0392 \n","[1900/3000] D(real): -0.60. D(fake): -0.69. rec_loss: 0.1060. gp: 0.0354 \n","[2000/3000] D(real): -0.70. D(fake): -0.82. rec_loss: 0.1059. gp: 0.0289 \n","[2100/3000] D(real): -0.62. D(fake): -0.85. rec_loss: 0.1008. gp: 0.0746 \n","[2200/3000] D(real): -0.61. D(fake): -0.86. rec_loss: 0.1006. gp: 0.2617 \n","[2300/3000] D(real): -0.60. D(fake): -0.85. rec_loss: 0.1003. gp: 0.3148 \n","[2400/3000] D(real): -0.71. D(fake): -0.93. rec_loss: 0.1004. gp: 0.0633 \n","[2500/3000] D(real): -0.63. D(fake): -0.91. rec_loss: 0.1014. gp: 0.0281 \n","[2600/3000] D(real): -0.80. D(fake): -0.96. rec_loss: 0.1014. gp: 0.0308 \n","[2700/3000] D(real): -0.63. D(fake): -0.94. rec_loss: 0.1012. gp: 0.0327 \n","[2800/3000] D(real): -0.74. D(fake): -0.95. rec_loss: 0.1005. gp: 0.0431 \n","[2900/3000] D(real): -0.76. D(fake): -0.97. rec_loss: 0.1004. gp: 0.0253 \n","Total time in scale 2: 8011[sec] (2.67[sec]/epoch on avg.). D(real): -0.762470, D(fake): -0.968068, rec_loss: 0.1004. gp: 0.0253\n","****************************** Finished working on scale 2 ******************************\n","Signal in scale 1 has 334800 samples, sample rate is 14400[Hz].\n","Total receptive field is 141[msec] (0.6% of input).\n","[0/3000] D(real): -0.97. D(fake): -0.95. rec_loss: 0.3051. gp: 0.1271 \n","[100/3000] D(real): -0.74. D(fake): -0.81. rec_loss: 0.2079. gp: 0.0155 \n","[200/3000] D(real): -0.65. D(fake): -0.72. rec_loss: 0.1915. gp: 0.0272 \n","[300/3000] D(real): -0.62. D(fake): -0.73. rec_loss: 0.1851. gp: 0.0348 \n","[400/3000] D(real): -0.60. D(fake): -0.75. rec_loss: 0.1703. gp: 0.0456 \n","[500/3000] D(real): -0.70. D(fake): -0.79. rec_loss: 0.1575. gp: 0.0279 \n","[600/3000] D(real): -0.60. D(fake): -0.74. rec_loss: 0.1519. gp: 0.0496 \n","[700/3000] D(real): -0.66. D(fake): -0.79. rec_loss: 0.1458. gp: 0.1004 \n","[800/3000] D(real): -0.69. D(fake): -0.83. rec_loss: 0.1407. gp: 0.0339 \n","[900/3000] D(real): -0.87. D(fake): -0.91. rec_loss: 0.1369. gp: 0.0099 \n","[1000/3000] D(real): -0.69. D(fake): -0.79. rec_loss: 0.1339. gp: 0.0237 \n","[1100/3000] D(real): -0.89. D(fake): -0.98. rec_loss: 0.1325. gp: 0.0203 \n","[1200/3000] D(real): -0.93. D(fake): -1.03. rec_loss: 0.1277. gp: 0.0160 \n","[1300/3000] D(real): -0.78. D(fake): -0.90. rec_loss: 0.1255. gp: 0.0307 \n","[1400/3000] D(real): -0.87. D(fake): -1.03. rec_loss: 0.1261. gp: 0.0184 \n","[1500/3000] D(real): -0.73. D(fake): -0.92. rec_loss: 0.1235. gp: 0.0762 \n","[1600/3000] D(real): -0.82. D(fake): -0.94. rec_loss: 0.1185. gp: 0.0196 \n","[1700/3000] D(real): -1.10. D(fake): -1.15. rec_loss: 0.1163. gp: 0.0196 \n","[1800/3000] D(real): -0.90. D(fake): -1.04. rec_loss: 0.1159. gp: 0.0313 \n","[1900/3000] D(real): -0.74. D(fake): -0.97. rec_loss: 0.1168. gp: 0.0966 \n","[2000/3000] D(real): -0.82. D(fake): -0.96. rec_loss: 0.1101. gp: 0.0453 \n","[2100/3000] D(real): -0.78. D(fake): -0.99. rec_loss: 0.1071. gp: 0.0348 \n","[2200/3000] D(real): -0.81. D(fake): -1.02. rec_loss: 0.1068. gp: 0.2040 \n","[2300/3000] D(real): -0.82. D(fake): -1.04. rec_loss: 0.1065. gp: 0.0183 \n","[2400/3000] D(real): -0.82. D(fake): -1.08. rec_loss: 0.1063. gp: 0.0248 \n","[2500/3000] D(real): -0.89. D(fake): -1.12. rec_loss: 0.1062. gp: 0.1100 \n","[2600/3000] D(real): -0.82. D(fake): -1.09. rec_loss: 0.1060. gp: 0.0219 \n","[2700/3000] D(real): -0.86. D(fake): -1.16. rec_loss: 0.1059. gp: 0.0291 \n","[2800/3000] D(real): -0.96. D(fake): -1.18. rec_loss: 0.1066. gp: 0.1338 \n","[2900/3000] D(real): -0.96. D(fake): -1.19. rec_loss: 0.1062. gp: 0.0255 \n","Total time in scale 1: 9694[sec] (3.23[sec]/epoch on avg.). D(real): -0.964661, D(fake): -1.191092, rec_loss: 0.1062. gp: 0.0255\n","****************************** Finished working on scale 1 ******************************\n","Signal in scale 0 has 372000 samples, sample rate is 16000[Hz].\n","Total receptive field is 127[msec] (0.5% of input).\n","[0/3000] D(real): -1.18. D(fake): -1.26. rec_loss: 0.3076. gp: 0.1223 \n","[100/3000] D(real): -1.04. D(fake): -1.14. rec_loss: 0.2118. gp: 0.0208 \n","[200/3000] D(real): -1.22. D(fake): -1.27. rec_loss: 0.1897. gp: 0.0151 \n","[300/3000] D(real): -1.08. D(fake): -1.15. rec_loss: 0.1750. gp: 0.0235 \n","[400/3000] D(real): -0.87. D(fake): -1.06. rec_loss: 0.1660. gp: 0.0563 \n","[500/3000] D(real): -0.93. D(fake): -1.16. rec_loss: 0.1646. gp: 0.0242 \n","[600/3000] D(real): -0.93. D(fake): -1.10. rec_loss: 0.1541. gp: 0.0357 \n","[700/3000] D(real): -0.99. D(fake): -1.05. rec_loss: 0.1469. gp: 0.0157 \n","[800/3000] D(real): -0.86. D(fake): -0.99. rec_loss: 0.1434. gp: 0.0879 \n","[900/3000] D(real): -1.00. D(fake): -1.13. rec_loss: 0.1381. gp: 0.0663 \n","[1000/3000] D(real): -1.10. D(fake): -1.17. rec_loss: 0.1343. gp: 0.0240 \n","[1100/3000] D(real): -1.04. D(fake): -1.15. rec_loss: 0.1336. gp: 0.0352 \n","[1200/3000] D(real): -0.99. D(fake): -1.19. rec_loss: 0.1310. gp: 0.0838 \n","[1300/3000] D(real): -1.27. D(fake): -1.38. rec_loss: 0.1268. gp: 0.0330 \n","[1400/3000] D(real): -1.18. D(fake): -1.25. rec_loss: 0.1248. gp: 0.0211 \n","[1500/3000] D(real): -1.18. D(fake): -1.25. rec_loss: 0.1239. gp: 0.0214 \n","[1600/3000] D(real): -0.95. D(fake): -1.17. rec_loss: 0.1223. gp: 0.1718 \n","[1700/3000] D(real): -1.03. D(fake): -1.16. rec_loss: 0.1211. gp: 0.0334 \n","[1800/3000] D(real): -1.22. D(fake): -1.32. rec_loss: 0.1179. gp: 0.0192 \n","[1900/3000] D(real): -0.97. D(fake): -1.11. rec_loss: 0.1140. gp: 0.0452 \n","[2000/3000] D(real): -1.10. D(fake): -1.22. rec_loss: 0.1146. gp: 0.0200 \n","[2100/3000] D(real): -1.01. D(fake): -1.28. rec_loss: 0.1093. gp: 0.0985 \n","[2200/3000] D(real): -1.08. D(fake): -1.33. rec_loss: 0.1088. gp: 0.0303 \n","[2300/3000] D(real): -1.08. D(fake): -1.31. rec_loss: 0.1087. gp: 0.3128 \n","[2400/3000] D(real): -1.09. D(fake): -1.36. rec_loss: 0.1090. gp: 0.0163 \n","[2500/3000] D(real): -1.13. D(fake): -1.36. rec_loss: 0.1100. gp: 0.0204 \n","[2600/3000] D(real): -1.10. D(fake): -1.43. rec_loss: 0.1088. gp: 0.0598 \n","[2700/3000] D(real): -1.13. D(fake): -1.45. rec_loss: 0.1087. gp: 0.0305 \n","[2800/3000] D(real): -1.11. D(fake): -1.43. rec_loss: 0.1102. gp: 0.0325 \n","[2900/3000] D(real): -1.15. D(fake): -1.42. rec_loss: 0.1080. gp: 0.0195 \n","Total time in scale 0: 10955[sec] (3.65[sec]/epoch on avg.). D(real): -1.149214, D(fake): -1.419797, rec_loss: 0.1080. gp: 0.0195\n","****************************** Finished working on scale 0 ******************************\n"]}],"source":["#training\n","startTime = time.time()\n","\n","if len(inpainting_indices)%2 != 0:\n","    raise Exception('Provide START and END indices of each hole!')\n","\n","if is_cuda:\n","    torch.cuda.set_device(gpu_num)\n","    device = torch.device(\"cuda:%d\" % gpu_num)\n","\n","if manual_random_seed != -1:\n","    random.seed(manual_random_seed)\n","    torch.manual_seed(manual_random_seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","samples, Fs = get_input_signal(input_file, max_length)\n","\n","fs_list = [f for f in fs_list if f <= Fs]\n","if fs_list[-1] != Fs:\n","    fs_list.append(Fs)\n","\n","scales = [Fs / f for f in fs_list]\n","\n","print('Working on file: %s' % input_file)\n","\n","scheduler_milestones = [int(num_epochs * 2 / 3)]\n","\n","alpha1 = 0\n","alpha2 = 1e-4\n","add_cond_noise = True\n","\n","dilation_factors = [2 ** i for i in range(num_layers)]\n","\n","if not os.path.exists(output_folder):\n","    os.mkdir(output_folder)\n","\n","if os.path.exists(output_folder):\n","    dirs = glob.glob(output_folder + '*')\n","    output_folder = output_folder + '_' + str(len(dirs) + 1)\n","\n","os.mkdir(output_folder)\n","print('Writing results to %s\\n' % output_folder)\n","\n","signals_list, fs_list = create_input_signals(scales, set_first_scale_by_energy, min_energy_th,  filter_size, torch.tensor(samples), Fs)\n","if len(signals_list) == 0:\n","    set_first_scale_by_energy = False\n","    scales = scales[2:]  # Manually start from 500\n","    signals_list, fs_list = create_input_signals(scales, set_first_scale_by_energy, min_energy_th,  filter_size, torch.tensor(samples), Fs)\n","scales = [Fs / f for f in fs_list]\n","\n","fs_list = fs_list\n","inputs_lengths = [len(s) for s in signals_list]\n","\n","print('Running on ' + str(device))\n","\n","output_signals, loss_vectors, generators_list, noise_amp_list, energy_list, reconstruction_noise_list = train(\n","                          manual_random_seed, fs_list, scales, growing_hidden_channels_factor,learning_rate, beta1, scheduler_lr_decay,\n","                          plot_losses, initial_noise_amp, noise_amp_factor, signals_list, dilation_factors, output_folder, inputs_lengths)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"bBY1GNTZpbgP","executionInfo":{"status":"ok","timestamp":1664231685473,"user_tz":-60,"elapsed":8,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}}},"outputs":[],"source":["#!zip -r outputs.zip outputs_2"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"pfWK4VohjG0Z","executionInfo":{"status":"ok","timestamp":1664231685474,"user_tz":-60,"elapsed":8,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}}},"outputs":[],"source":["class AudioGenerator(object):\n","    def __init__(self, output_folder, fs_list, dilation_factors, filter_size, device, generators_list=None, noise_amp_list=None, reconstruction_noise_list=None):\n","        super(AudioGenerator, self).__init__()\n","        self.generators_list = generators_list\n","        self.noise_amp_list = noise_amp_list\n","        self.reconstruction_noise_list = reconstruction_noise_list\n","        self.output_folder = output_folder\n","        self.fs_list= fs_list\n","        self.device = device\n","        self.dilation_factors = dilation_factors\n","        self.filter_size = filter_size\n","        if not os.path.exists(os.path.join(output_folder, 'GeneratedSignals')):\n","            os.mkdir(os.path.join(output_folder, 'GeneratedSignals'))\n","\n","    def generate(self, nSignals=1, length=20, generate_all_scales=False):\n","        for sig_idx in range(nSignals):\n","            # Draws a signal up to current scale, using learned generators\n","            output_signals_list = draw_signal(self.generators_list,\n","                                              [round(f * length) for f in self.fs_list], self.fs_list,\n","                                              self.noise_amp_list,  self.filter_size, self.dilation_factors, self.device, \n","                                              output_all_scales=generate_all_scales)\n","            # Write signals\n","            if generate_all_scales:\n","                for scale_idx, sig in enumerate(output_signals_list):\n","                    write_signal(\n","                        os.path.join(self.output_folder, 'GeneratedSignals',\n","                                     'generated@%dHz.wav' % self.fs_list[scale_idx]),\n","                        sig, self.fs_list[scale_idx], overwrite=False)\n","            else:\n","                write_signal(\n","                    os.path.join(self.output_folder, 'GeneratedSignals',\n","                                 'generated@%dHz.wav' % self.fs_list[-1]),\n","                    output_signals_list, self.fs_list[-1], overwrite=False)\n","\n","    def condition(self, condition, write=True):\n","        condition[\"condition_scale_idx\"] = np.where(np.array(self.fs_list) <= condition[\"condition_fs\"])[0][\n","                                               -1] + 1\n","        condition[\"condition_signal\"] = torch.Tensor(condition[\"condition_signal\"]).expand(1, 1, -1).to(\n","            self.device)\n","        lengths = [int(condition[\"condition_signal\"].shape[2] / condition[\"condition_fs\"] * fs) for fs in\n","                   self.fs_list]\n","        conditioned_signal = draw_signal(self.generators_list, lengths, self.fs_list, self.noise_amp_list, \n","                                         self.filter_size, self.dilation_factors, self.device,\n","                                         condition=condition)\n","        if write:\n","            output_file = os.path.join(self.output_folder, 'GeneratedSignals',\n","                                       'conditioned_on_' + condition['name'])\n","            write_signal(output_file, conditioned_signal, self.params.Fs)\n","        else:\n","            return conditioned_signal"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"2XgcJ9x-zq06","executionInfo":{"status":"ok","timestamp":1664232041646,"user_tz":-60,"elapsed":1120,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}}},"outputs":[],"source":["nSignals=1\n","length=25\n","generate_all_scales=False\n","\n","audio_generator = AudioGenerator(output_folder, fs_list, dilation_factors, filter_size, device, generators_list, noise_amp_list,\n","                                 reconstruction_noise_list=reconstruction_noise_list)\n","\n","audio_generator.generate(nSignals=nSignals, length=length,generate_all_scales=generate_all_scales)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_HyF3o0kkHQ"},"outputs":[],"source":["path = \"/content/outputs_2/GeneratedSignals\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wLMleJ-vk2oB"},"outputs":[],"source":["paths = []\n","size = 0\n","for root, dirs, files in os.walk(path):\n","    for file in files:\n","        if (file.endswith(\".wav\") and  (not (file.startswith(\".\") or file.startswith(\"noise\")))):\n","             paths.append(os.path.join(root, file))\n","             size += os.path.getsize(os.path.join(root, file))\n","             \n","\n","\n","print(f'We have {len(paths)} .Wav Files with {size/1024**2:.2f} Mb in size')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWO_wzVto6df"},"outputs":[],"source":["!!zip -r /content/outputs_2/GeneratedSignals.zip /content/outputs_2/GeneratedSignals"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"T112qY4jpfmr","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1664231685476,"user_tz":-60,"elapsed":10,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}},"outputId":"3af79719-4313-4284-a7dd-b96ede7189c4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_eb2f74d7-6351-4015-9a2b-43c5915e5f89\", \"GeneratedSignals.zip\", 781758)"]},"metadata":{}}],"source":["from google.colab import files\n","files.download('/content/outputs_2/GeneratedSignals.zip')"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"7aCyKbHdaDnH","executionInfo":{"status":"ok","timestamp":1664231686984,"user_tz":-60,"elapsed":1515,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}}},"outputs":[],"source":["!cp /content/outputs_2/GeneratedSignals.zip  /content/drive/MyDrive/FinalProject/CAW_outputs"]},{"cell_type":"markdown","metadata":{"id":"-Du88qF5aFIw"},"source":["# Remember to change this path everything you do generation ok"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"ruT6IPL-aKZn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664231692321,"user_tz":-60,"elapsed":26,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}},"outputId":"da2a070f-e568-41b1-f6c6-b65ed327afa5"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["We have 2 .Wav Files with 1.53 Mb in size\n","updating: content/outputs_2/GeneratedSignals/ (stored 0%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz.wav (deflated 2%)\n","  adding: content/outputs_2/GeneratedSignals/generated@16000Hz_1.wav (deflated 3%)\n","We have 3 .Wav Files with 2.29 Mb in size\n","updating: content/outputs_2/GeneratedSignals/ (stored 0%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz.wav (deflated 2%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_1.wav (deflated 3%)\n","  adding: content/outputs_2/GeneratedSignals/generated@16000Hz_2.wav (deflated 2%)\n","We have 4 .Wav Files with 3.05 Mb in size\n","updating: content/outputs_2/GeneratedSignals/ (stored 0%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz.wav (deflated 2%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_1.wav (deflated 3%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_2.wav (deflated 2%)\n","  adding: content/outputs_2/GeneratedSignals/generated@16000Hz_3.wav (deflated 2%)\n","We have 5 .Wav Files with 3.81 Mb in size\n","updating: content/outputs_2/GeneratedSignals/ (stored 0%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz.wav (deflated 2%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_1.wav (deflated 3%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_2.wav (deflated 2%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_3.wav (deflated 2%)\n","  adding: content/outputs_2/GeneratedSignals/generated@16000Hz_4.wav (deflated 2%)\n","We have 6 .Wav Files with 4.58 Mb in size\n","updating: content/outputs_2/GeneratedSignals/ (stored 0%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz.wav (deflated 2%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_1.wav (deflated 3%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_2.wav (deflated 2%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_3.wav (deflated 2%)\n","updating: content/outputs_2/GeneratedSignals/generated@16000Hz_4.wav (deflated 2%)\n","  adding: content/outputs_2/GeneratedSignals/generated@16000Hz_5.wav (deflated 3%)\n","Don't forget to change n according to the number of samples you want\n"]}],"source":["n = 5\n","\n","\n","for i in range (n):\n","  nSignals=1\n","  length=25\n","  generate_all_scales=False\n","\n","  audio_generator = AudioGenerator(output_folder, fs_list, dilation_factors, filter_size, device, generators_list, noise_amp_list,\n","                                 reconstruction_noise_list=reconstruction_noise_list)\n","\n","  audio_generator.generate(nSignals=nSignals, length=length,generate_all_scales=generate_all_scales)\n","\n","  paths = []\n","  size = 0\n","  for root, dirs, files in os.walk(path):\n","    for file in files:\n","        if (file.endswith(\".wav\") and  (not (file.startswith(\".\") or file.startswith(\"noise\")))):\n","             paths.append(os.path.join(root, file))\n","             size += os.path.getsize(os.path.join(root, file))\n","             \n","\n","\n","  print(f'We have {len(paths)} .Wav Files with {size/1024**2:.2f} Mb in size')\n","  !zip -r /content/outputs_2/GeneratedSignals.zip /content/outputs_2/GeneratedSignals\n","  !cp /content/outputs_2/GeneratedSignals.zip  /content/drive/MyDrive/FinalProject/CAW_outputs\n","\n","print(\"Don't forget to change n according to the number of samples you want\")\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[],"background_execution":"on"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}