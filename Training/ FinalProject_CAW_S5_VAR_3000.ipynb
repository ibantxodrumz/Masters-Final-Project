{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":122324,"status":"ok","timestamp":1663863921220,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"},"user_tz":-60},"id":"dg-Zaz71X5ao","outputId":"4c86b561-2ab7-4720-fc2b-497b2758a8a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.9.0\n","  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[K     |████████████████████████████████| 831.4 MB 2.2 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (4.1.1)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.9.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: librosa==0.8.1 in /usr/local/lib/python3.7/dist-packages (0.8.1)\n","Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (0.10.3.post1)\n","Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (3.0.0)\n","Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (0.4.0)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (1.1.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (1.21.6)\n","Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (0.56.2)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (1.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (21.3)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (4.4.2)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (1.6.0)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1) (1.7.3)\n","Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa==0.8.1) (57.4.0)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa==0.8.1) (0.39.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa==0.8.1) (4.12.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa==0.8.1) (3.0.9)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.1) (2.23.0)\n","Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.1) (1.4.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (1.24.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.1) (3.1.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa==0.8.1) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.8.1) (2.21)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa==0.8.1) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa==0.8.1) (3.8.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting soundfile==0.10.2\n","  Downloading SoundFile-0.10.2-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile==0.10.2) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile==0.10.2) (2.21)\n","Installing collected packages: soundfile\n","  Attempting uninstall: soundfile\n","    Found existing installation: SoundFile 0.10.3.post1\n","    Uninstalling SoundFile-0.10.3.post1:\n","      Successfully uninstalled SoundFile-0.10.3.post1\n","Successfully installed soundfile-0.10.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting bokeh==2.3.0\n","  Downloading bokeh-2.3.0.tar.gz (10.6 MB)\n","\u001b[K     |████████████████████████████████| 10.6 MB 12.3 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (6.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (2.8.2)\n","Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (2.11.3)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (1.21.6)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (7.1.2)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (21.3)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (5.1.1)\n","Requirement already satisfied: typing_extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh==2.3.0) (4.1.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.7->bokeh==2.3.0) (2.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh==2.3.0) (3.0.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh==2.3.0) (1.15.0)\n","Building wheels for collected packages: bokeh\n","  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bokeh: filename=bokeh-2.3.0-py3-none-any.whl size=11292273 sha256=ef38a156df5810e2267aa1108e7f99da7571245bc991d5e74cceabe592987e64\n","  Stored in directory: /root/.cache/pip/wheels/fe/2b/67/993b844d1b11a6129b91880955c1b315438d00fc39d2dcf489\n","Successfully built bokeh\n","Installing collected packages: bokeh\n","  Attempting uninstall: bokeh\n","    Found existing installation: bokeh 2.3.3\n","    Uninstalling bokeh-2.3.3:\n","      Successfully uninstalled bokeh-2.3.3\n","Successfully installed bokeh-2.3.0\n"]}],"source":["!pip install torch==1.9.0\n","!pip install librosa==0.8.1\n","!pip install soundfile==0.10.2\n","!pip install bokeh==2.3.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jkAW3HWGajZj"},"outputs":[],"source":["import torch\n","import librosa\n","import soundfile as sf\n","import torch.nn as nn\n","import numpy as np\n","from torch.nn.utils import weight_norm\n","from torch import optim\n","from math import ceil\n","import glob\n","import time\n","import random\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44656,"status":"ok","timestamp":1663863969172,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"},"user_tz":-60},"id":"LqIHLYluDlVe","outputId":"838d6e18-2ea1-48a8-9062-250aea7ef9ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["#Connect colab to your google drive\n","from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3656,"status":"ok","timestamp":1663863972825,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"},"user_tz":-60},"id":"UgrIrL9rKmI3","outputId":"b87d0f8d-6f75-431f-d598-84f1e195398d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SajXpcQKDUgM"},"outputs":[],"source":["#prepare input folder\n","input_folder='inputs'\n","if not os.path.exists(input_folder):\n","    os.mkdir(input_folder)\n","#copy file from drive to colab --remember it has to be from mydrive---for whatever reason it does not go any deeper!\n","!cp /content/drive/MyDrive/S5_V.wav /content/inputs/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4q0K6-KXcxmO"},"outputs":[],"source":["#paramaters\n","inpainting_indices= [0, 1]\n","is_cuda = torch.cuda.is_available()\n","gpu_num = 0\n","manual_random_seed = -1\n","input_file = 'S5_V.wav'\n","segments_to_train = []\n","start_time = 0\n","init_sample_rate =  16000\n","fs_list = [320, 400, 500, 640, 800, 1000, 1280, 1600, 2000, 2500, 4000, 8000, 10000, 12000, 14400, 16000]\n","max_length = 25\n","run_mode = 'normal' \n","num_epochs = 3000\n","learning_rate = 0.0015\n","scheduler_lr_decay = 0.1\n","beta1 = 0.5\n","speech = False\n","num_layers = 8\n","output_folder = 'outputs'\n","filter_size = 9\n","set_first_scale_by_energy = True\n","min_energy_th = 0.0025\n","hidden_channels_init = 16\n","growing_hidden_channels_factor = 6\n","plot_losses = False\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","initial_noise_amp = 1\n","noise_amp_factor = 0.01"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bELoeAltdEdB"},"outputs":[],"source":["#functions\n","def get_input_signal(input_file, max_length):\n","    file_name = input_file.split('.')\n","    if len(file_name) < 2:\n","        input_file = '.'.join([input_file, 'wav'])\n","    output_folder = file_name[0].replace(' ', '_')\n","    if len(segments_to_train) == 0:\n","        samples, Fs = librosa.load(os.path.join('inputs', input_file), sr=None,\n","                                   offset=start_time, duration=2 * max_length)\n","\n","    if samples.shape[0] / Fs > max_length:\n","        n_samples = int(max_length * Fs)\n","        samples = samples[:n_samples]\n","\n","    output_folder = output_folder\n","    output_folder = os.path.join('outputs', output_folder)\n","    Fs = Fs\n","    if init_sample_rate < Fs:\n","        hr_samples = samples.copy()\n","        samples = librosa.resample(hr_samples, Fs, init_sample_rate)\n","        Fs = init_sample_rate\n","    norm_factor = max(abs(samples.reshape(-1)))\n","    samples = samples / norm_factor\n","    return samples, Fs\n","\n","def create_input_signals(scales, set_first_scale_by_energy, min_energy_th,  filter_size, input_signal, Fs):\n","    # Performs downscaling for desired scales and outputs list of signals\n","    signals_list = []\n","    fs_list = []\n","    n_scales = len(scales)\n","    set_first_scale = False\n","    rf = calc_receptive_field(filter_size, dilation_factors)\n","    for k in range(n_scales):\n","        downsample = scales[k]\n","        fs = int(Fs / downsample)\n","        if downsample == 1:\n","            coarse_sig = input_signal\n","        else:\n","            coarse_sig = torch.Tensor(librosa.resample(input_signal.squeeze().numpy(), Fs, fs))\n","        if speech and fs < 500:\n","            continue\n","        if set_first_scale_by_energy and not speech:\n","            e = (coarse_sig ** 2).mean()\n","            if e < min_energy_th and not set_first_scale:\n","                continue\n","        set_first_scale = True\n","        signals_list.append(coarse_sig)\n","        assert np.mod(fs, 1) == 0, 'Sampling rate is not integer'\n","        fs_list.append(int(fs))\n","\n","        # Write downsampled real sound\n","        filename = 'real@%dHz.wav' % fs\n","        write_signal(os.path.join(output_folder, filename), coarse_sig.cpu(), fs)\n","\n","    return signals_list, fs_list\n","\n","def calc_receptive_field(filter_size, dilation_factors, Fs=None):\n","    if Fs is None:\n","        # in samples\n","        return (filter_size * dilation_factors[0] + sum(dilation_factors[1:]) * (filter_size - 1))\n","    else:\n","        # in [ms]\n","        return (filter_size * dilation_factors[0] + sum(dilation_factors[1:]) * (filter_size - 1)) / Fs * 1e3\n","\n","def write_signal(path, signal, fs, overwrite=False, subtype='PCM_16'):\n","    if signal is None:\n","        return\n","    if torch.is_tensor(signal):\n","        signal = signal.squeeze().detach().cpu().numpy()\n","    if not path.endswith('.wav'):\n","        path = path + '.wav'\n","    if not overwrite:\n","        if os.path.exists(path):\n","            files = glob.glob(path[:-4].replace('[Hz]', '[[]Hz[]]') + '*')\n","            path = path[:-4] + '_' + str(len(files)) + path[-4:]\n","    maxAmp = max(abs(signal.reshape(-1)))\n","    if maxAmp > 1:\n","        signal = signal / maxAmp  # normalize to avoid clipping\n","    sf.write(path, signal, fs, subtype=subtype)\n","\n","def calc_pad_size(dilation_factors, filter_size):\n","    return int(np.ceil(sum(dilation_factors) * (filter_size - 1) / 2))\n","\n","def get_noise(device, shape):\n","    return torch.randn(shape, device=device)\n","\n","def draw_signal(generators_list, signals_lengths_list, fs_list, noise_amp_list, filter_size, dilation_factors, device, reconstruction_noise_list=None,\n","                condition=None, output_all_scales=False):\n","    # Draws a signal up to current scale, using learned generators\n","    pad_size = calc_pad_size(dilation_factors, filter_size)\n","    if output_all_scales:\n","        signals_all_scales = []\n","    for scale_idx, (netG, noise_amp) in enumerate(zip(generators_list, noise_amp_list)):\n","        signal_padder = nn.ConstantPad1d(pad_size, 0)\n","        if condition is None:\n","            n_samples = signals_lengths_list[scale_idx]\n","            if reconstruction_noise_list is not None:\n","                noise_signal = reconstruction_noise_list[scale_idx]\n","            else:\n","                noise_signal = get_noise(device, (1, 1, n_samples))\n","                noise_signal = noise_signal * noise_amp\n","\n","            if scale_idx == 0:\n","                prev_sig = torch.full(noise_signal.shape, 0, device=device, dtype=noise_signal.dtype)\n","            else:\n","                prev_sig = signal_padder(prev_sig)\n","\n","            # pad noise with zeros, to match signal after filtering\n","            if reconstruction_noise_list is None:\n","                # reconstruction_noise is already padded\n","                noise_signal = signal_padder(noise_signal)\n","                if scale_idx == 0:\n","                    prev_sig = signal_padder(prev_sig)\n","        else:\n","            if scale_idx < condition[\"condition_scale_idx\"]:\n","                continue\n","            elif scale_idx == condition[\"condition_scale_idx\"]:\n","                prev_sig = resample_sig(device, condition[\"condition_signal\"], condition['condition_fs'],\n","                                        fs_list[scale_idx]).expand(1, 1, -1)\n","            noise_signal = get_noise(device, prev_sig.shape[2]).expand(1, 1, -1)\n","            noise_signal = signal_padder(noise_signal)\n","            noise_signal = noise_signal * noise_amp\n","            prev_sig = signal_padder(prev_sig)\n","\n","        # Generate this scale signal\n","        cur_sig = netG((noise_signal + prev_sig).detach(), prev_sig)\n","\n","        if output_all_scales:\n","            signals_all_scales.append(torch.squeeze(cur_sig).detach().cpu().numpy())\n","\n","        # Upsample for next scale\n","        if scale_idx < len(fs_list) - 1:\n","            up_sig = resample_sig( device, cur_sig, orig_fs=fs_list[scale_idx], target_fs=fs_list[scale_idx + 1])\n","            if up_sig.shape[2] > signals_lengths_list[scale_idx + 1]:\n","                assert abs(\n","                    up_sig.shape[2] > signals_lengths_list[scale_idx + 1]) < 20, 'Should not happen, check this!'\n","                up_sig = up_sig[:, :, :signals_lengths_list[scale_idx + 1]]\n","            elif up_sig.shape[2] < signals_lengths_list[scale_idx + 1]:\n","                assert abs(\n","                    up_sig.shape[2] < signals_lengths_list[scale_idx + 1]) < 20, 'Should not happen, check this!'\n","                up_sig = torch.cat(\n","                    (up_sig, up_sig.new_zeros(1, 1, signals_lengths_list[scale_idx + 1] - up_sig.shape[2])),\n","                    dim=2)\n","        else:\n","            up_sig = cur_sig\n","        prev_sig = up_sig\n","        prev_sig = prev_sig.detach()\n","\n","        del up_sig, cur_sig, noise_signal, netG\n","\n","    if output_all_scales:\n","        return signals_all_scales\n","    else:\n","        return prev_sig\n","\n","def resample_sig(device,input_signal, orig_fs=None, target_fs=None, resamplers=None):\n","    if resamplers == None:\n","        resamplers = {}\n","    if (orig_fs, target_fs) in resamplers.keys() and resamplers[(orig_fs, target_fs)].in_shape[2] == \\\n","            input_signal.shape[2]:\n","        resampler = resamplers[(orig_fs, target_fs)]\n","    else:\n","        in_shape = input_signal.shape\n","        scale_factors = (1, 1, target_fs / orig_fs)\n","        resampler = ResizeLayer(in_shape, scale_factors=scale_factors, device=device)\n","        resamplers[(orig_fs, target_fs)] = resampler\n","    new_sig = resampler(input_signal)\n","\n","    return new_sig\n","\n","def support_sz(sz):\n","    def wrapper(f):\n","        f.support_sz = sz\n","        return f\n","    return wrapper\n","\n","@support_sz(4)\n","def cubic(x):\n","    fw, to_dtype, eps = set_framework_dependencies(x)\n","    absx = fw.abs(x)\n","    absx2 = absx ** 2\n","    absx3 = absx ** 3\n","    return ((1.5 * absx3 - 2.5 * absx2 + 1.) * to_dtype(absx <= 1.) +\n","            (-0.5 * absx3 + 2.5 * absx2 - 4. * absx + 2.) *\n","            to_dtype((1. < absx) & (absx <= 2.)))\n","\n","class ResizeLayer(nn.Module):\n","    def __init__(self, in_shape, scale_factors=None, out_shape=None,\n","                 interp_method=cubic, support_sz=None,\n","                 antialiasing=True, device=None):\n","        super(ResizeLayer, self).__init__()\n","\n","        # fw stands for framework, that can be either numpy or torch. since\n","        # this is a torch layer, only one option in this case.\n","        fw = torch\n","        eps = fw.finfo(fw.float32).eps\n","\n","        # set missing scale factors or output shapem one according to another,\n","        # scream if both missing\n","        scale_factors, out_shape = set_scale_and_out_sz(in_shape, out_shape,\n","                                                        scale_factors, fw)\n","        \n","        # unless support size is specified by the user, it is an attribute\n","        # of the interpolation method\n","        if support_sz is None:\n","            support_sz = interp_method.support_sz\n","        \n","        self.n_dims = len(in_shape)       \n","\n","        # sort indices of dimensions according to scale of each dimension.\n","        # since we are going dim by dim this is efficient\n","        self.sorted_filtered_dims_and_scales = [(dim, scale_factors[dim])\n","                                                for dim in\n","                                                sorted(range(self.n_dims),\n","                                                key=lambda ind:\n","                                                scale_factors[ind])\n","                                                if scale_factors[dim] != 1.]\n","\n","        # iterate over dims\n","        field_of_view_list = []\n","        weights_list = []\n","        for dim, scale_factor in self.sorted_filtered_dims_and_scales:\n","\n","            # get 1d set of weights and fields of view for each output\n","            # location along this dim\n","            field_of_view, weights = prepare_weights_and_field_of_view_1d(\n","                dim, scale_factor, in_shape[dim], out_shape[dim],\n","                interp_method, support_sz, antialiasing, fw, eps, device)\n","\n","            # keep weights and fields of views for all dims\n","            weights_list.append(nn.Parameter(weights, requires_grad=False))\n","            field_of_view_list.append(nn.Parameter(field_of_view,\n","                                      requires_grad=False))\n","\n","        self.field_of_view = nn.ParameterList(field_of_view_list)\n","        self.weights = nn.ParameterList(weights_list)\n","        self.in_shape = in_shape\n","\n","    def forward(self, input):\n","        # output begins identical to input and changes with each iteration\n","        output = input\n","\n","        for (dim, scale_factor), field_of_view, weights in zip(\n","                self.sorted_filtered_dims_and_scales,\n","                self.field_of_view,\n","                self.weights):\n","            # multiply the weights by the values in the field of view and\n","            # aggreagate\n","            output = apply_weights(output, field_of_view, weights, dim,\n","                                   self.n_dims, torch)\n","        return output\n","\n","def prepare_weights_and_field_of_view_1d(dim, scale_factor, in_sz, out_sz,\n","                                         interp_method, support_sz, \n","                                         antialiasing, fw, eps, device=None):\n","    # If antialiasing is taking place, we modify the window size and the\n","    # interpolation method (see inside function)\n","    interp_method, cur_support_sz = apply_antialiasing_if_needed(\n","                                                             interp_method,\n","                                                             support_sz,\n","                                                             scale_factor,\n","                                                             antialiasing)\n","\n","    # STEP 1- PROJECTED GRID: The non-integer locations of the projection of\n","    # output pixel locations to the input tensor\n","    projected_grid = get_projected_grid(in_sz, out_sz, scale_factor, fw, device)\n","\n","    # STEP 2- FIELDS OF VIEW: for each output pixels, map the input pixels\n","    # that influence it\n","    field_of_view = get_field_of_view(projected_grid, cur_support_sz, in_sz,\n","                                      fw, eps)\n","\n","    # STEP 3- CALCULATE WEIGHTS: Match a set of weights to the pixels in the\n","    # field of view for each output pixel\n","    weights = get_weights(interp_method, projected_grid, field_of_view)\n","\n","    return field_of_view, weights\n","\n","def apply_weights(input, field_of_view, weights, dim, n_dims, fw):\n","    # STEP 4- APPLY WEIGHTS: Each output pixel is calculated by multiplying\n","    # its set of weights with the pixel values in its field of view.\n","    # We now multiply the fields of view with their matching weights.\n","    # We do this by tensor multiplication and broadcasting.\n","    # this step is separated to a different function, so that it can be\n","    # repeated with the same calculated weights and fields.\n","\n","    # for this operations we assume the resized dim is the first one.\n","    # so we transpose and will transpose back after multiplying\n","    tmp_input = fw_swapaxes(input, dim, 0, fw)\n","\n","    # field_of_view is a tensor of order 2: for each output (1d location\n","    # along cur dim)- a list of 1d neighbors locations.\n","    # note that this whole operations is applied to each dim separately,\n","    # this is why it is all in 1d.\n","    # neighbors = tmp_input[field_of_view] is a tensor of order image_dims+1:\n","    # for each output pixel (this time indicated in all dims), these are the\n","    # values of the neighbors in the 1d field of view. note that we only\n","    # consider neighbors along the current dim, but such set exists for every\n","    # multi-dim location, hence the final tensor order is image_dims+1.\n","    neighbors = tmp_input[field_of_view]\n","\n","    # weights is an order 2 tensor: for each output location along 1d- a list\n","    # of weighs matching the field of view. we augment it with ones, for\n","    # broadcasting, so that when multiplies some tensor the weights affect\n","    # only its first dim.\n","    tmp_weights = fw.reshape(weights, (*weights.shape, * [1] * (n_dims - 1)))\n","\n","    # now we simply multiply the weights with the neighbors, and then sum\n","    # along the field of view, to get a single value per out pixel\n","    tmp_output = (neighbors * tmp_weights).sum(1)\n","\n","    # we transpose back the resized dim to its original position\n","    return fw_swapaxes(tmp_output, 0, dim, fw)\n","\n","def get_weights(interp_method, projected_grid, field_of_view):\n","    # the set of weights per each output pixels is the result of the chosen\n","    # interpolation method applied to the distances between projected grid\n","    # locations and the pixel-centers in the field of view (distances are\n","    # directed, can be positive or negative)\n","    weights = interp_method(projected_grid[:, None] - field_of_view)\n","\n","    # we now carefully normalize the weights to sum to 1 per each output pixel\n","    sum_weights = weights.sum(1, keepdims=True)\n","    sum_weights[sum_weights == 0] = 1\n","    return weights / sum_weights\n","\n","def fw_ceil(x, fw):\n","    return x.ceil().long()\n","\n","\n","def fw_cat(x, fw):\n","    return fw.cat(x)\n","\n","\n","def fw_swapaxes(x, ax_1, ax_2, fw):\n","    return x.transpose(ax_1, ax_2)\n","    \n","def fw_set_device(x, device, fw):\n","    return x.to(device)\n","\n","def set_scale_and_out_sz(in_shape, out_shape, scale_factors, fw):\n","    # eventually we must have both scale-factors and out-sizes for all in/out\n","    # dims. however, we support many possible partial arguments\n","    if scale_factors is None and out_shape is None:\n","        raise ValueError(\"either scale_factors or out_shape should be \"\n","                         \"provided\")\n","    if out_shape is not None:\n","        # if out_shape has less dims than in_shape, we defaultly resize the\n","        # first dims for numpy and last dims for torch\n","        out_shape = list(out_shape) + list(in_shape[:-len(out_shape)])\n","        if scale_factors is None:\n","            # if no scale given, we calculate it as the out to in ratio\n","            # (not recomended)\n","            scale_factors = [out_sz / in_sz for out_sz, in_sz\n","                             in zip(out_shape, in_shape)]\n","    if scale_factors is not None:\n","        # by default, if a single number is given as scale, we assume resizing\n","        # two dims (most common are images with 2 spatial dims)\n","        scale_factors = (scale_factors\n","                         if isinstance(scale_factors, (list, tuple))\n","                         else [scale_factors, scale_factors])\n","        # if less scale_factors than in_shape dims, we defaultly resize the\n","        # first dims for numpy and last dims for torch\n","        scale_factors = list(scale_factors) + [1] * (len(in_shape) - len(scale_factors)) \n","        if out_shape is None:\n","            # when no out_shape given, it is calculated by multiplying the\n","            # scale by the in_shape (not recomended)\n","            out_shape = [ceil(scale_factor * in_sz)\n","                         for scale_factor, in_sz in\n","                         zip(scale_factors, in_shape)]\n","        # next line intentionally after out_shape determined for stability\n","        scale_factors = [float(sf) for sf in scale_factors]\n","    return scale_factors, out_shape\n","\n","def apply_antialiasing_if_needed(interp_method, support_sz, scale_factor,\n","                                 antialiasing):\n","    # antialiasing is \"stretching\" the field of view according to the scale\n","    # factor (only for downscaling). this is low-pass filtering. this\n","    # requires modifying both the interpolation (stretching the 1d\n","    # function and multiplying by the scale-factor) and the window size.\n","    if scale_factor >= 1.0 or not antialiasing:\n","        return interp_method, support_sz\n","    cur_interp_method = (lambda arg: scale_factor *\n","                         interp_method(scale_factor * arg))\n","    cur_support_sz = support_sz / scale_factor\n","    return cur_interp_method, cur_support_sz\n","\n","def get_projected_grid(in_sz, out_sz, scale_factor, fw, device=None):\n","    # we start by having the ouput coordinates which are just integer locations\n","    out_coordinates = fw.arange(out_sz)\n","    \n","    # if using torch we need to match the grid tensor device to the input device\n","    out_coordinates = fw_set_device(out_coordinates, device, fw)\n","        \n","    # This is projecting the ouput pixel locations in 1d to the input tensor,\n","    # as non-integer locations.\n","    # the following fomrula is derived in the paper\n","    # \"From Discrete to Continuous Convolutions\" by Shocher et al.\n","    return (out_coordinates / scale_factor +\n","            (in_sz - 1) / 2 - (out_sz - 1) / (2 * scale_factor))\n","\n","\n","def get_field_of_view(projected_grid, cur_support_sz, in_sz, fw, eps):\n","    # for each output pixel, map which input pixels influence it, in 1d.\n","    # we start by calculating the leftmost neighbor, using half of the window\n","    # size (eps is for when boundary is exact int)\n","    left_boundaries = fw_ceil(projected_grid - cur_support_sz / 2 - eps, fw)\n","\n","    # then we simply take all the pixel centers in the field by counting\n","    # window size pixels from the left boundary\n","    ordinal_numbers = fw.arange(ceil(cur_support_sz - eps))\n","    # in case using torch we need to match the device\n","    ordinal_numbers = fw_set_device(ordinal_numbers, projected_grid.device, fw)\n","    field_of_view = left_boundaries[:, None] + ordinal_numbers\n","\n","    # next we do a trick instead of padding, we map the field of view so that\n","    # it would be like mirror padding, without actually padding\n","    # (which would require enlarging the input tensor)\n","    mirror = fw_cat((fw.arange(in_sz), fw.arange(in_sz - 1, -1, step=-1)), fw)\n","    field_of_view = mirror[fw.remainder(field_of_view, mirror.shape[0])]\n","    field_of_view = fw_set_device(field_of_view,projected_grid.device, fw)\n","    return field_of_view\n","\n","def set_framework_dependencies(x):\n","    if type(x) is np.ndarray:\n","        to_dtype = lambda a: a\n","        fw = np\n","    else:\n","        to_dtype = lambda a: a.to(x.dtype)\n","        fw = torch\n","    eps = fw.finfo(fw.float32).eps\n","    return fw, to_dtype, eps\n","\n","def calc_gradient_penalty(run_mode, current_holes, netD, real_data, fake_data, LAMBDA, alpha=None, _grad_outputs=None, mask_ratio=None, not_valid_idx_start=None, not_valid_idx_end=None):\n","    # Gradient penalty method for WGAN\n","    if alpha is None:\n","        alpha = torch.rand(1, 1)\n","        alpha = alpha.expand(real_data.size())\n","        if torch.cuda.is_available():\n","            alpha = alpha.cuda(real_data.get_device())  # gpu) #if use_cuda else alpha\n","    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","    interpolates = torch.autograd.Variable(interpolates, requires_grad=True)\n","    use_mask = False\n","    mask_ratio = 1\n","    disc_interpolates = netD(interpolates, use_mask)\n","    if _grad_outputs is None:\n","        _grad_outputs = torch.ones(disc_interpolates.size())\n","        if torch.cuda.is_available():\n","            _grad_outputs = _grad_outputs.cuda(real_data.get_device())\n","    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n","                                    grad_outputs=_grad_outputs,\n","                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n","    gradient_penalty = ((mask_ratio * gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n","    del gradients, interpolates, _grad_outputs, disc_interpolates\n","    return gradient_penalty\n","\n","def stft(sig, n_fft, hop_length, window_size):\n","    s = torch.stft(sig, n_fft, hop_length, win_length=window_size,\n","                   window=torch.hann_window(window_size, device=sig.device), return_complex=False)\n","    return s\n","\n","def spec(x, n_fft, hop_length, window_size):\n","    s = stft(x, n_fft, hop_length, window_size)\n","    n = torch.norm(s, p=2, dim=-1)\n","    return n\n","\n","def norm(x):\n","    return (x.view(x.shape[0], -1) ** 2).sum(dim=-1).sqrt()\n","\n","\n","def squeeze(x):\n","    if len(x.shape) == 3:\n","        assert x.shape[-1] in [1, 2]\n","        x = torch.mean(x, -1)\n","    if len(x.shape) != 2:\n","        raise ValueError(f'Unknown input shape {x.shape}')\n","    return x\n","\n","def multi_scale_spectrogram_loss(multispec_loss_n_fft, multispec_loss_hop_length, multispec_loss_window_size, current_holes, x_in, x_out):\n","    losses = []\n","    args = [multispec_loss_n_fft,\n","            multispec_loss_hop_length,\n","            multispec_loss_window_size]\n","    for n_fft, hop_length, window_size in zip(*args):\n","        if window_size == -1:\n","            window_size = x_in.shape[1]\n","            hop_length = window_size + 1\n","            n_fft = int(2 ** np.ceil(np.log2(window_size)))\n","        spec_in = spec(squeeze(x_in.float()), n_fft, hop_length, window_size)\n","        spec_out = spec(squeeze(x_out.float()), n_fft, hop_length, window_size)\n","        losses.append(norm(spec_in - spec_out))\n","    return sum(losses) / len(losses)\n","\n","def reset_grads(model, require_grad):\n","    for p in model.parameters():\n","        p.requires_grad_(require_grad)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uv_GjFaJDb6G"},"outputs":[],"source":["#model \n","class Generator(nn.Module):\n","    def __init__(self, filter_size, hidden_channels, current_fs ):\n","        super(Generator, self).__init__()\n","        self.head = ConvBlock(filter_size, 1, hidden_channels, dilation_factors[0])\n","        self.body = nn.Sequential()\n","        self.Fs = current_fs\n","        for i in range(num_layers - 2):\n","            block = ConvBlock(filter_size, hidden_channels, hidden_channels, dilation_factors[i + 1])\n","            self.body.add_module('block%d' % (i + 1), block)\n","        self.tail = nn.Sequential()\n","        self.tail.add_module('tail0',\n","                             NormConv1d(in_channels=hidden_channels, out_channels=hidden_channels,\n","                                        kernel_size=filter_size,\n","                                        dilation=dilation_factors[-1]))\n","        self.filter = nn.Sequential(\n","            NormConv1d(in_channels=hidden_channels, out_channels=hidden_channels,\n","                       kernel_size=filter_size, padding=int((filter_size - 1) / 2)),\n","            nn.Tanh()\n","        )\n","        self.gate = nn.Sequential(\n","            NormConv1d(in_channels=hidden_channels, out_channels=hidden_channels,\n","                       kernel_size=filter_size, padding=int((filter_size - 1) / 2)),\n","            nn.Sigmoid()\n","        )\n","        self.out_conv = NormConv1d(hidden_channels, 1, kernel_size=1)\n","        self.pe_filter = PreEmphasisFilter(device)\n","\n","    def forward(self, noise_plus_sig, prev_sig):\n","        out_head = self.head(noise_plus_sig)\n","        out_body = self.body(out_head)\n","        out_tail = self.tail(out_body)\n","        filter = self.filter(out_tail)\n","        gate = self.gate(out_tail)\n","        out_tail = filter * gate\n","        out_tail = self.out_conv(out_tail)\n","        out_filt = self.pe_filter(out_tail)\n","        ind = int((prev_sig.shape[2] - out_filt.shape[2]) / 2)\n","        prev_sig = prev_sig[:, :, ind:(prev_sig.shape[2] - ind)]\n","        output = out_filt + prev_sig\n","        return output\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, run_mode, current_holes, hidden_channels, dilation_factors, num_layers, device,filter_size ):\n","        super(Discriminator, self).__init__()\n","        if run_mode == 'inpainting':\n","            mask = current_holes\n","        else:\n","            mask = None\n","        self.head = ConvBlock(filter_size, 1, hidden_channels, dilation_factors[0], mask=mask)\n","        mask = self.head.mask_out\n","        self.body = nn.ModuleList()\n","        for i in range(num_layers - 2):\n","            block = ConvBlock(filter_size, hidden_channels, hidden_channels,\n","                              dilation_factors[i + 1], mask=mask)\n","            mask = block.mask_out\n","            self.body.add_module('block%d' % (i + 1), block)\n","        self.mask_out = mask\n","        self.tail = NormConv1d(hidden_channels, 1, kernel_size=filter_size,\n","                               dilation=dilation_factors[-1])\n","        self.pe_filter = PreEmphasisFilter(device)\n","\n","    def forward(self, sig, use_mask=False):\n","        out_head = self.head(sig, use_mask)\n","        out_body = out_head\n","        for b in self.body:\n","            out_body = b(out_body, use_mask)\n","        out_tail = self.tail(out_body)\n","        output = self.pe_filter(out_tail)\n","        return output\n","\n","\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1 and classname.find('ConvBlock') == -1 and hasattr(m, 'weight'):\n","        if m.weight.numel() > 1 and m.weight.requires_grad:  # scalar blocks are initiailized upon creation\n","            m.weight.data.normal_(0.0, 0.02)\n","\n","    elif classname.find('Norm') != -1 and hasattr(m, 'weight'):\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n","\n","class PreEmphasisFilter(nn.Module):\n","    def __init__(self, device):\n","        super(PreEmphasisFilter, self).__init__()\n","        self.alpha = torch.Tensor([0.97]).to(device)\n","        self.alpha.requires_grad = False\n","\n","    def forward(self, x):\n","        output = torch.cat((x[:, :, 0].view(x.shape[0], x.shape[1], 1), x[:, :, 1:] - self.alpha * x[:, :, :-1]), dim=2)\n","        return output\n","\n","\n","class NormConv1d(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=True):\n","        super(NormConv1d, self).__init__()\n","        self.conv = weight_norm(nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n","                                          stride=stride, padding=padding, dilation=dilation, bias=bias))\n","\n","    def forward(self, x):\n","        output = self.conv(x)\n","        return output\n","\n","\n","class ConvBlock(nn.Sequential):\n","    def __init__(self, filter_size, in_channels, out_channels, dilation=1, mask=None):\n","        super(ConvBlock, self).__init__()\n","        if filter_size is None:\n","            filter_size = filter_size\n","        if mask is not None:\n","            self.mask_in = mask\n","            self.mask_out = []\n","            self.rf = int((filter_size - 1) * dilation)\n","            for hole in self.mask_in:\n","                self.mask_out.append([hole[0] - self.rf, hole[1]])\n","            # ???\n","            # for idx in range(len(self.mask_out) - 1):\n","            #     if self.mask_out[idx+1][0] < self.mask_out[idx][1]:\n","            #         self.mask_out[idx+1][0] = self.mask_out[idx][1] + 1\n","\n","        else:\n","            self.mask_out = None\n","        self.conv = NormConv1d(in_channels, out_channels, filter_size, dilation=dilation)\n","        self.norm = nn.BatchNorm1d(out_channels)\n","        self.activation = nn.LeakyReLU(0.2, inplace=True)\n","\n","    def forward(self, x, use_mask=False):\n","        out_conv = self.conv(x)\n","        if use_mask:\n","            #tmp = torch.cat((out_conv[:, :, :int(self.mask_out[0][0])], out_conv[:, :, int(self.mask_out[0][1] + 1):]), dim=2)\n","            tmp = out_conv[:, :, :int(self.mask_out[0][0])].clone()\n","            cut_idx = []\n","            cut_idx.append(tmp.shape[2])\n","            for idx in range(len(self.mask_out)-1):\n","                tmp = torch.cat((tmp, out_conv[:, :, int(self.mask_out[idx][1] + 1):int(self.mask_out[idx+1][0])]), dim=2)\n","                cut_idx.append(tmp.shape[2])\n","            tmp = torch.cat((tmp, out_conv[:, :, int(self.mask_out[-1][1] + 1):]), dim=2)\n","\n","            tmp_norm = self.norm(tmp)\n","            out_norm = out_conv\n","            out_norm[:, :, :int(self.mask_out[0][0])] = tmp_norm[:, :, :int(cut_idx[0])]\n","            for idx in range(len(self.mask_out) - 1):\n","                out_norm[:, :, int(self.mask_out[idx][1] + 1):int(self.mask_out[idx+1][0])] = tmp_norm[:, :, int(cut_idx[idx]):int(cut_idx[idx+1])] #tmp_norm[:, :, int(self.mask_out[idx][0]):int(self.mask_out[idx+1][0])]\n","                #out_norm[:, :, :int(self.mask_out[idx+1][0])] = tmp_norm[:, :, :int(self.mask_out[idx+1][0])]\n","            out_norm[:, :, int(self.mask_out[-1][1] + 1):] = tmp_norm[:, :, int(cut_idx[-1]):]\n","\n","        else:\n","            out_norm = self.norm(out_conv)\n","        return self.activation(out_norm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_r37Na9OBSA8"},"outputs":[],"source":["#training functions\n","def train(manual_random_seed, fs_list, scales, growing_hidden_channels_factor,learning_rate, beta1, scheduler_lr_decay, plot_losses,\n","          initial_noise_amp, noise_amp_factor, signals_list, dilation_factors, output_folder, inputs_lengths):\n","    if manual_random_seed != -1:\n","        random.seed(manual_random_seed)\n","        torch.manual_seed(manual_random_seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","    fs_list = fs_list\n","    n_scales = len(scales)\n","    generators_list = []\n","    noise_amp_list = []\n","    if run_mode == 'inpainting':\n","        energy_list = [(sig[mask] ** 2).mean().item() for sig, mask in zip(signals_list, masks)]\n","    else:\n","        energy_list = [(sig ** 2).mean().item() for sig in signals_list]\n","    reconstruction_noise_list = []\n","    output_signals = []\n","    loss_vectors = []\n","\n","    for scale_idx in range(n_scales):\n","        output_signals_single_scale, loss_vectors_single_scale, netG, reconstruction_noise_list, noise_amp = train_single_scale(\n","                      scales, device, run_mode, hidden_channels_init, growing_hidden_channels_factor,  learning_rate, beta1, \n","                      scheduler_lr_decay, plot_losses, initial_noise_amp, noise_amp_factor, signals_list, fs_list, \n","                      generators_list, noise_amp_list, energy_list, reconstruction_noise_list, dilation_factors, output_folder, inputs_lengths)\n","\n","        # Write fake sound\n","        fake_sound = output_signals_single_scale['fake_signal'].squeeze()\n","        filename = 'fake@%dHz.wav' % fs_list[scale_idx]\n","        write_signal(os.path.join(output_folder, filename), fake_sound,\n","                     fs_list[scale_idx], overwrite=False)\n","\n","        # Write reconstructed sound\n","        reconstructed_sound = output_signals_single_scale['reconstructed_signal'].squeeze()\n","        filename = 'reconstructed@%dHz.wav' % fs_list[scale_idx]\n","        write_signal(os.path.join(output_folder, filename),\n","                     reconstructed_sound, fs_list[scale_idx], overwrite=False)\n","        torch.save(reconstruction_noise_list,\n","                   os.path.join(output_folder, 'reconstruction_noise_list.pt'))\n","\n","        generators_list.append(netG)\n","        noise_amp_list.append(noise_amp)\n","        output_signals.append(output_signals_single_scale)\n","        loss_vectors.append(loss_vectors_single_scale)\n","\n","    return output_signals, loss_vectors, generators_list, noise_amp_list, energy_list, reconstruction_noise_list\n","\n","\n","def train_single_scale(scales, device, run_mode, hidden_channels_init, growing_hidden_channels_factor,\n","                       learning_rate, beta1, scheduler_lr_decay, plot_losses, initial_noise_amp, noise_amp_factor, signals_list,\n","                        fs_list, generators_list, noise_amp_list, energy_list, reconstruction_noise_list, dilation_factors, output_folder, inputs_lengths):\n","    # Terminology: 0 is the higher scale (original signal, no downsampling). Higher scale means larger downsampling, e.g shorter signals\n","    n_scales = len(scales)\n","    current_scale = n_scales - len(generators_list) - 1\n","    scale_idx = n_scales - current_scale - 1\n","    input_signal = signals_list[scale_idx].to(device)\n","    current_fs = fs_list[scale_idx]\n","    N = len(input_signal)\n","\n","    if run_mode == 'inpainting':\n","        current_mask = masks[scale_idx]\n","        current_mask = current_mask\n","        current_holes = torch.Tensor([(int(idx[0] / Fs * current_fs), int(idx[1] / Fs * current_fs)) for idx in inpainting_indices]).to(device)\n","    else:\n","        current_holes = None\n","\n","    # Create inputs\n","    real_signal = input_signal.reshape(1, 1, N)\n","\n","    hidden_channels = hidden_channels_init if scale_idx == 0 else int(\n","        hidden_channels_init * growing_hidden_channels_factor)\n","\n","    scale_num = n_scales - scale_idx - 1\n","    pad_size = calc_pad_size(dilation_factors, filter_size)\n","    signal_padder = nn.ConstantPad1d(pad_size, 0)\n","\n","    # Initialize models\n","    netD = Discriminator(run_mode, current_holes, hidden_channels, dilation_factors, num_layers, device, filter_size).to(device)\n","    netD.apply(weights_init)\n","    netG = Generator(filter_size, hidden_channels, current_fs).to(device)\n","    netG.apply(weights_init)\n","    receptive_field = calc_receptive_field(filter_size, dilation_factors, current_fs)\n","    receptive_field_percent = 100 * receptive_field / 1e3 / (N / current_fs)\n","    print('Signal in scale %d has %d samples, sample rate is %d[Hz].' % (\n","        scale_num, N, current_fs))\n","    print('Total receptive field is %d[msec] (%.1f%% of input).' % (receptive_field, receptive_field_percent))\n","    with open(os.path.join(output_folder, 'log.txt'), 'a') as f:\n","        f.write('*' * 30 + ' Scale ' + str(scale_num) + ' (' + str(current_fs) + ' [Hz]) ' + '*' * 30)\n","        f.write('\\nreceptive_field = %d[msec] (%.1f%% of input)' % (receptive_field, receptive_field_percent))\n","        f.write('\\nsignal_energy = %.4f' % energy_list[scale_idx])\n","\n","    if scale_idx == 0:\n","        reconstruction_noise = get_noise(device, real_signal.shape)\n","    else:\n","        reconstruction_noise = torch.zeros(real_signal.shape, device=device)\n","        if run_mode == 'inpainting':\n","            reconstruction_noise[:, :, torch.logical_not(current_mask)] = get_noise(device, torch.nonzero(\n","                torch.logical_not(current_mask)).shape[0]).expand(1, 1, -1).to(device)\n","\n","    reconstruction_noise = signal_padder(reconstruction_noise)\n","\n","    if scale_idx > 1:\n","        netG.load_state_dict(\n","            torch.load('%s/netGScale%d.pth' % (output_folder, scale_idx - 1), map_location=device))\n","        netD.load_state_dict(\n","            torch.load('%s/netDScale%d.pth' % (output_folder, scale_idx - 1), map_location=device))\n","\n","    output_folder = output_folder\n","\n","    # Create optimizers\n","    optimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n","    optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n","    schedulerD = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizerD, milestones=scheduler_milestones,\n","                                                      gamma=scheduler_lr_decay)\n","    schedulerG = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizerG, milestones=scheduler_milestones,\n","                                                      gamma=scheduler_lr_decay)\n","\n","    # Initialize error vectors\n","    v_err_real = np.zeros(num_epochs, )\n","    v_err_fake = np.zeros(num_epochs, )\n","    v_gp = np.zeros(num_epochs, )\n","    v_rec_loss = np.zeros(num_epochs, )\n","\n","    epochs_start_time = time.time()\n","    # prepare inputs for gradient penalty\n","    if not run_mode == 'inpainting':\n","        D_out_shape = torch.Size((1, 1, N - 2 * pad_size))\n","        _grad_outputs = torch.ones(D_out_shape, device=device)\n","    grad_pen_alpha_vec = torch.rand(num_epochs).to(device)\n","\n","    inputs_lengths = inputs_lengths\n","    for epoch_num in range(num_epochs):\n","        print_progress = epoch_num % 100 == 0\n","        # Create noise\n","        noise_signal = get_noise(device, real_signal.shape)\n","        noise_signal = signal_padder(noise_signal)\n","        #################################################################\n","        # Optimize D by maximizing D(realSignal)+(1-D(G(noise_signal))) #\n","        #################################################################\n","        netD.zero_grad()\n","        # Run on real signal\n","        not_valid_idx_start = []\n","        not_valid_idx_end = []\n","        if run_mode == 'inpainting':\n","            out_D_real = netD(real_signal, use_mask=True)\n","            tot_samples = out_D_real.shape[2]\n","            not_valid_idx_start = [int(idx[0] - receptive_field / 1e3 * current_fs + 1) for idx in current_holes]\n","            not_valid_idx_end = [int(idx[1] + 1) for idx in current_holes]  # +1 is because of pe filter\n","            out_D_real_cp = out_D_real.clone()\n","            out_D_real = out_D_real_cp[:, :, :not_valid_idx_start[0]]\n","            if len(current_holes) > 1:\n","                for i in range(len(current_holes) - 1):\n","                    out_D_real = torch.cat((out_D_real, out_D_real_cp[:, :, not_valid_idx_end[i] + 1:not_valid_idx_start[i+1]]), dim=2)\n","            out_D_real = torch.cat((out_D_real, out_D_real_cp[:, :, not_valid_idx_end[-1] + 1:]), dim=2)\n","            mask_ratio = tot_samples / out_D_real.shape[2]\n","        else:\n","            mask_ratio = 1\n","            out_D_real = netD(real_signal)\n","        err_real_D = -out_D_real.mean()\n","        err_real_D.backward(retain_graph=True)\n","        err_real_D = err_real_D.detach()\n","        if print_progress or plot_losses:\n","            err_real_D_val = err_real_D.item()\n","\n","        if epoch_num == 0:\n","            if run_mode == 'inpainting':\n","                D_out_shape = out_D_real.shape\n","                _grad_outputs = torch.ones(D_out_shape, device=device)\n","            if scale_idx == 0:  # We are at coarsest scale\n","                prev_signal = torch.full(noise_signal.shape, 0, device=device, dtype=noise_signal.dtype)\n","                prev_reconstructed_signal = torch.zeros(reconstruction_noise.shape, device=device)\n","                noise_amp = initial_noise_amp\n","            else:\n","                prev_signal = draw_signal(generators_list, inputs_lengths, fs_list, noise_amp_list, filter_size, dilation_factors, device)\n","                prev_signal = signal_padder(prev_signal)\n","                prev_reconstructed_signal = draw_signal(generators_list, inputs_lengths,\n","                                                        fs_list,\n","                                                        noise_amp_list, filter_size, dilation_factors, device,\n","                                                        reconstruction_noise_list)\n","                prev_reconstructed_signal = signal_padder(prev_reconstructed_signal)\n","                innovation = energy_list[scale_idx] - energy_list[scale_idx - 1]\n","                energy_diff = torch.sqrt(torch.Tensor([innovation])).to(device)\n","                noise_amp = noise_amp_factor * max(torch.Tensor([0]).to(device),\n","                                                          energy_diff)\n","\n","            if scale_idx == 1 and add_cond_noise:\n","                noise_amp = prev_reconstructed_signal.std()\n","\n","            with open(os.path.join(output_folder, 'log.txt'), 'a') as f:\n","                f.write('\\nnoise_amp: %.6f' % noise_amp)\n","\n","            reconstruction_noise = reconstruction_noise * noise_amp\n","            reconstruction_noise_list.append(reconstruction_noise)\n","        else:\n","            if scale_idx > 0:\n","                prev_signal = draw_signal(generators_list, inputs_lengths, fs_list, noise_amp_list, filter_size, dilation_factors, device)\n","                prev_signal = signal_padder(prev_signal)\n","\n","        input_noise = noise_signal * noise_amp\n","\n","        # Run on fake signal\n","        fake_signal = netG((input_noise + prev_signal).detach(), prev_signal)\n","        out_D_fake = netD(fake_signal.detach())\n","        err_fake_D = out_D_fake.mean()\n","        del out_D_real, out_D_fake\n","        err_fake_D.backward(retain_graph=True)\n","        err_fake_D = err_fake_D.detach()\n","        if print_progress or plot_losses:\n","            err_fake_D_val = err_fake_D.item()\n","\n","        lambda_grad=0.01\n","        gradient_penalty = calc_gradient_penalty(run_mode, current_holes, netD, real_signal, fake_signal, lambda_grad,\n","                                                 grad_pen_alpha_vec[epoch_num], _grad_outputs, mask_ratio)\n","        gradient_penalty.backward()\n","        if print_progress or plot_losses:\n","            gradient_penalty_val = gradient_penalty.item()\n","        del gradient_penalty\n","\n","        optimizerD.step()\n","\n","        if plot_losses:\n","            v_err_real[epoch_num] = err_real_D_val\n","            v_err_fake[epoch_num] = err_fake_D_val\n","            v_gp[epoch_num] = gradient_penalty_val\n","\n","        #############################################\n","        # Update G by maximizing D(G(noise_signal)) #\n","        #############################################\n","        netG.zero_grad()\n","        output = netD(fake_signal)\n","        errG = -output.mean()\n","        del output\n","        errG.backward(retain_graph=True)\n","        errG = errG.detach()\n","        if print_progress or plot_losses:\n","            errG_val = errG.item()\n","        if scale_idx == 0:\n","            reconstructed_signal = netG((reconstruction_noise + prev_reconstructed_signal).detach(),\n","                                        prev_reconstructed_signal)\n","        else:\n","            reconstructed_signal = netG((reconstruction_noise + prev_reconstructed_signal).detach(),\n","                                        prev_reconstructed_signal)\n","        if alpha1 > 0:\n","            if run_mode == 'inpainting':\n","                rec_loss_t = alpha1 * torch.mean(\n","                    (real_signal[:, :, current_mask] - reconstructed_signal[:, :, current_mask]) ** 2)\n","            else:\n","                rec_loss_t = alpha1 * torch.mean((real_signal - reconstructed_signal) ** 2)\n","        else:\n","            rec_loss_t = 0\n","        if alpha2 > 0:\n","            multispec_loss_n_fft = (2048, 1024, 512)\n","            multispec_loss_hop_length = (240, 120, 50)\n","            multispec_loss_window_size = (1200, 600, 240)\n","            rec_loss_f = alpha2 * multi_scale_spectrogram_loss(multispec_loss_n_fft, multispec_loss_hop_length, multispec_loss_window_size,\n","                                                               current_holes, real_signal.permute(0, 2, 1),reconstructed_signal.permute(0, 2, 1))\n","        else:\n","            rec_loss_f = 0\n","        rec_loss = rec_loss_t + rec_loss_f\n","        rec_loss.backward(retain_graph=True)\n","        rec_loss = rec_loss.detach()\n","        if alpha1 > 0:\n","            rec_loss_t = rec_loss_t.detach()\n","        if alpha2 > 0:\n","            rec_loss_f = rec_loss_f.detach()\n","        if print_progress or plot_losses:\n","            rec_loss_val = rec_loss.item()\n","\n","        optimizerG.step()\n","\n","        if plot_losses:\n","            v_rec_loss[epoch_num] = rec_loss_val\n","\n","        if print_progress:\n","            print('[%d/%d] D(real): %.2f. D(fake): %.2f. rec_loss: %.4f. gp: %.4f ' % (\n","                epoch_num, num_epochs, -err_real_D_val, err_fake_D_val, rec_loss_val, gradient_penalty_val))\n","\n","        schedulerD.step()\n","        schedulerG.step()\n","\n","        # Some memory cleanup\n","        fake_signal = fake_signal.detach()\n","        reconstructed_signal = reconstructed_signal.detach()\n","        if epoch_num < num_epochs - 1:\n","            del fake_signal, reconstructed_signal, rec_loss, rec_loss_t, rec_loss_f\n","        del noise_signal, input_noise\n","        if scale_idx > 0:\n","            del prev_signal\n","\n","    epochs_stop_time = time.time()\n","    runtime_msg = 'Total time in scale %d: %d[sec] (%.2f[sec]/epoch on avg.). D(real): %f, D(fake): %f, rec_loss: %.4f. gp: %.4f' % (\n","        current_scale, epochs_stop_time - epochs_start_time,\n","        (epochs_stop_time - epochs_start_time) / num_epochs,\n","        -err_real_D_val, err_fake_D_val, rec_loss_val, gradient_penalty_val)\n","    print(runtime_msg)\n","    with open(os.path.join(output_folder, 'log.txt'), 'a') as f:\n","        f.write('\\n%s\\n' % runtime_msg)\n","\n","    # Save this scale models\n","    torch.save(netG.state_dict(), '%s/netGScale%d.pth' % (output_folder, scale_idx))\n","    torch.save(netD.state_dict(), '%s/netDScale%d.pth' % (output_folder, scale_idx))\n","    # Pack outputs\n","    if plot_losses:\n","        loss_vectors = {'v_err_real': v_err_real,\n","                        'v_err_fake': v_err_fake,\n","                        'v_rec_loss': v_rec_loss,\n","                        'v_gp': v_gp}\n","    else:\n","        loss_vectors = []\n","    fake_signal = fake_signal.detach().cpu().numpy()[:, 0, :]\n","    reconstructed_signal = reconstructed_signal.detach().cpu().numpy()[:, 0, :]\n","    output_signals = {'fake_signal': fake_signal, 'reconstructed_signal': reconstructed_signal}\n","    del fake_signal, real_signal, netD, _grad_outputs, grad_pen_alpha_vec, input_signal, reconstructed_signal, prev_reconstructed_signal, reconstruction_noise\n","    netG = reset_grads(netG, False)\n","    netG.eval()\n","    if is_cuda:\n","        torch.cuda.empty_cache()\n","    print('*' * 30 + ' Finished working on scale ' + str(current_scale) + ' ' + '*' * 30)\n","    return output_signals, loss_vectors, netG, reconstruction_noise_list, noise_amp"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uzV4gMXQZR92","outputId":"969c0fea-f367-4676-d4d4-c9206206f405","executionInfo":{"status":"ok","timestamp":1663910583950,"user_tz":-60,"elapsed":718281,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Working on file: S5_V.wav\n","Writing results to outputs_2\n","\n","Running on cuda:0\n","Signal in scale 15 has 6720 samples, sample rate is 320[Hz].\n","Total receptive field is 6378[msec] (30.4% of input).\n","[0/3000] D(real): -0.01. D(fake): -0.01. rec_loss: 0.0339. gp: 0.0080 \n","[100/3000] D(real): 0.01. D(fake): -0.03. rec_loss: 0.0189. gp: 0.0163 \n","[200/3000] D(real): 0.05. D(fake): -0.05. rec_loss: 0.0198. gp: 0.0217 \n","[300/3000] D(real): 0.05. D(fake): -0.11. rec_loss: 0.0187. gp: 0.0399 \n","[400/3000] D(real): 0.02. D(fake): -0.14. rec_loss: 0.0167. gp: 0.0442 \n","[500/3000] D(real): -0.01. D(fake): -0.21. rec_loss: 0.0154. gp: 0.0070 \n","[600/3000] D(real): -0.01. D(fake): -0.18. rec_loss: 0.0195. gp: 0.0350 \n","[700/3000] D(real): 0.01. D(fake): -0.23. rec_loss: 0.0189. gp: 0.0618 \n","[800/3000] D(real): 0.01. D(fake): -0.25. rec_loss: 0.0203. gp: 0.0777 \n","[900/3000] D(real): -0.11. D(fake): -0.32. rec_loss: 0.0178. gp: 0.0492 \n","[1000/3000] D(real): -0.24. D(fake): -0.35. rec_loss: 0.0199. gp: 0.0186 \n","[1100/3000] D(real): -0.13. D(fake): -0.42. rec_loss: 0.0203. gp: 0.0311 \n","[1200/3000] D(real): -0.18. D(fake): -0.37. rec_loss: 0.0188. gp: 0.0121 \n","[1300/3000] D(real): -0.11. D(fake): -0.33. rec_loss: 0.0177. gp: 0.0223 \n","[1400/3000] D(real): 0.02. D(fake): -0.37. rec_loss: 0.0181. gp: 0.1950 \n","[1500/3000] D(real): -0.10. D(fake): -0.53. rec_loss: 0.0180. gp: 0.0964 \n","[1600/3000] D(real): -0.09. D(fake): -0.54. rec_loss: 0.0194. gp: 0.1837 \n","[1700/3000] D(real): -0.16. D(fake): -0.50. rec_loss: 0.0279. gp: 0.0578 \n","[1800/3000] D(real): -0.07. D(fake): -0.55. rec_loss: 0.0214. gp: 0.0981 \n","[1900/3000] D(real): -0.14. D(fake): -0.66. rec_loss: 0.0172. gp: 0.0755 \n","[2000/3000] D(real): -0.22. D(fake): -0.46. rec_loss: 0.0180. gp: 0.1053 \n","[2100/3000] D(real): -0.13. D(fake): -0.51. rec_loss: 0.0160. gp: 0.0602 \n","[2200/3000] D(real): -0.06. D(fake): -0.61. rec_loss: 0.0156. gp: 0.2759 \n","[2300/3000] D(real): -0.04. D(fake): -0.58. rec_loss: 0.0156. gp: 0.2811 \n","[2400/3000] D(real): -0.01. D(fake): -0.57. rec_loss: 0.0157. gp: 0.1819 \n","[2500/3000] D(real): -0.04. D(fake): -0.55. rec_loss: 0.0157. gp: 0.1491 \n","[2600/3000] D(real): -0.04. D(fake): -0.58. rec_loss: 0.0161. gp: 0.5674 \n","[2700/3000] D(real): -0.13. D(fake): -0.57. rec_loss: 0.0162. gp: 0.1013 \n","[2800/3000] D(real): -0.01. D(fake): -0.60. rec_loss: 0.0160. gp: 0.0487 \n","[2900/3000] D(real): -0.06. D(fake): -0.47. rec_loss: 0.0156. gp: 0.0793 \n","Total time in scale 15: 183[sec] (0.06[sec]/epoch on avg.). D(real): -0.060841, D(fake): -0.466476, rec_loss: 0.0156. gp: 0.0793\n","****************************** Finished working on scale 15 ******************************\n","Signal in scale 14 has 8400 samples, sample rate is 400[Hz].\n","Total receptive field is 5102[msec] (24.3% of input).\n","[0/3000] D(real): -0.00. D(fake): -0.00. rec_loss: 0.0270. gp: 0.0081 \n","[100/3000] D(real): 0.00. D(fake): -0.01. rec_loss: 0.0194. gp: 0.0284 \n","[200/3000] D(real): 0.02. D(fake): 0.00. rec_loss: 0.0196. gp: 0.0065 \n","[300/3000] D(real): 0.05. D(fake): -0.01. rec_loss: 0.0199. gp: 0.0298 \n","[400/3000] D(real): 0.04. D(fake): -0.05. rec_loss: 0.0190. gp: 0.0297 \n","[500/3000] D(real): 0.02. D(fake): -0.05. rec_loss: 0.0184. gp: 0.0096 \n","[600/3000] D(real): 0.02. D(fake): -0.03. rec_loss: 0.0184. gp: 0.0111 \n","[700/3000] D(real): -0.01. D(fake): -0.05. rec_loss: 0.0175. gp: 0.0268 \n","[800/3000] D(real): 0.04. D(fake): -0.11. rec_loss: 0.0182. gp: 0.0994 \n","[900/3000] D(real): 0.03. D(fake): -0.15. rec_loss: 0.0179. gp: 0.1517 \n","[1000/3000] D(real): -0.01. D(fake): -0.17. rec_loss: 0.0182. gp: 0.0994 \n","[1100/3000] D(real): -0.07. D(fake): -0.21. rec_loss: 0.0181. gp: 0.0353 \n","[1200/3000] D(real): -0.12. D(fake): -0.29. rec_loss: 0.0191. gp: 0.0238 \n","[1300/3000] D(real): -0.03. D(fake): -0.30. rec_loss: 0.0183. gp: 0.0426 \n","[1400/3000] D(real): -0.11. D(fake): -0.32. rec_loss: 0.0179. gp: 0.1026 \n","[1500/3000] D(real): -0.03. D(fake): -0.31. rec_loss: 0.0174. gp: 0.0218 \n","[1600/3000] D(real): -0.24. D(fake): -0.33. rec_loss: 0.0168. gp: 0.0108 \n","[1700/3000] D(real): 0.00. D(fake): -0.27. rec_loss: 0.0175. gp: 0.0392 \n","[1800/3000] D(real): -0.08. D(fake): -0.22. rec_loss: 0.0173. gp: 0.0508 \n","[1900/3000] D(real): 0.03. D(fake): -0.21. rec_loss: 0.0164. gp: 0.2344 \n","[2000/3000] D(real): -0.12. D(fake): -0.35. rec_loss: 0.0158. gp: 0.0982 \n","[2100/3000] D(real): -0.04. D(fake): -0.36. rec_loss: 0.0156. gp: 0.1314 \n","[2200/3000] D(real): -0.01. D(fake): -0.33. rec_loss: 0.0155. gp: 0.1385 \n","[2300/3000] D(real): -0.04. D(fake): -0.38. rec_loss: 0.0154. gp: 0.0825 \n","[2400/3000] D(real): -0.03. D(fake): -0.39. rec_loss: 0.0153. gp: 0.0542 \n","[2500/3000] D(real): -0.04. D(fake): -0.46. rec_loss: 0.0153. gp: 0.0560 \n","[2600/3000] D(real): -0.06. D(fake): -0.42. rec_loss: 0.0153. gp: 0.0512 \n","[2700/3000] D(real): -0.05. D(fake): -0.45. rec_loss: 0.0151. gp: 0.1200 \n","[2800/3000] D(real): -0.06. D(fake): -0.49. rec_loss: 0.0150. gp: 0.3899 \n","[2900/3000] D(real): -0.06. D(fake): -0.44. rec_loss: 0.0150. gp: 0.1347 \n","Total time in scale 14: 261[sec] (0.09[sec]/epoch on avg.). D(real): -0.061074, D(fake): -0.443364, rec_loss: 0.0150. gp: 0.1347\n","****************************** Finished working on scale 14 ******************************\n","Signal in scale 13 has 10500 samples, sample rate is 500[Hz].\n","Total receptive field is 4082[msec] (19.4% of input).\n","[0/3000] D(real): -0.48. D(fake): -0.51. rec_loss: 0.0327. gp: 0.1614 \n","[100/3000] D(real): -0.07. D(fake): -0.17. rec_loss: 0.0272. gp: 0.0512 \n","[200/3000] D(real): -0.13. D(fake): -0.26. rec_loss: 0.0235. gp: 0.0396 \n","[300/3000] D(real): -0.15. D(fake): -0.33. rec_loss: 0.0233. gp: 0.0852 \n","[400/3000] D(real): -0.26. D(fake): -0.35. rec_loss: 0.0244. gp: 0.0105 \n","[500/3000] D(real): -0.08. D(fake): -0.31. rec_loss: 0.0223. gp: 0.0247 \n","[600/3000] D(real): -0.13. D(fake): -0.20. rec_loss: 0.0188. gp: 0.0259 \n","[700/3000] D(real): -0.04. D(fake): -0.26. rec_loss: 0.0190. gp: 0.1982 \n","[800/3000] D(real): -0.14. D(fake): -0.28. rec_loss: 0.0165. gp: 0.0512 \n","[900/3000] D(real): -0.10. D(fake): -0.26. rec_loss: 0.0157. gp: 0.0531 \n","[1000/3000] D(real): -0.10. D(fake): -0.15. rec_loss: 0.0146. gp: 0.0057 \n","[1100/3000] D(real): -0.03. D(fake): -0.13. rec_loss: 0.0143. gp: 0.0199 \n","[1200/3000] D(real): -0.02. D(fake): -0.11. rec_loss: 0.0135. gp: 0.0275 \n","[1300/3000] D(real): -0.00. D(fake): -0.13. rec_loss: 0.0138. gp: 0.0613 \n","[1400/3000] D(real): 0.02. D(fake): -0.13. rec_loss: 0.0129. gp: 0.0161 \n","[1500/3000] D(real): 0.11. D(fake): -0.08. rec_loss: 0.0130. gp: 0.0240 \n","[1600/3000] D(real): 0.16. D(fake): -0.07. rec_loss: 0.0130. gp: 0.1479 \n","[1700/3000] D(real): 0.06. D(fake): -0.09. rec_loss: 0.0112. gp: 0.0382 \n","[1800/3000] D(real): -0.05. D(fake): -0.14. rec_loss: 0.0116. gp: 0.0747 \n","[1900/3000] D(real): 0.08. D(fake): 0.03. rec_loss: 0.0112. gp: 0.0055 \n","[2000/3000] D(real): 0.24. D(fake): 0.15. rec_loss: 0.0103. gp: 0.0273 \n","[2100/3000] D(real): 0.25. D(fake): 0.13. rec_loss: 0.0096. gp: 0.0193 \n","[2200/3000] D(real): 0.25. D(fake): 0.13. rec_loss: 0.0094. gp: 0.0287 \n","[2300/3000] D(real): 0.27. D(fake): 0.11. rec_loss: 0.0092. gp: 0.1437 \n","[2400/3000] D(real): 0.27. D(fake): 0.16. rec_loss: 0.0093. gp: 0.0239 \n","[2500/3000] D(real): 0.27. D(fake): 0.14. rec_loss: 0.0092. gp: 0.0909 \n","[2600/3000] D(real): 0.28. D(fake): 0.14. rec_loss: 0.0091. gp: 0.0750 \n","[2700/3000] D(real): 0.30. D(fake): 0.18. rec_loss: 0.0091. gp: 0.0651 \n","[2800/3000] D(real): 0.31. D(fake): 0.19. rec_loss: 0.0090. gp: 0.0264 \n","[2900/3000] D(real): 0.32. D(fake): 0.17. rec_loss: 0.0089. gp: 0.1171 \n","Total time in scale 13: 331[sec] (0.11[sec]/epoch on avg.). D(real): 0.323032, D(fake): 0.168450, rec_loss: 0.0089. gp: 0.1171\n","****************************** Finished working on scale 13 ******************************\n","Signal in scale 12 has 13440 samples, sample rate is 640[Hz].\n","Total receptive field is 3189[msec] (15.2% of input).\n","[0/3000] D(real): 0.11. D(fake): 0.08. rec_loss: 0.0394. gp: 0.1759 \n","[100/3000] D(real): 0.17. D(fake): 0.11. rec_loss: 0.0190. gp: 0.0326 \n","[200/3000] D(real): 0.20. D(fake): 0.13. rec_loss: 0.0216. gp: 0.0304 \n","[300/3000] D(real): 0.20. D(fake): 0.12. rec_loss: 0.0167. gp: 0.0593 \n","[400/3000] D(real): 0.23. D(fake): 0.13. rec_loss: 0.0142. gp: 0.0295 \n","[500/3000] D(real): 0.30. D(fake): 0.13. rec_loss: 0.0157. gp: 0.0827 \n","[600/3000] D(real): 0.30. D(fake): 0.19. rec_loss: 0.0135. gp: 0.0125 \n","[700/3000] D(real): 0.26. D(fake): 0.22. rec_loss: 0.0126. gp: 0.0220 \n","[800/3000] D(real): 0.27. D(fake): 0.17. rec_loss: 0.0122. gp: 0.0716 \n","[900/3000] D(real): 0.33. D(fake): 0.24. rec_loss: 0.0119. gp: 0.0186 \n","[1000/3000] D(real): 0.36. D(fake): 0.23. rec_loss: 0.0115. gp: 0.0721 \n","[1100/3000] D(real): 0.42. D(fake): 0.33. rec_loss: 0.0122. gp: 0.0857 \n","[1200/3000] D(real): 0.26. D(fake): 0.12. rec_loss: 0.0108. gp: 0.0662 \n","[1300/3000] D(real): 0.40. D(fake): 0.12. rec_loss: 0.0107. gp: 0.0564 \n","[1400/3000] D(real): 0.55. D(fake): 0.33. rec_loss: 0.0105. gp: 0.0964 \n","[1500/3000] D(real): 0.37. D(fake): 0.28. rec_loss: 0.0098. gp: 0.0361 \n","[1600/3000] D(real): 0.37. D(fake): 0.32. rec_loss: 0.0097. gp: 0.0098 \n","[1700/3000] D(real): 0.48. D(fake): 0.37. rec_loss: 0.0098. gp: 0.0664 \n","[1800/3000] D(real): 0.44. D(fake): 0.34. rec_loss: 0.0098. gp: 0.0493 \n","[1900/3000] D(real): 0.46. D(fake): 0.34. rec_loss: 0.0102. gp: 0.0626 \n","[2000/3000] D(real): 0.52. D(fake): 0.33. rec_loss: 0.0091. gp: 0.1696 \n","[2100/3000] D(real): 0.55. D(fake): 0.38. rec_loss: 0.0083. gp: 0.1054 \n","[2200/3000] D(real): 0.57. D(fake): 0.40. rec_loss: 0.0082. gp: 0.0379 \n","[2300/3000] D(real): 0.59. D(fake): 0.44. rec_loss: 0.0081. gp: 0.1195 \n","[2400/3000] D(real): 0.58. D(fake): 0.36. rec_loss: 0.0080. gp: 0.0618 \n","[2500/3000] D(real): 0.53. D(fake): 0.33. rec_loss: 0.0080. gp: 0.0774 \n","[2600/3000] D(real): 0.58. D(fake): 0.34. rec_loss: 0.0080. gp: 0.0379 \n","[2700/3000] D(real): 0.62. D(fake): 0.41. rec_loss: 0.0078. gp: 0.1020 \n","[2800/3000] D(real): 0.56. D(fake): 0.39. rec_loss: 0.0079. gp: 0.0543 \n","[2900/3000] D(real): 0.61. D(fake): 0.35. rec_loss: 0.0079. gp: 0.0175 \n","Total time in scale 12: 425[sec] (0.14[sec]/epoch on avg.). D(real): 0.608261, D(fake): 0.354142, rec_loss: 0.0079. gp: 0.0175\n","****************************** Finished working on scale 12 ******************************\n","Signal in scale 11 has 16800 samples, sample rate is 800[Hz].\n","Total receptive field is 2551[msec] (12.1% of input).\n","[0/3000] D(real): 0.29. D(fake): 0.31. rec_loss: 0.0432. gp: 0.0376 \n","[100/3000] D(real): 0.40. D(fake): 0.35. rec_loss: 0.0235. gp: 0.0257 \n","[200/3000] D(real): 0.34. D(fake): 0.27. rec_loss: 0.0204. gp: 0.0078 \n","[300/3000] D(real): 0.35. D(fake): 0.29. rec_loss: 0.0198. gp: 0.0642 \n","[400/3000] D(real): 0.41. D(fake): 0.34. rec_loss: 0.0189. gp: 0.0442 \n","[500/3000] D(real): 0.40. D(fake): 0.34. rec_loss: 0.0166. gp: 0.0446 \n","[600/3000] D(real): 0.51. D(fake): 0.34. rec_loss: 0.0154. gp: 0.0420 \n","[700/3000] D(real): 0.39. D(fake): 0.35. rec_loss: 0.0142. gp: 0.0090 \n","[800/3000] D(real): 0.33. D(fake): 0.28. rec_loss: 0.0149. gp: 0.0148 \n","[900/3000] D(real): 0.47. D(fake): 0.39. rec_loss: 0.0132. gp: 0.0147 \n","[1000/3000] D(real): 0.45. D(fake): 0.37. rec_loss: 0.0127. gp: 0.0246 \n","[1100/3000] D(real): 0.55. D(fake): 0.34. rec_loss: 0.0133. gp: 0.0837 \n","[1200/3000] D(real): 0.49. D(fake): 0.35. rec_loss: 0.0133. gp: 0.0310 \n","[1300/3000] D(real): 0.54. D(fake): 0.37. rec_loss: 0.0126. gp: 0.0227 \n","[1400/3000] D(real): 0.56. D(fake): 0.46. rec_loss: 0.0118. gp: 0.0670 \n","[1500/3000] D(real): 0.37. D(fake): 0.33. rec_loss: 0.0110. gp: 0.0061 \n","[1600/3000] D(real): 0.50. D(fake): 0.44. rec_loss: 0.0106. gp: 0.0094 \n","[1700/3000] D(real): 0.54. D(fake): 0.46. rec_loss: 0.0110. gp: 0.0598 \n","[1800/3000] D(real): 0.55. D(fake): 0.47. rec_loss: 0.0101. gp: 0.0353 \n","[1900/3000] D(real): 0.46. D(fake): 0.29. rec_loss: 0.0105. gp: 0.0267 \n","[2000/3000] D(real): 0.48. D(fake): 0.37. rec_loss: 0.0097. gp: 0.0264 \n","[2100/3000] D(real): 0.54. D(fake): 0.42. rec_loss: 0.0085. gp: 0.0660 \n","[2200/3000] D(real): 0.55. D(fake): 0.39. rec_loss: 0.0084. gp: 0.0425 \n","[2300/3000] D(real): 0.54. D(fake): 0.33. rec_loss: 0.0084. gp: 0.1168 \n","[2400/3000] D(real): 0.59. D(fake): 0.36. rec_loss: 0.0084. gp: 0.0192 \n","[2500/3000] D(real): 0.62. D(fake): 0.46. rec_loss: 0.0082. gp: 0.0485 \n","[2600/3000] D(real): 0.50. D(fake): 0.37. rec_loss: 0.0082. gp: 0.0491 \n","[2700/3000] D(real): 0.56. D(fake): 0.32. rec_loss: 0.0081. gp: 0.0263 \n","[2800/3000] D(real): 0.59. D(fake): 0.32. rec_loss: 0.0081. gp: 0.3256 \n","[2900/3000] D(real): 0.43. D(fake): 0.29. rec_loss: 0.0080. gp: 0.0230 \n","Total time in scale 11: 553[sec] (0.18[sec]/epoch on avg.). D(real): 0.427981, D(fake): 0.289168, rec_loss: 0.0080. gp: 0.0230\n","****************************** Finished working on scale 11 ******************************\n","Signal in scale 10 has 21000 samples, sample rate is 1000[Hz].\n","Total receptive field is 2041[msec] (9.7% of input).\n","[0/3000] D(real): 0.25. D(fake): 0.25. rec_loss: 0.0447. gp: 0.1261 \n","[100/3000] D(real): 0.40. D(fake): 0.38. rec_loss: 0.0325. gp: 0.0118 \n","[200/3000] D(real): 0.36. D(fake): 0.26. rec_loss: 0.0295. gp: 0.0411 \n","[300/3000] D(real): 0.37. D(fake): 0.33. rec_loss: 0.0236. gp: 0.0200 \n","[400/3000] D(real): 0.31. D(fake): 0.28. rec_loss: 0.0175. gp: 0.0086 \n","[500/3000] D(real): 0.28. D(fake): 0.21. rec_loss: 0.0162. gp: 0.0085 \n","[600/3000] D(real): 0.41. D(fake): 0.27. rec_loss: 0.0181. gp: 0.0750 \n","[700/3000] D(real): 0.32. D(fake): 0.24. rec_loss: 0.0154. gp: 0.0215 \n","[800/3000] D(real): 0.41. D(fake): 0.32. rec_loss: 0.0138. gp: 0.0402 \n","[900/3000] D(real): 0.13. D(fake): 0.12. rec_loss: 0.0133. gp: 0.0059 \n","[1000/3000] D(real): 0.12. D(fake): 0.10. rec_loss: 0.0117. gp: 0.0102 \n","[1100/3000] D(real): 0.15. D(fake): 0.13. rec_loss: 0.0106. gp: 0.0096 \n","[1200/3000] D(real): 0.19. D(fake): 0.17. rec_loss: 0.0114. gp: 0.0145 \n","[1300/3000] D(real): 0.16. D(fake): 0.11. rec_loss: 0.0096. gp: 0.0281 \n","[1400/3000] D(real): 0.15. D(fake): 0.14. rec_loss: 0.0092. gp: 0.0045 \n","[1500/3000] D(real): 0.18. D(fake): 0.15. rec_loss: 0.0092. gp: 0.0071 \n","[1600/3000] D(real): 0.21. D(fake): 0.19. rec_loss: 0.0093. gp: 0.0211 \n","[1700/3000] D(real): 0.23. D(fake): 0.16. rec_loss: 0.0094. gp: 0.0365 \n","[1800/3000] D(real): 0.27. D(fake): 0.22. rec_loss: 0.0087. gp: 0.0133 \n","[1900/3000] D(real): 0.33. D(fake): 0.30. rec_loss: 0.0096. gp: 0.0141 \n","[2000/3000] D(real): 0.28. D(fake): 0.20. rec_loss: 0.0089. gp: 0.0275 \n","[2100/3000] D(real): 0.29. D(fake): 0.23. rec_loss: 0.0081. gp: 0.0248 \n","[2200/3000] D(real): 0.27. D(fake): 0.19. rec_loss: 0.0081. gp: 0.0196 \n","[2300/3000] D(real): 0.22. D(fake): 0.11. rec_loss: 0.0080. gp: 0.0212 \n","[2400/3000] D(real): 0.28. D(fake): 0.22. rec_loss: 0.0080. gp: 0.0281 \n","[2500/3000] D(real): 0.26. D(fake): 0.10. rec_loss: 0.0079. gp: 0.0249 \n","[2600/3000] D(real): 0.24. D(fake): 0.09. rec_loss: 0.0080. gp: 0.0216 \n","[2700/3000] D(real): 0.28. D(fake): 0.12. rec_loss: 0.0079. gp: 0.0446 \n","[2800/3000] D(real): 0.20. D(fake): 0.10. rec_loss: 0.0078. gp: 0.0255 \n","[2900/3000] D(real): 0.23. D(fake): 0.14. rec_loss: 0.0079. gp: 0.0166 \n","Total time in scale 10: 689[sec] (0.23[sec]/epoch on avg.). D(real): 0.226730, D(fake): 0.135326, rec_loss: 0.0079. gp: 0.0166\n","****************************** Finished working on scale 10 ******************************\n","Signal in scale 9 has 26880 samples, sample rate is 1280[Hz].\n","Total receptive field is 1594[msec] (7.6% of input).\n","[0/3000] D(real): 0.05. D(fake): 0.05. rec_loss: 0.0494. gp: 0.1091 \n","[100/3000] D(real): 0.19. D(fake): 0.17. rec_loss: 0.0343. gp: 0.0120 \n","[200/3000] D(real): 0.23. D(fake): 0.16. rec_loss: 0.0334. gp: 0.0372 \n","[300/3000] D(real): 0.31. D(fake): 0.28. rec_loss: 0.0336. gp: 0.0174 \n","[400/3000] D(real): 0.31. D(fake): 0.27. rec_loss: 0.0330. gp: 0.0174 \n","[500/3000] D(real): 0.29. D(fake): 0.22. rec_loss: 0.0330. gp: 0.0073 \n","[600/3000] D(real): 0.33. D(fake): 0.28. rec_loss: 0.0319. gp: 0.0236 \n","[700/3000] D(real): 0.33. D(fake): 0.27. rec_loss: 0.0300. gp: 0.0129 \n","[800/3000] D(real): 0.38. D(fake): 0.26. rec_loss: 0.0190. gp: 0.0226 \n","[900/3000] D(real): 0.40. D(fake): 0.31. rec_loss: 0.0154. gp: 0.0477 \n","[1000/3000] D(real): 0.37. D(fake): 0.32. rec_loss: 0.0126. gp: 0.0133 \n","[1100/3000] D(real): 0.49. D(fake): 0.37. rec_loss: 0.0131. gp: 0.0225 \n","[1200/3000] D(real): 0.38. D(fake): 0.26. rec_loss: 0.0118. gp: 0.0133 \n","[1300/3000] D(real): 0.47. D(fake): 0.38. rec_loss: 0.0129. gp: 0.0448 \n","[1400/3000] D(real): 0.42. D(fake): 0.37. rec_loss: 0.0108. gp: 0.0059 \n","[1500/3000] D(real): 0.40. D(fake): 0.38. rec_loss: 0.0098. gp: 0.0075 \n","[1600/3000] D(real): 0.49. D(fake): 0.45. rec_loss: 0.0118. gp: 0.0292 \n","[1700/3000] D(real): 0.53. D(fake): 0.46. rec_loss: 0.0112. gp: 0.0255 \n","[1800/3000] D(real): 0.60. D(fake): 0.41. rec_loss: 0.0097. gp: 0.0414 \n","[1900/3000] D(real): 0.57. D(fake): 0.47. rec_loss: 0.0101. gp: 0.0259 \n","[2000/3000] D(real): 0.55. D(fake): 0.48. rec_loss: 0.0100. gp: 0.0328 \n","[2100/3000] D(real): 0.60. D(fake): 0.46. rec_loss: 0.0090. gp: 0.0268 \n","[2200/3000] D(real): 0.66. D(fake): 0.52. rec_loss: 0.0089. gp: 0.0265 \n","[2300/3000] D(real): 0.64. D(fake): 0.57. rec_loss: 0.0088. gp: 0.1264 \n","[2400/3000] D(real): 0.61. D(fake): 0.51. rec_loss: 0.0088. gp: 0.0147 \n","[2500/3000] D(real): 0.67. D(fake): 0.55. rec_loss: 0.0088. gp: 0.2359 \n","[2600/3000] D(real): 0.69. D(fake): 0.57. rec_loss: 0.0087. gp: 0.0227 \n","[2700/3000] D(real): 0.71. D(fake): 0.54. rec_loss: 0.0089. gp: 0.0458 \n","[2800/3000] D(real): 0.72. D(fake): 0.54. rec_loss: 0.0087. gp: 0.0535 \n","[2900/3000] D(real): 0.72. D(fake): 0.56. rec_loss: 0.0086. gp: 0.0886 \n","Total time in scale 9: 876[sec] (0.29[sec]/epoch on avg.). D(real): 0.715705, D(fake): 0.555959, rec_loss: 0.0086. gp: 0.0886\n","****************************** Finished working on scale 9 ******************************\n","Signal in scale 8 has 33600 samples, sample rate is 1600[Hz].\n","Total receptive field is 1275[msec] (6.1% of input).\n","[0/3000] D(real): 0.43. D(fake): 0.46. rec_loss: 0.0615. gp: 0.0750 \n","[100/3000] D(real): 0.42. D(fake): 0.33. rec_loss: 0.0381. gp: 0.0288 \n","[200/3000] D(real): 0.42. D(fake): 0.33. rec_loss: 0.0366. gp: 0.0410 \n","[300/3000] D(real): 0.39. D(fake): 0.31. rec_loss: 0.0299. gp: 0.0144 \n","[400/3000] D(real): 0.44. D(fake): 0.39. rec_loss: 0.0207. gp: 0.0180 \n","[500/3000] D(real): 0.43. D(fake): 0.41. rec_loss: 0.0167. gp: 0.0217 \n","[600/3000] D(real): 0.42. D(fake): 0.35. rec_loss: 0.0153. gp: 0.0067 \n","[700/3000] D(real): 0.42. D(fake): 0.39. rec_loss: 0.0137. gp: 0.0068 \n","[800/3000] D(real): 0.41. D(fake): 0.39. rec_loss: 0.0131. gp: 0.0081 \n","[900/3000] D(real): 0.40. D(fake): 0.39. rec_loss: 0.0119. gp: 0.0072 \n","[1000/3000] D(real): 0.41. D(fake): 0.37. rec_loss: 0.0122. gp: 0.0128 \n","[1100/3000] D(real): 0.42. D(fake): 0.38. rec_loss: 0.0114. gp: 0.0141 \n","[1200/3000] D(real): 0.43. D(fake): 0.40. rec_loss: 0.0115. gp: 0.0194 \n","[1300/3000] D(real): 0.51. D(fake): 0.40. rec_loss: 0.0107. gp: 0.0290 \n","[1400/3000] D(real): 0.46. D(fake): 0.38. rec_loss: 0.0104. gp: 0.0330 \n","[1500/3000] D(real): 0.40. D(fake): 0.38. rec_loss: 0.0106. gp: 0.0052 \n","[1600/3000] D(real): 0.49. D(fake): 0.41. rec_loss: 0.0120. gp: 0.0303 \n","[1700/3000] D(real): 0.44. D(fake): 0.39. rec_loss: 0.0105. gp: 0.0070 \n","[1800/3000] D(real): 0.50. D(fake): 0.34. rec_loss: 0.0108. gp: 0.0217 \n","[1900/3000] D(real): 0.47. D(fake): 0.34. rec_loss: 0.0104. gp: 0.0246 \n","[2000/3000] D(real): 0.40. D(fake): 0.32. rec_loss: 0.0105. gp: 0.0192 \n","[2100/3000] D(real): 0.44. D(fake): 0.35. rec_loss: 0.0087. gp: 0.0703 \n","[2200/3000] D(real): 0.47. D(fake): 0.38. rec_loss: 0.0087. gp: 0.0228 \n","[2300/3000] D(real): 0.43. D(fake): 0.32. rec_loss: 0.0091. gp: 0.0205 \n","[2400/3000] D(real): 0.49. D(fake): 0.33. rec_loss: 0.0086. gp: 0.0247 \n","[2500/3000] D(real): 0.48. D(fake): 0.28. rec_loss: 0.0085. gp: 0.2444 \n","[2600/3000] D(real): 0.47. D(fake): 0.31. rec_loss: 0.0085. gp: 0.0212 \n","[2700/3000] D(real): 0.46. D(fake): 0.29. rec_loss: 0.0085. gp: 0.0330 \n","[2800/3000] D(real): 0.48. D(fake): 0.31. rec_loss: 0.0086. gp: 0.0215 \n","[2900/3000] D(real): 0.46. D(fake): 0.35. rec_loss: 0.0085. gp: 0.0450 \n","Total time in scale 8: 1094[sec] (0.36[sec]/epoch on avg.). D(real): 0.459174, D(fake): 0.347260, rec_loss: 0.0085. gp: 0.0450\n","****************************** Finished working on scale 8 ******************************\n","Signal in scale 7 has 42000 samples, sample rate is 2000[Hz].\n","Total receptive field is 1020[msec] (4.9% of input).\n","[0/3000] D(real): 0.24. D(fake): 0.26. rec_loss: 0.0599. gp: 0.0674 \n","[100/3000] D(real): 0.29. D(fake): 0.27. rec_loss: 0.0394. gp: 0.0174 \n","[200/3000] D(real): 0.27. D(fake): 0.20. rec_loss: 0.0397. gp: 0.0269 \n","[300/3000] D(real): 0.22. D(fake): 0.18. rec_loss: 0.0388. gp: 0.0157 \n","[400/3000] D(real): 0.25. D(fake): 0.14. rec_loss: 0.0379. gp: 0.0363 \n","[500/3000] D(real): 0.12. D(fake): 0.09. rec_loss: 0.0353. gp: 0.0159 \n","[600/3000] D(real): 0.17. D(fake): 0.16. rec_loss: 0.0182. gp: 0.0132 \n","[700/3000] D(real): 0.17. D(fake): 0.12. rec_loss: 0.0223. gp: 0.0106 \n","[800/3000] D(real): 0.16. D(fake): 0.11. rec_loss: 0.0147. gp: 0.0401 \n","[900/3000] D(real): 0.16. D(fake): 0.11. rec_loss: 0.0151. gp: 0.0215 \n","[1000/3000] D(real): 0.09. D(fake): 0.09. rec_loss: 0.0124. gp: 0.0070 \n","[1100/3000] D(real): 0.16. D(fake): 0.12. rec_loss: 0.0118. gp: 0.0273 \n","[1200/3000] D(real): 0.14. D(fake): 0.09. rec_loss: 0.0138. gp: 0.0150 \n","[1300/3000] D(real): 0.16. D(fake): 0.11. rec_loss: 0.0127. gp: 0.0347 \n","[1400/3000] D(real): 0.16. D(fake): 0.08. rec_loss: 0.0113. gp: 0.0592 \n","[1500/3000] D(real): 0.17. D(fake): 0.10. rec_loss: 0.0116. gp: 0.0359 \n","[1600/3000] D(real): 0.13. D(fake): 0.05. rec_loss: 0.0136. gp: 0.0361 \n","[1700/3000] D(real): 0.11. D(fake): 0.03. rec_loss: 0.0107. gp: 0.0198 \n","[1800/3000] D(real): 0.03. D(fake): 0.02. rec_loss: 0.0100. gp: 0.0052 \n","[1900/3000] D(real): 0.05. D(fake): 0.03. rec_loss: 0.0111. gp: 0.0089 \n","[2000/3000] D(real): 0.06. D(fake): 0.02. rec_loss: 0.0097. gp: 0.0054 \n","[2100/3000] D(real): 0.07. D(fake): 0.04. rec_loss: 0.0091. gp: 0.0247 \n","[2200/3000] D(real): 0.06. D(fake): 0.04. rec_loss: 0.0091. gp: 0.0142 \n","[2300/3000] D(real): 0.06. D(fake): 0.03. rec_loss: 0.0090. gp: 0.0233 \n","[2400/3000] D(real): 0.07. D(fake): -0.00. rec_loss: 0.0090. gp: 0.0130 \n","[2500/3000] D(real): 0.07. D(fake): 0.02. rec_loss: 0.0091. gp: 0.0095 \n","[2600/3000] D(real): 0.07. D(fake): 0.04. rec_loss: 0.0089. gp: 0.0243 \n","[2700/3000] D(real): 0.07. D(fake): -0.01. rec_loss: 0.0089. gp: 0.0261 \n","[2800/3000] D(real): 0.07. D(fake): 0.02. rec_loss: 0.0090. gp: 0.0162 \n","[2900/3000] D(real): 0.07. D(fake): 0.03. rec_loss: 0.0089. gp: 0.0142 \n","Total time in scale 7: 1343[sec] (0.45[sec]/epoch on avg.). D(real): 0.072207, D(fake): 0.028923, rec_loss: 0.0089. gp: 0.0142\n","****************************** Finished working on scale 7 ******************************\n","Signal in scale 6 has 52500 samples, sample rate is 2500[Hz].\n","Total receptive field is 816[msec] (3.9% of input).\n","[0/3000] D(real): -0.04. D(fake): -0.04. rec_loss: 0.0651. gp: 0.0101 \n","[100/3000] D(real): 0.02. D(fake): -0.03. rec_loss: 0.0240. gp: 0.0354 \n","[200/3000] D(real): -0.02. D(fake): -0.06. rec_loss: 0.0164. gp: 0.0230 \n","[300/3000] D(real): 0.00. D(fake): -0.03. rec_loss: 0.0149. gp: 0.0074 \n","[400/3000] D(real): 0.03. D(fake): 0.01. rec_loss: 0.0136. gp: 0.0237 \n","[500/3000] D(real): -0.02. D(fake): -0.04. rec_loss: 0.0136. gp: 0.0043 \n","[600/3000] D(real): 0.03. D(fake): -0.02. rec_loss: 0.0131. gp: 0.0190 \n","[700/3000] D(real): 0.03. D(fake): 0.00. rec_loss: 0.0130. gp: 0.0208 \n","[800/3000] D(real): 0.04. D(fake): 0.02. rec_loss: 0.0125. gp: 0.0135 \n","[900/3000] D(real): 0.04. D(fake): 0.02. rec_loss: 0.0122. gp: 0.0141 \n","[1000/3000] D(real): 0.06. D(fake): -0.01. rec_loss: 0.0122. gp: 0.0369 \n","[1100/3000] D(real): 0.08. D(fake): 0.05. rec_loss: 0.0116. gp: 0.0148 \n","[1200/3000] D(real): 0.02. D(fake): -0.02. rec_loss: 0.0111. gp: 0.0128 \n","[1300/3000] D(real): -0.00. D(fake): -0.02. rec_loss: 0.0109. gp: 0.0050 \n","[1400/3000] D(real): 0.03. D(fake): 0.01. rec_loss: 0.0116. gp: 0.0074 \n","[1500/3000] D(real): 0.03. D(fake): -0.00. rec_loss: 0.0106. gp: 0.0144 \n","[1600/3000] D(real): 0.04. D(fake): 0.03. rec_loss: 0.0114. gp: 0.0128 \n","[1700/3000] D(real): 0.01. D(fake): -0.00. rec_loss: 0.0103. gp: 0.0104 \n","[1800/3000] D(real): 0.06. D(fake): -0.00. rec_loss: 0.0104. gp: 0.0238 \n","[1900/3000] D(real): 0.08. D(fake): -0.01. rec_loss: 0.0108. gp: 0.0379 \n","[2000/3000] D(real): 0.08. D(fake): 0.01. rec_loss: 0.0104. gp: 0.0223 \n","[2100/3000] D(real): 0.06. D(fake): 0.01. rec_loss: 0.0098. gp: 0.0390 \n","[2200/3000] D(real): 0.07. D(fake): -0.01. rec_loss: 0.0098. gp: 0.0200 \n","[2300/3000] D(real): 0.08. D(fake): 0.03. rec_loss: 0.0099. gp: 0.0138 \n","[2400/3000] D(real): 0.06. D(fake): 0.02. rec_loss: 0.0098. gp: 0.0397 \n","[2500/3000] D(real): 0.07. D(fake): -0.01. rec_loss: 0.0098. gp: 0.0701 \n","[2600/3000] D(real): 0.08. D(fake): 0.03. rec_loss: 0.0097. gp: 0.0100 \n","[2700/3000] D(real): 0.06. D(fake): -0.01. rec_loss: 0.0097. gp: 0.0253 \n","[2800/3000] D(real): 0.10. D(fake): 0.02. rec_loss: 0.0097. gp: 0.0160 \n","[2900/3000] D(real): 0.07. D(fake): -0.02. rec_loss: 0.0097. gp: 0.0149 \n","Total time in scale 6: 1685[sec] (0.56[sec]/epoch on avg.). D(real): 0.071292, D(fake): -0.020614, rec_loss: 0.0097. gp: 0.0149\n","****************************** Finished working on scale 6 ******************************\n","Signal in scale 5 has 84000 samples, sample rate is 4000[Hz].\n","Total receptive field is 510[msec] (2.4% of input).\n","[0/3000] D(real): -0.04. D(fake): -0.04. rec_loss: 0.0750. gp: 0.0159 \n","[100/3000] D(real): 0.08. D(fake): 0.05. rec_loss: 0.0409. gp: 0.0100 \n","[200/3000] D(real): 0.12. D(fake): 0.10. rec_loss: 0.0263. gp: 0.0210 \n","[300/3000] D(real): 0.09. D(fake): 0.08. rec_loss: 0.0203. gp: 0.0116 \n","[400/3000] D(real): 0.10. D(fake): 0.08. rec_loss: 0.0258. gp: 0.0106 \n","[500/3000] D(real): 0.09. D(fake): 0.07. rec_loss: 0.0247. gp: 0.0109 \n","[600/3000] D(real): 0.07. D(fake): 0.05. rec_loss: 0.0181. gp: 0.0127 \n","[700/3000] D(real): 0.02. D(fake): 0.01. rec_loss: 0.0168. gp: 0.0062 \n","[800/3000] D(real): 0.02. D(fake): 0.01. rec_loss: 0.0166. gp: 0.0056 \n","[900/3000] D(real): 0.04. D(fake): 0.03. rec_loss: 0.0157. gp: 0.0116 \n","[1000/3000] D(real): 0.03. D(fake): 0.02. rec_loss: 0.0154. gp: 0.0046 \n","[1100/3000] D(real): 0.06. D(fake): 0.03. rec_loss: 0.0153. gp: 0.0057 \n","[1200/3000] D(real): 0.06. D(fake): 0.05. rec_loss: 0.0154. gp: 0.0125 \n","[1300/3000] D(real): 0.10. D(fake): 0.08. rec_loss: 0.0211. gp: 0.0156 \n","[1400/3000] D(real): 0.05. D(fake): 0.03. rec_loss: 0.0146. gp: 0.0045 \n","[1500/3000] D(real): 0.11. D(fake): 0.09. rec_loss: 0.0145. gp: 0.0437 \n","[1600/3000] D(real): 0.14. D(fake): 0.11. rec_loss: 0.0142. gp: 0.0090 \n","[1700/3000] D(real): 0.11. D(fake): 0.08. rec_loss: 0.0148. gp: 0.0070 \n","[1800/3000] D(real): 0.08. D(fake): 0.07. rec_loss: 0.0130. gp: 0.0050 \n","[1900/3000] D(real): 0.11. D(fake): 0.07. rec_loss: 0.0166. gp: 0.0179 \n","[2000/3000] D(real): 0.07. D(fake): 0.06. rec_loss: 0.0133. gp: 0.0045 \n","[2100/3000] D(real): 0.11. D(fake): 0.08. rec_loss: 0.0123. gp: 0.0059 \n","[2200/3000] D(real): 0.11. D(fake): 0.08. rec_loss: 0.0123. gp: 0.0061 \n","[2300/3000] D(real): 0.11. D(fake): 0.06. rec_loss: 0.0122. gp: 0.0063 \n","[2400/3000] D(real): 0.13. D(fake): 0.09. rec_loss: 0.0122. gp: 0.0127 \n","[2500/3000] D(real): 0.13. D(fake): 0.05. rec_loss: 0.0121. gp: 0.0084 \n","[2600/3000] D(real): 0.12. D(fake): 0.07. rec_loss: 0.0121. gp: 0.0069 \n","[2700/3000] D(real): 0.12. D(fake): 0.07. rec_loss: 0.0120. gp: 0.0076 \n","[2800/3000] D(real): 0.10. D(fake): 0.05. rec_loss: 0.0121. gp: 0.0070 \n","[2900/3000] D(real): 0.14. D(fake): 0.09. rec_loss: 0.0121. gp: 0.0141 \n","Total time in scale 5: 2514[sec] (0.84[sec]/epoch on avg.). D(real): 0.142123, D(fake): 0.085877, rec_loss: 0.0121. gp: 0.0141\n","****************************** Finished working on scale 5 ******************************\n","Signal in scale 4 has 168000 samples, sample rate is 8000[Hz].\n","Total receptive field is 255[msec] (1.2% of input).\n","[0/3000] D(real): 0.03. D(fake): 0.03. rec_loss: 0.1044. gp: 0.0164 \n","[100/3000] D(real): 0.01. D(fake): 0.00. rec_loss: 0.0505. gp: 0.0050 \n","[200/3000] D(real): 0.02. D(fake): 0.00. rec_loss: 0.0470. gp: 0.0052 \n","[300/3000] D(real): 0.03. D(fake): 0.01. rec_loss: 0.0516. gp: 0.0098 \n","[400/3000] D(real): 0.02. D(fake): 0.01. rec_loss: 0.0414. gp: 0.0115 \n","[500/3000] D(real): 0.02. D(fake): 0.02. rec_loss: 0.0378. gp: 0.0046 \n","[600/3000] D(real): -0.02. D(fake): -0.05. rec_loss: 0.0358. gp: 0.0071 \n","[700/3000] D(real): -0.02. D(fake): -0.04. rec_loss: 0.0351. gp: 0.0187 \n","[800/3000] D(real): -0.02. D(fake): -0.05. rec_loss: 0.0327. gp: 0.0060 \n","[900/3000] D(real): -0.08. D(fake): -0.09. rec_loss: 0.0318. gp: 0.0057 \n","[1000/3000] D(real): -0.07. D(fake): -0.07. rec_loss: 0.0300. gp: 0.0080 \n","[1100/3000] D(real): -0.03. D(fake): -0.05. rec_loss: 0.0265. gp: 0.0053 \n","[1200/3000] D(real): -0.08. D(fake): -0.09. rec_loss: 0.0246. gp: 0.0051 \n","[1300/3000] D(real): -0.06. D(fake): -0.07. rec_loss: 0.0245. gp: 0.0047 \n","[1400/3000] D(real): -0.04. D(fake): -0.05. rec_loss: 0.0230. gp: 0.0066 \n","[1500/3000] D(real): -0.11. D(fake): -0.12. rec_loss: 0.0222. gp: 0.0045 \n","[1600/3000] D(real): -0.07. D(fake): -0.07. rec_loss: 0.0224. gp: 0.0043 \n","[1700/3000] D(real): -0.03. D(fake): -0.06. rec_loss: 0.0216. gp: 0.0112 \n","[1800/3000] D(real): -0.04. D(fake): -0.06. rec_loss: 0.0210. gp: 0.0199 \n","[1900/3000] D(real): -0.02. D(fake): -0.06. rec_loss: 0.0223. gp: 0.0070 \n","[2000/3000] D(real): -0.01. D(fake): -0.04. rec_loss: 0.0221. gp: 0.0567 \n","[2100/3000] D(real): -0.03. D(fake): -0.07. rec_loss: 0.0197. gp: 0.0050 \n","[2200/3000] D(real): -0.03. D(fake): -0.08. rec_loss: 0.0196. gp: 0.0059 \n","[2300/3000] D(real): -0.01. D(fake): -0.04. rec_loss: 0.0196. gp: 0.0272 \n","[2400/3000] D(real): -0.03. D(fake): -0.06. rec_loss: 0.0199. gp: 0.0060 \n","[2500/3000] D(real): -0.01. D(fake): -0.07. rec_loss: 0.0195. gp: 0.0060 \n","[2600/3000] D(real): -0.04. D(fake): -0.08. rec_loss: 0.0195. gp: 0.0066 \n","[2700/3000] D(real): -0.05. D(fake): -0.09. rec_loss: 0.0199. gp: 0.0048 \n","[2800/3000] D(real): -0.01. D(fake): -0.06. rec_loss: 0.0196. gp: 0.0135 \n","[2900/3000] D(real): -0.03. D(fake): -0.10. rec_loss: 0.0194. gp: 0.0054 \n","Total time in scale 4: 4703[sec] (1.57[sec]/epoch on avg.). D(real): -0.028862, D(fake): -0.096616, rec_loss: 0.0194. gp: 0.0054\n","****************************** Finished working on scale 4 ******************************\n","Signal in scale 3 has 210000 samples, sample rate is 10000[Hz].\n","Total receptive field is 204[msec] (1.0% of input).\n","[0/3000] D(real): -0.15. D(fake): -0.13. rec_loss: 0.1074. gp: 0.0081 \n","[100/3000] D(real): -0.15. D(fake): -0.16. rec_loss: 0.0412. gp: 0.0054 \n","[200/3000] D(real): -0.18. D(fake): -0.20. rec_loss: 0.0345. gp: 0.0062 \n","[300/3000] D(real): -0.19. D(fake): -0.21. rec_loss: 0.0363. gp: 0.0059 \n","[400/3000] D(real): -0.19. D(fake): -0.19. rec_loss: 0.0295. gp: 0.0045 \n","[500/3000] D(real): -0.17. D(fake): -0.17. rec_loss: 0.0299. gp: 0.0043 \n","[600/3000] D(real): -0.17. D(fake): -0.17. rec_loss: 0.0280. gp: 0.0053 \n","[700/3000] D(real): -0.15. D(fake): -0.16. rec_loss: 0.0272. gp: 0.0048 \n","[800/3000] D(real): -0.19. D(fake): -0.20. rec_loss: 0.0261. gp: 0.0052 \n","[900/3000] D(real): -0.18. D(fake): -0.18. rec_loss: 0.0267. gp: 0.0045 \n","[1000/3000] D(real): -0.14. D(fake): -0.15. rec_loss: 0.0244. gp: 0.0054 \n","[1100/3000] D(real): -0.13. D(fake): -0.16. rec_loss: 0.0247. gp: 0.0065 \n","[1200/3000] D(real): -0.16. D(fake): -0.16. rec_loss: 0.0230. gp: 0.0044 \n","[1300/3000] D(real): -0.08. D(fake): -0.09. rec_loss: 0.0226. gp: 0.0054 \n","[1400/3000] D(real): -0.08. D(fake): -0.10. rec_loss: 0.0221. gp: 0.0077 \n","[1500/3000] D(real): -0.09. D(fake): -0.10. rec_loss: 0.0217. gp: 0.0072 \n","[1600/3000] D(real): -0.09. D(fake): -0.09. rec_loss: 0.0220. gp: 0.0051 \n","[1700/3000] D(real): -0.07. D(fake): -0.08. rec_loss: 0.0216. gp: 0.0062 \n","[1800/3000] D(real): -0.06. D(fake): -0.07. rec_loss: 0.0206. gp: 0.0059 \n","[1900/3000] D(real): -0.05. D(fake): -0.07. rec_loss: 0.0220. gp: 0.0152 \n","[2000/3000] D(real): -0.07. D(fake): -0.08. rec_loss: 0.0199. gp: 0.0064 \n","[2100/3000] D(real): -0.07. D(fake): -0.08. rec_loss: 0.0193. gp: 0.0078 \n","[2200/3000] D(real): -0.08. D(fake): -0.08. rec_loss: 0.0192. gp: 0.0074 \n","[2300/3000] D(real): -0.09. D(fake): -0.09. rec_loss: 0.0193. gp: 0.0058 \n","[2400/3000] D(real): -0.09. D(fake): -0.12. rec_loss: 0.0191. gp: 0.0058 \n","[2500/3000] D(real): -0.10. D(fake): -0.13. rec_loss: 0.0192. gp: 0.0053 \n","[2600/3000] D(real): -0.11. D(fake): -0.14. rec_loss: 0.0191. gp: 0.0048 \n","[2700/3000] D(real): -0.10. D(fake): -0.13. rec_loss: 0.0192. gp: 0.0062 \n","[2800/3000] D(real): -0.11. D(fake): -0.15. rec_loss: 0.0192. gp: 0.0199 \n","[2900/3000] D(real): -0.10. D(fake): -0.16. rec_loss: 0.0192. gp: 0.0666 \n","Total time in scale 3: 5942[sec] (1.98[sec]/epoch on avg.). D(real): -0.104152, D(fake): -0.163459, rec_loss: 0.0192. gp: 0.0666\n","****************************** Finished working on scale 3 ******************************\n","Signal in scale 2 has 252000 samples, sample rate is 12000[Hz].\n","Total receptive field is 170[msec] (0.8% of input).\n","[0/3000] D(real): -0.19. D(fake): -0.18. rec_loss: 0.1160. gp: 0.0087 \n","[100/3000] D(real): -0.17. D(fake): -0.18. rec_loss: 0.0367. gp: 0.0047 \n","[200/3000] D(real): -0.15. D(fake): -0.16. rec_loss: 0.0312. gp: 0.0061 \n","[300/3000] D(real): -0.17. D(fake): -0.17. rec_loss: 0.0283. gp: 0.0044 \n","[400/3000] D(real): -0.10. D(fake): -0.11. rec_loss: 0.0283. gp: 0.0068 \n","[500/3000] D(real): -0.10. D(fake): -0.10. rec_loss: 0.0266. gp: 0.0098 \n","[600/3000] D(real): -0.09. D(fake): -0.11. rec_loss: 0.0250. gp: 0.0105 \n","[700/3000] D(real): -0.06. D(fake): -0.07. rec_loss: 0.0251. gp: 0.0524 \n","[800/3000] D(real): -0.10. D(fake): -0.11. rec_loss: 0.0231. gp: 0.0053 \n","[900/3000] D(real): -0.04. D(fake): -0.07. rec_loss: 0.0252. gp: 0.0056 \n","[1000/3000] D(real): -0.10. D(fake): -0.11. rec_loss: 0.0225. gp: 0.0042 \n","[1100/3000] D(real): -0.10. D(fake): -0.11. rec_loss: 0.0227. gp: 0.0066 \n","[1200/3000] D(real): -0.06. D(fake): -0.10. rec_loss: 0.0241. gp: 0.0234 \n","[1300/3000] D(real): -0.09. D(fake): -0.11. rec_loss: 0.0211. gp: 0.0124 \n","[1400/3000] D(real): -0.06. D(fake): -0.11. rec_loss: 0.0223. gp: 0.0225 \n","[1500/3000] D(real): -0.08. D(fake): -0.10. rec_loss: 0.0219. gp: 0.0222 \n","[1600/3000] D(real): -0.07. D(fake): -0.10. rec_loss: 0.0207. gp: 0.0178 \n","[1700/3000] D(real): -0.07. D(fake): -0.10. rec_loss: 0.0207. gp: 0.0153 \n","[1800/3000] D(real): -0.07. D(fake): -0.10. rec_loss: 0.0239. gp: 0.0058 \n","[1900/3000] D(real): -0.11. D(fake): -0.12. rec_loss: 0.0192. gp: 0.0040 \n","[2000/3000] D(real): -0.04. D(fake): -0.08. rec_loss: 0.0224. gp: 0.0132 \n","[2100/3000] D(real): -0.06. D(fake): -0.13. rec_loss: 0.0185. gp: 0.0064 \n","[2200/3000] D(real): -0.06. D(fake): -0.10. rec_loss: 0.0185. gp: 0.0129 \n","[2300/3000] D(real): -0.07. D(fake): -0.12. rec_loss: 0.0184. gp: 0.0105 \n","[2400/3000] D(real): -0.06. D(fake): -0.13. rec_loss: 0.0184. gp: 0.0709 \n","[2500/3000] D(real): -0.10. D(fake): -0.14. rec_loss: 0.0186. gp: 0.0052 \n","[2600/3000] D(real): -0.07. D(fake): -0.12. rec_loss: 0.0186. gp: 0.0059 \n","[2700/3000] D(real): -0.07. D(fake): -0.12. rec_loss: 0.0185. gp: 0.0091 \n","[2800/3000] D(real): -0.07. D(fake): -0.10. rec_loss: 0.0183. gp: 0.0051 \n","[2900/3000] D(real): -0.07. D(fake): -0.12. rec_loss: 0.0183. gp: 0.0071 \n","Total time in scale 2: 7217[sec] (2.41[sec]/epoch on avg.). D(real): -0.072074, D(fake): -0.119942, rec_loss: 0.0183. gp: 0.0071\n","****************************** Finished working on scale 2 ******************************\n","Signal in scale 1 has 302400 samples, sample rate is 14400[Hz].\n","Total receptive field is 141[msec] (0.7% of input).\n","[0/3000] D(real): -0.16. D(fake): -0.15. rec_loss: 0.1009. gp: 0.0079 \n","[100/3000] D(real): -0.09. D(fake): -0.11. rec_loss: 0.0473. gp: 0.0909 \n","[200/3000] D(real): -0.20. D(fake): -0.20. rec_loss: 0.0314. gp: 0.0040 \n","[300/3000] D(real): -0.18. D(fake): -0.19. rec_loss: 0.0293. gp: 0.0045 \n","[400/3000] D(real): -0.14. D(fake): -0.16. rec_loss: 0.0295. gp: 0.0048 \n","[500/3000] D(real): -0.13. D(fake): -0.15. rec_loss: 0.0274. gp: 0.0055 \n","[600/3000] D(real): -0.14. D(fake): -0.17. rec_loss: 0.0296. gp: 0.0085 \n","[700/3000] D(real): -0.14. D(fake): -0.14. rec_loss: 0.0254. gp: 0.0049 \n","[800/3000] D(real): -0.17. D(fake): -0.18. rec_loss: 0.0249. gp: 0.0067 \n","[900/3000] D(real): -0.13. D(fake): -0.15. rec_loss: 0.0246. gp: 0.0066 \n","[1000/3000] D(real): -0.10. D(fake): -0.16. rec_loss: 0.0273. gp: 0.0062 \n","[1100/3000] D(real): -0.14. D(fake): -0.15. rec_loss: 0.0230. gp: 0.0064 \n","[1200/3000] D(real): -0.14. D(fake): -0.18. rec_loss: 0.0246. gp: 0.0083 \n","[1300/3000] D(real): -0.16. D(fake): -0.19. rec_loss: 0.0265. gp: 0.0045 \n","[1400/3000] D(real): -0.18. D(fake): -0.19. rec_loss: 0.0233. gp: 0.0061 \n","[1500/3000] D(real): -0.13. D(fake): -0.16. rec_loss: 0.0237. gp: 0.0329 \n","[1600/3000] D(real): -0.16. D(fake): -0.18. rec_loss: 0.0229. gp: 0.0063 \n","[1700/3000] D(real): -0.18. D(fake): -0.18. rec_loss: 0.0213. gp: 0.0052 \n","[1800/3000] D(real): -0.18. D(fake): -0.21. rec_loss: 0.0222. gp: 0.0059 \n","[1900/3000] D(real): -0.21. D(fake): -0.24. rec_loss: 0.0216. gp: 0.0049 \n","[2000/3000] D(real): -0.13. D(fake): -0.17. rec_loss: 0.0221. gp: 0.0256 \n","[2100/3000] D(real): -0.16. D(fake): -0.18. rec_loss: 0.0199. gp: 0.0056 \n","[2200/3000] D(real): -0.18. D(fake): -0.23. rec_loss: 0.0200. gp: 0.0051 \n","[2300/3000] D(real): -0.20. D(fake): -0.23. rec_loss: 0.0198. gp: 0.0157 \n","[2400/3000] D(real): -0.19. D(fake): -0.24. rec_loss: 0.0198. gp: 0.0395 \n","[2500/3000] D(real): -0.16. D(fake): -0.20. rec_loss: 0.0198. gp: 0.0127 \n","[2600/3000] D(real): -0.18. D(fake): -0.22. rec_loss: 0.0199. gp: 0.0053 \n","[2700/3000] D(real): -0.16. D(fake): -0.22. rec_loss: 0.0196. gp: 0.0232 \n","[2800/3000] D(real): -0.18. D(fake): -0.21. rec_loss: 0.0208. gp: 0.0078 \n","[2900/3000] D(real): -0.17. D(fake): -0.21. rec_loss: 0.0201. gp: 0.0100 \n","Total time in scale 1: 8778[sec] (2.93[sec]/epoch on avg.). D(real): -0.172254, D(fake): -0.210972, rec_loss: 0.0201. gp: 0.0100\n","****************************** Finished working on scale 1 ******************************\n","Signal in scale 0 has 336000 samples, sample rate is 16000[Hz].\n","Total receptive field is 127[msec] (0.6% of input).\n","[0/3000] D(real): -0.24. D(fake): -0.27. rec_loss: 0.0983. gp: 0.0224 \n","[100/3000] D(real): -0.19. D(fake): -0.23. rec_loss: 0.0415. gp: 0.0092 \n","[200/3000] D(real): -0.21. D(fake): -0.23. rec_loss: 0.0325. gp: 0.0149 \n","[300/3000] D(real): -0.19. D(fake): -0.21. rec_loss: 0.0312. gp: 0.0061 \n","[400/3000] D(real): -0.19. D(fake): -0.22. rec_loss: 0.0293. gp: 0.0061 \n","[500/3000] D(real): -0.19. D(fake): -0.22. rec_loss: 0.0270. gp: 0.0064 \n","[600/3000] D(real): -0.20. D(fake): -0.20. rec_loss: 0.0260. gp: 0.0041 \n","[700/3000] D(real): -0.17. D(fake): -0.21. rec_loss: 0.0309. gp: 0.0128 \n","[800/3000] D(real): -0.19. D(fake): -0.20. rec_loss: 0.0251. gp: 0.0080 \n","[900/3000] D(real): -0.17. D(fake): -0.17. rec_loss: 0.0243. gp: 0.0058 \n","[1000/3000] D(real): -0.17. D(fake): -0.19. rec_loss: 0.0232. gp: 0.0059 \n","[1100/3000] D(real): -0.20. D(fake): -0.22. rec_loss: 0.0235. gp: 0.0044 \n","[1200/3000] D(real): -0.16. D(fake): -0.18. rec_loss: 0.0230. gp: 0.0066 \n","[1300/3000] D(real): -0.19. D(fake): -0.22. rec_loss: 0.0278. gp: 0.0099 \n","[1400/3000] D(real): -0.14. D(fake): -0.17. rec_loss: 0.0270. gp: 0.0330 \n","[1500/3000] D(real): -0.19. D(fake): -0.19. rec_loss: 0.0214. gp: 0.0056 \n","[1600/3000] D(real): -0.18. D(fake): -0.22. rec_loss: 0.0268. gp: 0.0093 \n","[1700/3000] D(real): -0.21. D(fake): -0.23. rec_loss: 0.0208. gp: 0.0062 \n","[1800/3000] D(real): -0.17. D(fake): -0.19. rec_loss: 0.0229. gp: 0.0054 \n","[1900/3000] D(real): -0.16. D(fake): -0.19. rec_loss: 0.0214. gp: 0.0047 \n","[2000/3000] D(real): -0.17. D(fake): -0.21. rec_loss: 0.0207. gp: 0.0085 \n","[2100/3000] D(real): -0.16. D(fake): -0.18. rec_loss: 0.0197. gp: 0.0068 \n","[2200/3000] D(real): -0.15. D(fake): -0.18. rec_loss: 0.0197. gp: 0.0840 \n","[2300/3000] D(real): -0.17. D(fake): -0.20. rec_loss: 0.0197. gp: 0.0083 \n","[2400/3000] D(real): -0.18. D(fake): -0.23. rec_loss: 0.0198. gp: 0.0100 \n","[2500/3000] D(real): -0.18. D(fake): -0.22. rec_loss: 0.0198. gp: 0.0046 \n","[2600/3000] D(real): -0.17. D(fake): -0.21. rec_loss: 0.0198. gp: 0.0408 \n","[2700/3000] D(real): -0.17. D(fake): -0.23. rec_loss: 0.0199. gp: 0.0855 \n","[2800/3000] D(real): -0.18. D(fake): -0.23. rec_loss: 0.0200. gp: 0.0064 \n","[2900/3000] D(real): -0.18. D(fake): -0.23. rec_loss: 0.0196. gp: 0.0050 \n","Total time in scale 0: 9992[sec] (3.33[sec]/epoch on avg.). D(real): -0.179346, D(fake): -0.229702, rec_loss: 0.0196. gp: 0.0050\n","****************************** Finished working on scale 0 ******************************\n"]}],"source":["#training\n","startTime = time.time()\n","\n","if len(inpainting_indices)%2 != 0:\n","    raise Exception('Provide START and END indices of each hole!')\n","\n","if is_cuda:\n","    torch.cuda.set_device(gpu_num)\n","    device = torch.device(\"cuda:%d\" % gpu_num)\n","\n","if manual_random_seed != -1:\n","    random.seed(manual_random_seed)\n","    torch.manual_seed(manual_random_seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","samples, Fs = get_input_signal(input_file, max_length)\n","\n","fs_list = [f for f in fs_list if f <= Fs]\n","if fs_list[-1] != Fs:\n","    fs_list.append(Fs)\n","\n","scales = [Fs / f for f in fs_list]\n","\n","print('Working on file: %s' % input_file)\n","\n","scheduler_milestones = [int(num_epochs * 2 / 3)]\n","\n","alpha1 = 0\n","alpha2 = 1e-4\n","add_cond_noise = True\n","\n","dilation_factors = [2 ** i for i in range(num_layers)]\n","\n","if not os.path.exists(output_folder):\n","    os.mkdir(output_folder)\n","\n","if os.path.exists(output_folder):\n","    dirs = glob.glob(output_folder + '*')\n","    output_folder = output_folder + '_' + str(len(dirs) + 1)\n","\n","os.mkdir(output_folder)\n","print('Writing results to %s\\n' % output_folder)\n","\n","signals_list, fs_list = create_input_signals(scales, set_first_scale_by_energy, min_energy_th,  filter_size, torch.tensor(samples), Fs)\n","if len(signals_list) == 0:\n","    set_first_scale_by_energy = False\n","    scales = scales[2:]  # Manually start from 500\n","    signals_list, fs_list = create_input_signals(scales, set_first_scale_by_energy, min_energy_th,  filter_size, torch.tensor(samples), Fs)\n","scales = [Fs / f for f in fs_list]\n","\n","fs_list = fs_list\n","inputs_lengths = [len(s) for s in signals_list]\n","\n","print('Running on ' + str(device))\n","\n","output_signals, loss_vectors, generators_list, noise_amp_list, energy_list, reconstruction_noise_list = train(\n","                          manual_random_seed, fs_list, scales, growing_hidden_channels_factor,learning_rate, beta1, scheduler_lr_decay,\n","                          plot_losses, initial_noise_amp, noise_amp_factor, signals_list, dilation_factors, output_folder, inputs_lengths)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"pfWK4VohjG0Z","executionInfo":{"status":"ok","timestamp":1663956961881,"user_tz":-60,"elapsed":336,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}}},"outputs":[],"source":["class AudioGenerator(object):\n","    def __init__(self, output_folder, fs_list, dilation_factors, filter_size, Fs, device, generators_list=None, noise_amp_list=None, reconstruction_noise_list=None):\n","        super(AudioGenerator, self).__init__()\n","        self.generators_list = generators_list\n","        self.noise_amp_list = noise_amp_list\n","        self.reconstruction_noise_list = reconstruction_noise_list\n","        self.output_folder = output_folder\n","        self.fs_list= fs_list\n","        self.device = device\n","        self.dilation_factors = dilation_factors\n","        self.filter_size = filter_size\n","        self.Fs= Fs\n","        if not os.path.exists(os.path.join(output_folder, 'GeneratedSignals')):\n","            os.mkdir(os.path.join(output_folder, 'GeneratedSignals'))\n","\n","    def generate(self, nSignals=1, length=20, generate_all_scales=False):\n","        for sig_idx in range(nSignals):\n","            # Draws a signal up to current scale, using learned generators\n","            output_signals_list = draw_signal(self.generators_list,\n","                                              [round(f * length) for f in self.fs_list], self.fs_list,\n","                                              self.noise_amp_list,  self.filter_size, self.dilation_factors, self.device, \n","                                              output_all_scales=generate_all_scales)\n","            # Write signals\n","            if generate_all_scales:\n","                for scale_idx, sig in enumerate(output_signals_list):\n","                    write_signal(\n","                        os.path.join(self.output_folder, 'GeneratedSignals',\n","                                     'generated@%dHz.wav' % self.fs_list[scale_idx]),\n","                        sig, self.fs_list[scale_idx], overwrite=False)\n","            else:\n","                write_signal(\n","                    os.path.join(self.output_folder, 'GeneratedSignals',\n","                                 'generated@%dHz.wav' % self.fs_list[-1]),\n","                    output_signals_list, self.fs_list[-1], overwrite=False)\n","\n","    def condition(self, condition, write=True):\n","        condition[\"condition_scale_idx\"] = np.where(np.array(self.fs_list) <= condition[\"condition_fs\"])[0][\n","                                               -1] + 1\n","        condition[\"condition_signal\"] = torch.Tensor(condition[\"condition_signal\"]).expand(1, 1, -1).to(\n","            self.device)\n","        lengths = [int(condition[\"condition_signal\"].shape[2] / condition[\"condition_fs\"] * fs) for fs in\n","                   self.fs_list]\n","        conditioned_signal = draw_signal(self.generators_list, lengths, self.fs_list, self.noise_amp_list, \n","                                         self.filter_size, self.dilation_factors, self.device,\n","                                         condition=condition)\n","        if write:\n","            output_file = os.path.join(self.output_folder, 'GeneratedSignals',\n","                                       'conditioned_on_' + condition['name'])\n","            write_signal(output_file, conditioned_signal, self.Fs)\n","        else:\n","            return conditioned_signal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XgcJ9x-zq06"},"outputs":[],"source":["audio_generator = AudioGenerator(output_folder, fs_list, dilation_factors, filter_size, Fs, device, generators_list, noise_amp_list,\n","                                 reconstruction_noise_list=reconstruction_noise_list)\n","\n","condition_signal, condition_fs = librosa.load(\n","                os.path.join(output_folder, 'real@%dHz.wav' % fs_list[0]), sr=None)\n","condition = {'condition_signal': condition_signal, 'name': 'self', 'condition_fs': condition_fs}\n","audio_generator.condition(condition)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_h5MYm6XlkT"},"outputs":[],"source":["path = \"/content/outputs_2/GeneratedSignals\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8G7WET3XpL-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663910739963,"user_tz":-60,"elapsed":4,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}},"outputId":"6656d56b-44e6-4392-e462-a79349632150"},"outputs":[{"output_type":"stream","name":"stdout","text":["We have 7 .Wav Files with 4.49 Mb in size\n"]}],"source":["paths = []\n","size = 0\n","for root, dirs, files in os.walk(path):\n","    for file in files:\n","        if (file.endswith(\".wav\") and  (not (file.startswith(\".\") or file.startswith(\"noise\")))):\n","             paths.append(os.path.join(root, file))\n","             size += os.path.getsize(os.path.join(root, file))\n","             \n","\n","\n","print(f'We have {len(paths)} .Wav Files with {size/1024**2:.2f} Mb in size')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wEOF_KDFXuLP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663910743744,"user_tz":-60,"elapsed":1623,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}},"outputId":"b8b3714a-5ebf-4623-df5f-7e9d8afd54f9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['updating: content/outputs_2/GeneratedSignals/ (stored 0%)',\n"," 'updating: content/outputs_2/GeneratedSignals/conditioned_on_self.wav (deflated 8%)',\n"," 'updating: content/outputs_2/GeneratedSignals/conditioned_on_self_1.wav (deflated 8%)',\n"," 'updating: content/outputs_2/GeneratedSignals/conditioned_on_self_2.wav (deflated 8%)',\n"," '  adding: content/outputs_2/GeneratedSignals/conditioned_on_self_4.wav (deflated 8%)',\n"," '  adding: content/outputs_2/GeneratedSignals/conditioned_on_self_3.wav (deflated 8%)',\n"," '  adding: content/outputs_2/GeneratedSignals/conditioned_on_self_5.wav (deflated 8%)',\n"," '  adding: content/outputs_2/GeneratedSignals/conditioned_on_self_6.wav (deflated 8%)']"]},"metadata":{},"execution_count":38}],"source":["!!zip -r /content/outputs_2/GeneratedSignals.zip /content/outputs_2/GeneratedSignals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h-xlABrTXzot","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1663910585023,"user_tz":-60,"elapsed":6,"user":{"displayName":"ibantxo drums","userId":"09404425136583861915"}},"outputId":"1c2d8572-d459-4046-f71c-04afc7eb2b29"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_bb863704-751f-42ee-b165-be6deeca2454\", \"GeneratedSignals.zip\", 617899)"]},"metadata":{}}],"source":["from google.colab import files\n","files.download('/content/outputs_2/GeneratedSignals.zip')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7lsHlC2_X33Z"},"outputs":[],"source":["!cp /content/outputs_2/GeneratedSignals.zip  /content/drive/MyDrive/FinalProject/CAW_outputs"]},{"cell_type":"markdown","metadata":{"id":"Hcwlfmc9aOop"},"source":["# Remember to change this path everytime you do generation ok"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}